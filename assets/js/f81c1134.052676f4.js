"use strict";(self.webpackChunkmy_docs_website=self.webpackChunkmy_docs_website||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"architectural-evolution","metadata":{"permalink":"/blog/architectural-evolution","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/showcase-architectural-evolution.md","source":"@site/blog/showcase-architectural-evolution.md","title":"The architectural evolution of OceanBase Database","description":"oceanbase database","date":"2024-06-04T13:18:36.000Z","tags":[],"readingTime":3.9,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"The architectural evolution of OceanBase Database","slug":"architectural-evolution"},"unlisted":false,"nextItem":{"title":"Data Compression Technology Explained Balance between Costs & Performance","permalink":"/blog/compression-ratio"}},"content":"![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682303077795.jpg)\\n\\n_Yang Zhifeng, OceanBase\u2019s Chief Architect, lately introduced the evolution of the technical architecture of OceanBase Database from V0.5 to V4.0 and shared his thoughts along the journey. This article is only part of his sharing. Since the content of the sharing is so extensive, we will divide it into the following articles:_\\n\\n_\u2012 The architectural evolution of OceanBase Database_\\n\\n_\u2012_ [_What is the integrated architecture of OceanBase Database?_](https://oceanbase.medium.com/integrated-architecture-of-oceanbase-database-615dcf707f38)\\n\\n_\u2012 SQL engine and transaction processing in the integrated architecture of OceanBase Database_\\n\\n_\u2012 Performance of OceanBase Database in standalone mode: a performance comparison with MySQL 8.0_\\n\\nNow, let\u2019s roll out the first article and present to you the architectural evolution of the OceanBase Database in over ten years.\\n\\n\x3c!-- truncate --\x3e\\n\\n# OceanBase Database V0.5 to V3.0\\n\\nThe development of the OceanBase Database started in 2010. The first version, OceanBase Database V0.5, consists of a storage layer and a computing layer, as shown in the figure below. The computing layer, which is stateless, provides SQL services, and the storage layer is a storage cluster of two types of servers.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682303106461.jpg)\\n\\nOceanBase architecture v0.5\\n\\nThis architecture allows OceanBase Database to well support services such as Taobao Favorites. Resources, especially those for data reads, are moderately scalable. The SQL layer is stateless and can be freely scaled.\\n\\nThe most significant vulnerability of this architecture is that data writes are handled by UpdateServer nodes. Such an architecture featuring single-point writes and multi-point reads cannot be scaled to cope with a higher concurrency.\\n\\nAnother issue that may not be found just from the figure is that with the storage layer and SQL layer split that way, we can hardly keep the query latency under control, which is serious. As a matter of fact, the latency of online services fluctuates, which is hard to control if the latency requirement is high.\\n\\nTo address that issue, we laid off the old architecture and developed a new one, which has empowered OceanBase Database V1.0 to V3.0. Generally, the new architecture features equivalent nodes in a cluster. All nodes can handle transactions and save data while processing SQL queries. As shown in the figure below, the new architecture is horizontally replicable and vertically scalable. Horizontally, you can copy zone replicas to ensure the high availability of the service; vertically, you can scale out the cluster by adding more OBServers.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682303185573.jpg)\\n\\nOceanBase architecture v1.0 \u2014 v3.0\\n\\nThis new architecture showed excellent scalability, which gave us the confidence to challenge the Transaction Processing Performance Council Benchmark C (TPC-C) test with OceanBase Database V3.0. The result was encouraging. OceanBase Database became the world\u2019s only distributed database and China\u2019s first database that passed the test back then and got a record-high score that remains untouched so far.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682303213301.jpg)\\n\\nHorizontal scalability of OceanBase\\n\\nThe stunning performance indicates the great horizontal scalability of OceanBase Database V3.0. You can see in the figure above that, as the number of OBServer nodes increases from three, the performance curve of OceanBase Database swings up linearly before it finally hits the record-high tpmC of 707 million. This large OceanBase cluster of 1,557 OBServers processed 20 million transactions per second within the 8-hour TPC-C stress test. The test results show that the new architecture is highly scalable. Such high scalability and concurrency satisfy the needs of most online service systems in the world today.\\n\\n# OceanBase Database V4.0\\n\\nIn response to new business requirements, we have developed an up-to-date architecture to support OceanBase Database V4.0. The most significant update is the introduction of dynamic log streams. In the previous architecture, transaction, and storage resources are scaled by the same granularity. If the storage is sharded and scaled by shard, transaction processing, and high availability capabilities will also be scaled by shard. In the architecture for V4.0, we decouple transaction scaling from storage scaling, so that several storage shards share the same transaction log stream and the high availability capabilities corresponding to that log stream.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682303236782.jpg)\\n\\nOceanBase 4.0 architecture\\n\\nThe fundamental idea that leads to this update is that we want to support smaller applications. It\u2019s true that the architecture for V3.0 has no problem in supporting huge applications like those of Ant Group. However, as OceanBase Database becomes more popular among customers other than tech giants, if we keep the association between the number of log streams and that of partitions, it cannot be adapted to many scenarios, especially for small and medium-sized companies. This is because the resource overhead of databases that hold small applications becomes greater as the number of log streams increases.\\n\\nIn the next articles, you will learn more details about the integrated architecture of the OceanBase Database. Stay tuned!"},{"id":"compression-ratio","metadata":{"permalink":"/blog/compression-ratio","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/showcase-compress.md","source":"@site/blog/showcase-compress.md","title":"Data Compression Technology Explained Balance between Costs & Performance","description":"With more and more data being generated, storage and maintenance costs are increasing accordingly. Data compression seems to be a natural choice to reduce storage costs.","date":"2024-06-04T13:18:36.000Z","tags":[],"readingTime":13.09,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Data Compression Technology Explained Balance between Costs & Performance","slug":"compression-ratio"},"unlisted":false,"prevItem":{"title":"The architectural evolution of OceanBase Database","permalink":"/blog/architectural-evolution"},"nextItem":{"title":"Flink CDC + OceanBase integration solution for full and incremental synchronization","permalink":"/blog/flink-cdc"}},"content":"With more and more data being generated, storage and maintenance costs are increasing accordingly. Data compression seems to be a natural choice to reduce storage costs.\\n\\nHowever, there is a dilemma when it comes to the data compression ratio. If the ratio is high, it usually takes a great deal of time to compress and decompress data, which tends to reduce the I/O performance of the memory and disk and is not a good idea for a latency-sensitive key business. If the ratio is low, the compressed files still occupy a large share of disk space, which makes the compression ineffective.\\n\\n\x3c!-- truncate --\x3e\\n\\n# A high compression ratio doesn\u2019t necessarily make sense\\n\\nMost database products on the market provide data compression features. However, compression design and performance vary based on the storage engine architecture and database application scenarios.\\n\\nGenerally, conventional transactional databases store data in fixed-length blocks, which is good for read/write performance but causes additional overhead and space waste.\\n\\nDatabases oriented to online transaction processing (OLTP) must support higher transactions per second (TPS) during data write and update operations. Such databases often use row-oriented B-tree storage engines and therefore are more conservative on data compression. A B-tree storage engine usually aligns the size of a fixed-length data node with that of a persistent data block for data management. In some cases, updated data should be written to data blocks in real-time, which leads to the compression of the whole data node even if the DML operations affect only a few rows in the node, bringing more overhead. Besides, the compression of fixed-length data blocks also causes a waste of storage space because it is difficult to predict the post-compression size of a data block.\\n\\nAnalytical databases, however, are naturally more suitable for high-ratio compression but are not good for query and update performance.\\n\\nFor systems oriented to online analytical processing (OLAP), such as a data warehouse, data is usually imported by batch, with little incremental data. Therefore, analytical databases usually use column-oriented storage engines that write logs for incremental data and update baseline data periodically. Such storage engines tend to perform data compression during batch import and data updates in the background, based on compression strategies that aim for a higher compression ratio. For example, a column-oriented storage engine will compress more data into larger data blocks, store the data of the same column in adjacent data blocks, and then encode the data based on the data characteristics of the column to achieve a higher compression ratio. However, the compression may significantly reduce the performance of point queries and decrease the TPS of data updates.\\n\\n**In short, the higher the compression ratio, the higher the compression and decompression overhead, leading to a greater impact on performance.** We found that users are more concerned about database performance than the compression ratio, especially when it comes to critical business. Given that compressing or decompressing data while it is being read or written inevitably consumes computing resources and affects the transaction processing performance, a conventional business database only supports compression of cold data such as archives or backups. Hot data that is frequently queried or updated cannot be compressed for the sake of business performance.\\n\\n**A high compression ratio makes sense for user benefits only when high database performance is guaranteed in the first place.** That way, we can improve the business efficiency at lower costs.\\n\\n# A new solution from OceanBase: Encoding and compression\\n\\nCompression does not consider data characteristics, while encoding compresses data by column based on data characteristics. These two types of methods are orthogonal, meaning that we can first encode a data block and then compress it to achieve a higher compression ratio.\\n\\nAt the stage of compression, OceanBase Database uses compression algorithms to compress a microblock, without considering the data format, and eliminate data redundancy if detected. OceanBase Database supports compression algorithms like zlib, Snappy, zstd, and LZ4. In most cases, Snappy and LZ4 are faster but provide lower compression ratios. LZ4 is faster than Snappy in terms of both compression and decompression. Both zlib and zstd deliver higher compression ratios but are slower, with zstd being faster in decompression. You can use DDL statements to configure and change the compression algorithms for tables based on table application scenarios.\\n\\nBefore data is read from a compressed microblock, the whole microblock is decompressed for scanning, which causes CPU overhead. To minimize the impact of decompression on query performance, OceanBase Database assigns the decompression task to asynchronous I/O threads, which will call the callback function to decompress the microblock when the I/O operations on the microblock are finished and then store the decompressed microblock in the block cache as needed. This, combined with query prefetching, provides a pipeline of microblocks for query processing threads and eliminates the additional overhead due to decompression.\\n\\nThe advantage of compression is that it makes no assumptions about the data to compress and can always find a pattern for any data and then compress it. A relational database has more prior knowledge about the structured data stored in it. We believe that we can make use of the prior knowledge to improve data compression efficiency.\\n\\nE**ncoding algorithms in OceanBase Database**\\n\\nTo achieve a higher compression ratio and help users reduce storage costs, OceanBase Database has developed a variety of encoding algorithms that have worked well. In addition to single-column encoding algorithms, such as bit-packing, Hex encoding, dictionary encoding, run-length encoding (RLE), constant encoding, delta encoding for numeric data, and delta encoding for fixed-length strings, OceanBase Database also provides innovative column equal encoding and column prefix encoding to compress different types of redundant data in one or several columns, respectively.\\n\\n1.  **Bit-packing and Hex encoding: Reducing bit width for storage**\\n\\nBit-packing and Hex encoding are similar in that they represent the original data by encoding it with fewer bits if the cardinality of data is small. Given an int64 column with a value range of \\\\[0, 7\\\\], for example, we can store only the lowest three bits to represent the originals and remove higher zero bits to save storage space. Or, for a string column with a character cardinality of less than 17, we can map each character in the column to a hexadecimal number in the range of \\\\[0x0, 0xF\\\\]. This way, each original character is represented by a 4-bit hexadecimal number, which occupies less storage space. Moreover, these two encoding algorithms are not exclusive, which means that you can perform bit-packing or hex encoding on numeric data or strings generated by other encoding algorithms to further reduce data redundancy.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682307908847.jpg)\\n\\nBit packing\\n\\nHex encoding\\n\\n**2\\\\. Dictionary encoding and RLE: De-duplicating data in a single column**\\n\\nWe can create a dictionary for a data block to compress data with low cardinality. If low-cardinality data within a microblock is not evenly distributed, meaning that rows with the same data in a column cluster together, we can also use RLE to compress the reference values of the dictionary. Further, if most of the rows in the microblock of a low-cardinality column have the same value, we can compress the data even more by using constant encoding, which stores the most frequent data as constants and the non-constant data and corresponding row subscripts in an exception list.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682307917471.jpg)\\n\\nDictionary encoding and RLE\\n\\n**3\\\\. Delta encoding: Compressing data based on the value range**\\n\\nOceanBase Database supports delta encoding for numeric data and delta encoding for fixed-length strings. Delta encoding for numeric data is suitable for compressing numeric data in a small value range. To compress date, timestamp, or other numeric data of small adjacent differences, we can keep the minimum value, and store the difference between the original data and the minimum value in each row. These differences can usually be compressed by using bit-packing. Delta encoding for fixed-length strings is a better choice for compressing artificially generated nominal numbers, such as order IDs and ID card numbers, as well as strings of a certain pattern, such as URLs. We can specify a pattern string for a microblock of such data and store the difference between the original string and the pattern string in each row to achieve better compression results.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682307925452.jpg)\\n\\nNumeric difference\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682307934304.jpg)\\n\\nString difference\\n\\n**4\\\\. Span-column encoding: Reducing redundancy across columns**\\n\\nOceanBase Database provides span-column encoding to improve the compression ratio by making use of the data similarity between different columns. Technically, data is encoded within a column in a column-oriented database. In a real-life business database, however, it\u2019s common that data in different columns to be associated.\\n\\n- If most data of column A and column B are the same, we can use column equal encoding. In this coding, column A is taken as the reference of column B, and for column B, only rows that are different from those in column A need to be stored.\\n- If data in column A is the prefix of data in column B, OceanBase Database adopts column prefix encoding. In this encoding, all data in column A and data excluding prefixes in column B are stored.\\n\\nSpan-column encoding performs better in the compression of composite columns and some system-generated data, and can also reduce data redundancy due to inappropriate table design paradigms.\\n\\nA**daptive compression: Allow the database to select encoding algorithms**\\n\\nThe compression performance of data encoding algorithms is related not only to the table schema but also to data characteristics such as the distribution of data and the value range of data within a microblock. This means it is hard to achieve the best compression performance by specifying a columnar encoding algorithm when you design the table data model. To make your work easier and improve the compression performance, OceanBase Database automatically detects the data in all columns and selects an appropriate encoding algorithm for each column during data compaction. The database supports encoding the same column in different microblocks with different algorithms.\\n\\nThe process of detection and selection involves a lot of computing work, which puts more pressure on the CPU during the compaction. Therefore, in a compaction task, OceanBase Database analyzes the data type, value range, number of distinct values (NDV), and other data characteristics and selects a more suitable encoding algorithm by using a heuristic algorithm based on the encoding algorithm and compression ratio selected for the same column in the previous microblock. In addition to selecting a more suitable encoding algorithm, this method ensures acceptable CPU overhead for encoding during the compaction.\\n\\nO**ne more step: Optimize the query of encoded data**\\n\\nTo better balance the compression result and query performance, we have considered the impact on query performance when designing data encoding methods.\\n\\n1.  Row-level random access\\n\\nIn compression scenarios, the whole data block will be decompressed even if you want to access just a slice of data in it. In some analytical systems intended for scan queries rather than point queries, patched frame-of-reference (PFOR) delta coding, for example, is adopted to decode adjacent or all preceding rows before accessing a row in a data block.\\n\\nTo better support transactional workloads, OceanBase Database needs to support more efficient point queries. So, our encoding methods ensure row-level random access to encoded data. To be specific, the database accesses and decodes only the metadata of the target row in a point query, which reduces the computational amplification during random point queries. In addition, OceanBase Database stores all metadata required for decoding data in a microblock just within the microblock, so that the microblock is self-describing with better memory locality during decoding.\\n\\n2\\\\. Cache decoder\\n\\nOceanBase Database initializes a decoder for data decoding of each column. Creating the decoder certainly consumes some CPU and memory resources. To further reduce the response time for accessing encoded data, OceanBase Database caches the decoder and data block together in the block cache. This way, the cached decoder can directly decode the cached data block. The creation and caching of decoders are also executed by asynchronous I/O callback threads to reduce the decoder initialization overhead. When it fails to hit a decoder in the block cache, OceanBase Database also builds a cache pool for the metadata memory and objects required by the decoder, which are reused in different queries.\\n\\nAfter those optimizations, even encoded data in SSTables with a hybrid row-column storage architecture can well support transactional workloads.\\n\\nWhen it comes to analytical queries, encoded data, which is stored in a hybrid row-column storage architecture, is distributed more compactly and is more CPU cache friendly. These characteristics of encoded data are similar to those of data stored by column, and therefore we can improve analytical queries by applying some optimization methods that often work on columnar storage, such as Single Instruction Multiple Data (SIMD) processing.\\n\\nFurthermore, since we store dictionaries, null bitmaps, constants, and other metadata in a microblock to describe the distribution of the encoded data, we can use the metadata to optimize the execution of some filter and aggregate operators during a data scan and to perform calculations directly on the compressed data. Many data warehouses use similar approaches to optimize the query execution. In a SIGMOD 2022 paper titled \u201cCompressDB: Enabling Efficient Compressed Data Direct Processing for Various Databases\u201d, authors also describe a similar idea, where the system efficiency is improved by pushing some computations down to the storage layer and executing them directly on the compressed data. The performance turns out to be pretty good.\\n\\nOceanBase Database significantly strengthened its analytical processing capabilities in V3.2. In the latest versions, aggregation and filtering are pushed down to the storage layer and the vectorized engine is used for vectorized batch decoding based on the columnar storage characteristics of encoded data. When processing a query, OceanBase Database performs calculations directly on the encoded data by making full use of the encoding metadata and the locality of encoded data stored in a columnar storage architecture. This brings a significant increase in the execution efficiency of the pushed-down operators and the data decoding efficiency of the vectorized engine. Data encoding-based operator push-down and vectorized decoding are also important features that have empowered OceanBase Database to efficiently handle analytical workloads and demonstrate an extraordinary performance in TPC-H benchmark tests.\\n\\n# Testing the encoding and compression algorithms\\n\\nWe performed a simple test to see the influence of different compression methods on the compression performance of OceanBase Database.\\n\\nWe used OceanBase Database V4.0 to test the compression ratio with a TPC-H 10 GB data model in transaction scenarios and with an IJCAI-16 Brick-and-Mortar Store Recommendation Dataset in user behavior log scenarios.\\n\\nThe TPC-H model simulated real-life order and transaction processing. We tested the compression ratios of two large tables in the TPC-H model: ORDERS table, which stores order information, and LINEITEM table, which stores product information. Under the default configuration where both the zstd and encoding algorithms were used, the compression ratio of the two tables reached about 4.6 in OceanBase Database, much higher than the compression ratio achieved when only the encoding or zstd algorithm was used.\\n\\nThe IJCAI-16 Taobao user log dataset stored desensitized real behavior logs of Taobao users. The compression ratio reached 9.9 when the zstd and encoding algorithms were combined, 8.3 for the encoding algorithm alone, and 6.0 for the zstd algorithm alone.\\n\\nAs you can see, OceanBase Database works better in real business data compression and also shows an impressive performance on datasets with less data redundancy like the TPC-H model.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682307942389.jpg)\\n\\nData compression performance test\\n\\nThis article shares our thoughts and solution for data compression in OceanBase Database. Based on the LSM-Tree storage engine, OceanBase Database has achieved a balance between storage costs and query performance.\\n\\nIf you have any questions, welcome to leave a comment below!"},{"id":"flink-cdc","metadata":{"permalink":"/blog/flink-cdc","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/showcase-flink.md","source":"@site/blog/showcase-flink.md","title":"Flink CDC + OceanBase integration solution for full and incremental synchronization","description":"This article introduces OceanBase and explains the application scenarios of Flink CDC and OceanBase.","date":"2024-06-04T13:18:36.000Z","tags":[],"readingTime":9.77,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Flink CDC + OceanBase integration solution for full and incremental synchronization","slug":"flink-cdc"},"unlisted":false,"prevItem":{"title":"Data Compression Technology Explained Balance between Costs & Performance","permalink":"/blog/compression-ratio"},"nextItem":{"title":"Integrated Architecture of OceanBase Database","permalink":"/blog/integrated-architecture"}},"content":"This article introduces OceanBase and explains the application scenarios of Flink CDC and OceanBase.\\n\\nThis article was compiled from the speech by Wang He (Chuanfen) (OceanBase Technical Expert) at the Flink CDC Meetup on May 21. The main contents include:\\n\\n1. An Introduction to OceanBase\\n2. The Implementation Principle of Flink CDC OceanBase Connector\\n3. The Application Scenarios of Flink CDC + OceanBase\\n4. The Future Outlook of Flink CDC OceanBase Connector\\n\\n\x3c!-- truncate --\x3e\\n\\n\x3c!-- https://www.alibabacloud.com/blog/599338 --\x3e\\n\\n## 1. An Introduction to OceanBase\\n\\n![](/img/ob-history.png)\\n\\nOceanBase is a distributed database developed by Ant Group. The project was established in 2010 and developed iteratively. Its earliest application is to Taobao\'s favorites. In 2014, the OceanBase R&D Team moved from Taobao to Ant Group, mainly responsible for Alipay\'s internal de-IOE work. It means replacing the Oracle database used by Alipay. Currently, all the data in Ant Group databases have been migrated to OceanBase. On June 1, 2021, OceanBase was officially opened source to the public, and a MySQL-compatible version was launched.\\n\\nOceanBase has undergone three generations of architecture upgrades, including a distributed storage system initially applied to e-commerce companies, a general-purpose distributed database, and an enterprise-level distributed database.\\n\\n![](https://yqintl.alicdn.com/2551bf4e7b277e5a3af59a91d93ec311b2bb5253.png)\\n\\nThe preceding figure shows the architecture of OceanBase.\\n\\nThe top-level app accesses the server side of the OceanBase database through the OBProxy (SLB proxy). The data on the server side has multiple replicas. The relationship between the replicas is similar to the primary-secondary relationship in the database architecture, but it is table-level. The partition of the partition table is divided into multiple replicas at the table level and then scattered on multiple servers.\\n\\nThe architecture of OceanBase has the following features:\\n\\n- **No Shared Architecture:** Each node has its complete SQL engines, storage engines, and transaction processing logic. All nodes are completely peer-to-peer, and there are no layered structures.\\n- **Partition-Level Availability:** It provides partition-level availability. The partition is the basic unit for implementing reliability and scalability, achieving access routing, SLB, and automatic fault recovery in the OceanBase database.\\n- **High Availability and Strong Consistency:** There are multiple data replicas. The consistency protocol of Paxos is used to provide high reliability between multiple replicas and ensure the Persistent Event Log (PEL) is successful at the majority of nodes.\\n\\n![](https://yqintl.alicdn.com/1ad5f7ab919762810f62c4df3af5e7fa98073068.png)\\n\\nThe core features of OceanBase:\\n\\n1. **High Availability:** It is based on the Paxos protocol, with strong consistency. A few copies are faulty, but data is not lost, and services are not stopped.\\n2. **High Scalability:** It supports online horizontal scale-out and capacity reduction. Nodes can automatically achieve SLB.\\n3. **High Compatibility:** The Community Edition is compatible with MySQL protocols and syntax.\\n4. **Low Cost:** OceanBase database costs about one-third of MySQL. The **storage compression ratio** is extremely high because of its low hardware quality requirements and a lot of optimization of storage.\\n5. **Multi-Tenancy:** Resources are completely isolated between tenants. Different service parties only need to manage data in their tenants, which can save costs.\\n6. **HTAP:** It implements both OLTP and OLAP in one set of engines.\\n\\n## 2. The Implementation Principle of Flink CDC OceanBase Connector\\n\\n![](https://yqintl.alicdn.com/23dd7319f3dd972de1ba223fa81c271bb194650c.png)\\n\\nThe current mainstream CDC mainly relies on the logs of the database. After capturing the incremental logs of the database, it is necessary to ensure their orderliness and integrity, process the logs, and write them to the destination, such as a data warehouse or query engine.\\n\\n![](https://yqintl.alicdn.com/44b0297d17d2126dd13e890b4a3726efadd953a5.png)\\n\\nOceanBase provides some components for capturing incremental data. It is a distributed database, so its data is scattered when it falls into the log. It provides an obcdc component for capturing database logs. It interacts with the OceanBase Server through RPC to capture the original log information. After processing, an orderly log flow can be spit out, and the downstream can consume the orderly log flow by accessing the obcdc component.\\n\\nThere are currently three major downstream consumers:\\n\\n- **Oblogproxy:** An open-source component that is used to consume log flows. Flink CDC relies on this component to capture incremental data.\\n- **OMS Store:** The data migration service provided by OceanBase. The commercial version of OMS has been upgraded in many iterations and supports many data sources. Last year, OMS supported the community edition of the two data sources, OceanBase and MySQL.\\n- **JNI Client:** You can directly use obcdc to interact with OBSserver to capture incremental logs. This downstream consumer is under an open-source plan.\\n\\n![](https://yqintl.alicdn.com/328e4a928e5085a8cec7788caf02dd0270c92724.png)\\n\\nCurrently, there are two OceanBase CDC components provided by the Open-Source Community:\\n\\n- **OceanBase Canal:** Canal is an open-source tool of Alibaba for capturing MySQL incremental logs. OceanBase community strengthens the ability to capture and parse incremental logs based on the latest code of Canal in the open-source edition.\\n- **Flink CDC:** Flink CDC uses obcdc through oblogproxy to capture incremental logs from OceanBase and then uses another open-source component logproxy-client to consume incremental logs and process them.\\n\\n![](https://yqintl.alicdn.com/c2e8f3a1e5be13f89433bf823ecd1e17215886a4.png)\\n\\nThe lower-left corner of the figure above shows how to define a dynamic table. Data streams are converted into tables in Flink in the form of dynamic tables. You can only perform SQL operations if the data stream is converted into a table. After that, Continuous Query queries the growing dynamic table. The obtained data is still presented in the form of a table. Then, the Continuous Query will convert it into a data stream and send it to downstream consumers.\\n\\n![](https://yqintl.alicdn.com/822e48b74147da81cce8f7445c39df9126e3a195.png)\\n\\nThe figure above shows the implementation principle of Flink CDC.\\n\\nFlink CDC Connector only reads the source data. It only reads the data from the data source and then inputs it to the Flink engine.\\n\\nThe current Flink CDC Connectors are mainly divided into the following three categories:\\n\\n- **MySqlSource:** It implements the latest source interface and achieves concurrent reads.\\n- **DebeziumSourceFunction:** It implements SourceFunction based on Debezium and supports earlier versions of MySQL, Oracle, MongoDB, SqlServer, and PostgreSQL.\\n- **OceanBaseSourceFunction:** It implements the SourceFunction interface and achieves full and incremental reads based on JDBC and logproxy-client.\\n\\n![](https://yqintl.alicdn.com/ab26b801cb8343ff90148423812e6a7c6e6eccd8.png)\\n\\nThe preceding figure shows the data path of the Flink CDC OceanBase Connector.\\n\\nIncremental data is captured first through logproxy. The logproxy-client monitors the data stream of incremental logs. After Flink CDC reads the incremental logs, the data will be written to Flink in line with the processing logic of Flink CDC. Full data are captured through JDBC.\\n\\nThe functions supported by the current Flink CDC OceanBase Connector are mainly limited by logproxy. Currently, it supports capturing data at a specified time. However, since OceanBase is a distributed database, you cannot accurately find the starting point of incremental log data. However, if you specify the starting point with a timestamp, there may be some duplicate data.\\n\\nOceanBase Community Edition does not have table locks in the process of capturing full data. Therefore, the data edge points cannot be determined by locking the full data.\\n\\nConsidering the two aspects above, currently, only the at-least-once working mode is supported, and the exactly-once mode has not been realized.\\n\\n## 3. Application Scenarios of Flink CDC + OceanBase\\n\\n![](https://yqintl.alicdn.com/ee62c27fbff429d58d3ccd5aa342097b437e546c.png)\\n\\n### 3.1 Scenario 1: Data Integration Based on Database and Table Sharding\\n\\nFlink CDC supports real-time consistent synchronization and processing of full data and incremental data in tables. OceanBase Connector supports regular expression matching for reading data. For database and table sharding, the OceanBase Connector can be used to create dynamic tables to read data from data sources and write all the data into a table to realize the aggregation of table data.\\n\\n![](https://yqintl.alicdn.com/7f69f6fa9b9350ec1cb117c82b7d542d1c8218fb.png)\\n\\n### 3.2 Scenario 2: Cross-Cluster/Tenant Data Integration\\n\\nOceanBase is a multi-tenant system. Currently, cross-tenant access is not available for the tenants of the community edition of MySQL. Therefore, if you need to read data across tenants, you must connect with multiple databases to read data separately. Flink CDC is naturally suitable for cross-tenant data reading, in which each tenant corresponds to a dynamic table for data source reading, and then the data converges in Flink.\\n\\n![](https://yqintl.alicdn.com/8cb06edf87b1a326c4574193d6b0deacc8327f2e.png)\\n\\n### 3.3 Scenario 3: Data Integration of Multiple Data Sources\\n\\nYou can aggregate data from different types of data sources. There is no change in cost for the integration of MySQL, TiDB, and other data sources compatible with the MySQL protocol since the data format is the same.\\n\\n![](https://yqintl.alicdn.com/7c9962ae45c6441c135d83ec7d56a320ce7c7a53.png)\\n\\n### 3.4 Scenario 4: Building an OLAP Application\\n\\nOceanBase is an HTAP database that has a strong TP capability and can be used as a data warehouse. The JDBC connector in Flink allows you to write data to databases compatible with the MySQL protocol. Therefore, you can use Flink CDC to read source data and write this data to OceanBase through the Flink JDBC connector. OceanBase is used as a destination.\\n\\nOceanBase provides three data processing methods: SQL, Table API, and HBase API. All required components are open-source.\\n\\n## 4. The Future of OceanBase Connector\\n\\n![](https://yqintl.alicdn.com/5ec8f36591abb9ae66d52d3cb8c9bdbc2afe5e99.png)\\n\\nThe preceding figure lists the current status of the OceanBase CDC solution.\\n\\n**OMS Community Edition:** It is a functional subset of OMS Commercial Edition, but it is not open-source. As a white screen tool, its operation is friendly, and it supports real-time consistent synchronization and processing of full data and incremental data, with data checksum and O&M capabilities. Its disadvantage is that the deployment process is a bit cumbersome. It only supports two data sources, MySQL and OceanBase Community Edition, and does not support incremental DDL.\\n\\n**DataX + Canal/Otter:** It is an open-source solution for data migration. Otter is the parent project of Canal. It is mainly aimed at multi-site high availability and supports two-way data synchronization. Its incremental data reading is based on Canal. The advantage of this solution is that it supports a variety of targets and supports HBase, ES, and RDB. The disadvantage is that Canal and Otter do incremental, DataX do full, incremental and full are separated, and data redundancy will occur in the connection part.\\n\\n**Flink CDC:** It is a pure open-source solution. The community is active, and the users are growing rapidly. It supports multiple sources and targets as well as real-time consistent synchronization and processing of full data and incremental data. At the same time, as an excellent big data processing engine, Flink can be used for ETL. The downside is that OceanBase Connector currently does not support incremental DDL and exactly-once. Therefore, there may be data redundancy in the overlap between incremental and full data.\\n\\n![](https://yqintl.alicdn.com/1b6e0bd4ebf2229a950f47f0327a6891f917ecc8.png)\\n\\nWe will optimize data reading in the future. Parallelize the full data part and use the new parallel processing framework of the source interface. In the incremental data part, skip the logproxy service and directly capture incremental data from the OceanBase database. Use the obcdc component through the JNI client to directly capture data.\\n\\nSecondly, enrich the functional features. Currently, Flink CDC only supports OceanBase Community Edition, which uses the same components for incremental log reading as the OceanBase Enterprise Edition. Therefore, OceanBase Enterprise Edition can support incremental log reading, incremental DDL, exactly-once mode, and rate limiting, with only minor changes.\\n\\nFinally, improve code quality. First, we will operate end-to-end testing. As for format conversion, use runtime converter instead of JdbcValueConverters to improve performance. Implement support for the new source interface (parallel processing framework).\\n\\n## Q&A\\n\\n**Q: How about the usability and stability after Flink CDC OceanBase Connector is open-source?**\\n\\nA: In terms of usability, we have successively developed many open-source components with the support of non-open-source community editions (such as OMS and OCP). In terms of stability, OceanBase has been widely used in Ant Group, and the MySQL-compatible version has been put into large-scale applications in more than 20 enterprises. Therefore, there is no need to worry about its stability.\\n\\n**Q: Where is metadata (such as the shard information and the index information) stored in OceanBase?**\\n\\nA: They are stored in the OB server and can be directly queried through SQL."},{"id":"integrated-architecture","metadata":{"permalink":"/blog/integrated-architecture","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/showcase-integrated-architecture.md","source":"@site/blog/showcase-integrated-architecture.md","title":"Integrated Architecture of OceanBase Database","description":"oceanbase database","date":"2024-06-04T13:18:36.000Z","tags":[],"readingTime":5.53,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Integrated Architecture of OceanBase Database","slug":"integrated-architecture"},"unlisted":false,"prevItem":{"title":"Flink CDC + OceanBase integration solution for full and incremental synchronization","permalink":"/blog/flink-cdc"},"nextItem":{"title":"Integrated SQL Engine in OceanBase Database","permalink":"/blog/integrated-sql-engine"}},"content":"![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302303091.jpg)\\n\\n_Yang Zhifeng, OceanBase\u2019s Chief Architect, lately introduced the evolution of the technical architecture of OceanBase Database from V0.5 to V4.0 and shared his thoughts along the journey. This article is only part of his sharing. Since the content of the sharing is so extensive, we will divide it into the following articles:_\\n\\n_\u2012_ [_The architectural evolution of OceanBase Database_](https://medium.com/@oceanbase/the-architectural-evolution-of-oceanbase-database-9ab70506fc15)\\n\\n\x3c!-- truncate --\x3e\\n\\n_\u2012 What is the integrated architecture of OceanBase Database?_\\n\\n_\u2012 SQL engine and transaction processing in the integrated architecture of OceanBase Database_\\n\\n_\u2012 Performance of OceanBase Database in standalone mode: a performance comparison with MySQL 8.0_\\n\\n_This article introduces the integrated architecture of the OceanBase database._\\n\\nThe architecture of OceanBase Database V4.0 allows you to deploy a distributed database or a MySQL-like standalone database in the same way that you are familiar with.\\n\\nIf you deploy a standalone OceanBase database or a single-container tenant in an OceanBase cluster, the database provides the same efficiency and performance as a conventional standalone database does.\\n\\nIf you deploy OceanBase Database in a distributed architecture, no additional costs will be incurred in tenant management, transaction processing, and SQL statement execution.\\n\\nWith that in mind, we must design each database layer, such as the SQL layer, storage layer, and transaction layer, in a way that integrates features of a standalone database and a distributed database to meet various needs with a balanced performance at each layer.\\n\\n# Shared-everything or shared-nothing?\\n\\nIn the development of databases, a long-debated question is whether we should design databases in shared-nothing clusters or shared-everything architectures to maximize efficiency.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302331364.jpg)\\n\\nA physical cluster in a real production environment today is neither completely shared-everything nor completely shared-nothing.\\n\\nA server may have multiple CPU cores, 96 or more in some cases, with increasingly large memory space. Servers interact with disks and external devices by using high-speed system buses over a high-performance network. The cluster that consists of such servers is not purely shared-nothing. In such a cluster, each server has excellent computing and storage capabilities that we can use. That is how all standalone or centralized databases are scaled up. We have no reason to ignore the vertical scaling option of distributed databases, which are generally horizontally scaled.\\n\\nThe question is, how to vertically scale up a distributed database?\\n\\nAssume that we have built a distributed database of architecture as shown on the left side of the following figure. This architecture consists of separated computing nodes and storage nodes and is adopted by many distributed databases today. The computing node layer and storage node layer are separated. They interact with each other by using a GTM (Global Transaction Manager) or TSO (Timestamp Oracle), which handles global transactions and is necessary for multi-server interactions.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302372544.jpg)\\n\\nIf this distributed database is deployed on a single node and runs as a standalone database, the biggest problem here is that the interactions between the components on a single server lead to considerable overhead, which is unnecessary for a standalone database such as MySQL. So, a distributed database in such a simple deployment mode cannot compete with a standalone database.\\n\\n# The integrated architecture: Meeting the needs in both standalone and distributed scenarios\\n\\nA standalone database must have separate layers of SQL, transaction, and storage engines. Similarly, you can consider the distributed SQL engine, transaction engine, and storage engine of a distributed database as three separate layers. We want to integrate the three engines of a standalone database with those of a distributed database by using the same code that can dynamically adapt to different deployment modes.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302392770.jpg)\\n\\nIn addition, to maximize the scalability of the single server of a standalone database, the SQL layer must support parallel execution and the transaction layer must provide a concurrency management feature, such as Multi-Version Concurrency Control (MVCC). Techniques like group commit are also required to concurrently execute multiple transactions on a single server. The storage layer must support parallel I/O so that the server can make full use of additional disks and bandwidth.\\n\\nWhen you deploy a distributed database that also provides features of a standalone system, it is not enough to focus only on scalability. Each server of the distributed database must also achieve very high efficiency and support the efficient serial execution of distributed transactions so that standalone transactions can be adaptively optimized.\\n\\nOceanBase Database also provides the standalone LSM-Tree storage engine, a one-of-a-kind innovative technology. In a standalone database, we can only perform major compactions that may affect both the front end and the back end. Fortunately, we are not talking about a 100% standalone database, and we can introduce a distribution strategy so that the major compactions of multiple replicas can be performed in turn during off-peak hours. We have taken into account both standalone and distributed features in the design of the SQL layer, transaction layer, and storage layer so that each layer consumes no extra overhead in both standalone and distributed scenarios.\\n\\nAs a result, OceanBase Database provides two essential features:\\n\\nFirst, in standalone mode, no additional components are deployed and OceanBase Database works with only a single process, which means no complex interactions are performed between different processes. In this single-process multi-thread model, interactions are performed by calling functions, which is very important.\\n\\nSecond, components interact vertically between the SQL layer, transaction layer, and storage layer. Function calls are made for interaction between the layers on a single server. RPCs are used only when it is necessary to communicate between nodes.\\n\\nLike the shared-nothing and shared-everything architecture mentioned above, OceanBase Database performs interactions between the SQL layer, transaction layer, and storage layer within each OBserver by calling functions, which is the same as in a standalone database. When interactions between multiple OBservers are required, you can choose the most efficient solution based on your needs.\\n\\nFor example, if an SQL query is sent from OBServer A to the SQL layer of OBServer B and requests to access the storage layer of OBServer B, the optimal solution depends on the specific task, and the choice may vary based on the workload. The same is true for the transaction layer. For a standalone transaction, interactions with other transaction components are unnecessary.\\n\\nIf you want to learn more about the architecture of OceanBase\u2019s storage, SQL, and transaction engines, here are some references for you:\\n\\n1.  [Design A Storage Engine for Distributed Relational Database from Scratch](https://medium.com/@oceanbase/design-a-storage-engine-for-distributed-relational-database-from-scratch-a27d69a47fe0)\\n2.  [Designing a Distributed SQL Engine: Challenges & Decisions](https://medium.com/@oceanbase/designing-a-distributed-sql-engine-challenges-decisions-52bea749b2f0)\\n3.  [Infinite Possibilities of a Well-Designed Transaction Engine in Distributed Database](https://medium.com/@oceanbase/infinite-possibilities-of-a-well-designed-transaction-engine-in-distributed-database-273dd2ad3c5d)\\n\\nIn the next articles, you will learn more details about the SQL engine and transaction processing in OceanBase. Stay tuned!"},{"id":"integrated-sql-engine","metadata":{"permalink":"/blog/integrated-sql-engine","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/showcase-sql-engine.md","source":"@site/blog/showcase-sql-engine.md","title":"Integrated SQL Engine in OceanBase Database","description":"oceanbase database","date":"2024-06-04T13:18:36.000Z","tags":[],"readingTime":4,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Integrated SQL Engine in OceanBase Database","slug":"integrated-sql-engine"},"unlisted":false,"prevItem":{"title":"Integrated Architecture of OceanBase Database","permalink":"/blog/integrated-architecture"},"nextItem":{"title":"Implementing a Vectorized Engine in a Distributed Database","permalink":"/blog/vectorized-engine"}},"content":"![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682301944833.jpg)\\n\\n_Yang Zhifeng, OceanBase\u2019s Chief Architect, lately introduced the evolution of the technical architecture of OceanBase Database from V0.5 to V4.0 and shared his thoughts along the journey. This article is only part of his sharing. Since the content of the sharing is so extensive, we will divide it into the following articles:_\\n\\n_\u2012_ [_The architectural evolution of OceanBase Database_](https://medium.com/@oceanbase/the-architectural-evolution-of-oceanbase-database-9ab70506fc15)\\n\\n\x3c!-- truncate --\x3e\\n\\n_\u2012_ [_What is the integrated architecture of OceanBase Database?_](https://medium.com/@oceanbase/integrated-architecture-of-oceanbase-database-615dcf707f38)\\n\\n_\u2012 SQL engine and transaction processing in the integrated architecture of OceanBase Database_\\n\\n_\u2012 Performance of OceanBase Database in standalone mode: a performance comparison with MySQL 8.0_\\n\\n_This article introduces the SQL engine and transaction processing in the integrated architecture of the OceanBase database._\\n\\nWe have designed the SQL execution engine of OceanBase Database based on many scenarios. Because we want it to be adaptive and provide the best performance in each scenario.\\n\\nGenerally speaking, each SQL statement can be executed in serial or parallel mode.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302044156.jpg)\\n\\n# Serial execution, parallel execution on a standalone server, and distributed parallel execution\\n\\nIn serial execution, if the table or partition involved is located on the local server, the execution process is exactly the same as that of an SQL statement on a local or standalone server. If the required data is stored on another server, OceanBase Database either fetches the remote data and processes it on the local server or performs remote execution. In remote execution, if all data required in a transaction is located on another server, OceanBase Database forwards the transaction to that server, which will access the storage, process the data, commit the transaction, and then return the results. If the data required in an SQL statement is located on many servers, to consume the minimal overhead comparable to serial execution on a standalone server, OceanBase Database performs distributed execution, pushes the computing tasks to each server for local processing, and then aggregates the results. This way, the degree of parallelism (DOP) is 1, and no extra resources are consumed. In the case of parallel execution, OceanBase Database supports parallel execution on a standalone server, distributed parallel execution, and parallel DML write.\\n\\nAs mentioned above, serial execution consumes minimal overhead. In this case, OceanBase Database performs serial scans on a single server without context switchover or remote data access, which is highly efficient. For a small business, serial execution is sufficient to meet the requirements. If you need to process a large amount of data, OceanBase Database also supports parallel execution on a standalone server. This capability is not supported by many open-source standalone databases. With enough CPUs, OceanBase Database can linearly shorten the processing time of an SQL statement by performing parallel execution. You only need to deploy OceanBase Database on a high-performance multiprocessor server and enable parallel execution.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302118057.jpg)\\n\\nOceanBase Database supports the parallel execution of distributed execution plans on multiple servers. This means that the DOP will no longer depend on the number of CPU cores of a standalone server. Hundreds or even thousands of CPU cores can be added to support a higher DOP.\\n\\n# Serial execution: DAS execution and distributed execution\\n\\nDAS execution and distributed execution are two types of serial execution supported in the OceanBase Database.\\n\\nData pull is a way to perform DAS execution with minimal resource consumption. If the required data is located on another server and only single-point query or table access by the index primary key is involved, OceanBase Database will pull the required data to the local server. The execution plan has the form of a local execution plan. The executor will automatically pull data. Sometimes, however, it is better to push down the computing tasks. Therefore, OceanBase Database also supports distributed execution. Note that distributed execution does not consume extra resources, and is performed with the same DOP as DAS execution.\\n\\nFor some special queries or large-scale scans, OceanBase Database will adaptively select the execution mode based on the cost to achieve the best results.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302163423.jpg)\\n\\nThe parallel execution framework of OceanBase Database adaptively handles both parallel executions on a standalone server and distributed parallel execution. Parallel execution can be performed by worker threads on the local server or many different servers. OceanBase Database has an adaptive data transmission layer in its distributed execution framework. In the case of parallel execution on a standalone server, the data transmission layer automatically converts the data interactions between the threads into replicas stored in the memory. In this way, the data transmission layer makes it possible to adaptively handle tasks in two different scenarios. In fact, the parallel execution engine schedules tasks for parallel execution on a standalone server and distributed parallel execution in the exact same way.\\n\\nIn the next articles, you will learn more details about transaction processing in OceanBase. Stay tuned!"},{"id":"vectorized-engine","metadata":{"permalink":"/blog/vectorized-engine","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/showcase-vectorize.md","source":"@site/blog/showcase-vectorize.md","title":"Implementing a Vectorized Engine in a Distributed Database","description":"oceanbase database","date":"2024-06-04T13:18:36.000Z","tags":[],"readingTime":12.945,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Implementing a Vectorized Engine in a Distributed Database","slug":"vectorized-engine"},"unlisted":false,"prevItem":{"title":"Integrated SQL Engine in OceanBase Database","permalink":"/blog/integrated-sql-engine"},"nextItem":{"title":"BOSS Zhipin \u2014\u2014 How to save 70% storage cost through OceanBase with an archive database of 1 billion rows per day?","permalink":"/blog/boss-zhipin"}},"content":"![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306489558.jpg)\\n\\nPhoto by [What Is Picture Perfect](https://unsplash.com/@whatispictureperfect?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/vectors?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\\n\\n_This article is written by Qu Bin, a tech expert at OceanBase. Qu Bin has years of experience in the database industry and he had worked on column-oriented and time-series database kernel development. Currently, he is mainly working on vectorized engine development._\\n\\nThis article introduces the value, design, and technical solutions of vectorized engines from the following three aspects:\\n\\n- What is vectorized engine?\\n- Why vectorized engine?\\n- Implementing a vectorized engine\\n\\n\x3c!-- truncate --\x3e\\n\\n# Background\\n\\nWhen talking to customers, we found that many users want to perform OLAP tasks such as JOIN queries and aggregate analysis while they are processing online transactions. The SQL execution engine of a database must be highly productive in order to deal with OLAP tasks, which often involve the processing of massive data and complicated computing and queries, and are therefore time-consuming.\\n\\nWe used to use parallel execution to evenly share workloads among multiple CPU cores in the database\u2019s distributed architecture and successfully cut the query response time (RT) by reducing the amount of data processed by each CPU core. As the user data builds up, the workload of each CPU also increases, if no computing resources are added. On-site investigations indicate that the CPU utilization approximates 100% in some OLAP tasks, such as aggregate analysis and JOIN queries when massive data is involved.\\n\\nTo improve single-core processing performance and reduce response time, we have designed a vectorized query engine from scratch.\\n\\n# What is Vectorized Engine?\\n\\nThe concept of the vectorized engine was introduced in a 2005 paper titled \u201cMonetDB/X100: Hyper-Pipelining Query Execution\u201d. Prior to the vectorized engine era, the volcano model was widely adopted in the database industry.\\n\\nOriginally known as the iterator model, the volcano model was formally introduced in the 1994 paper \u201cVolcano \u2014 An Extensible and Parallel Query Evaluation System\u201d, and was adopted by many early versions of mainstream relational databases at that time.\\n\\nIn the early years of database technologies, when database I/O was slow and memory and CPU resources were expensive, database experts developed the classic volcano model, which allows an SQL engine to compute one data row at a time to avoid memory exhaustion.\\n\\nAlthough the volcano model has been widely applied, its design does not bring out the full potential of CPU performance. And it often causes data congestion during complex queries such as JOIN queries, queries with subqueries, and queries containing the ORDER BY keyword.\\n\\nIn the paper \u201cDBMSs On A Modern Processor: Where Does Time Go?\u201d, the authors have minutely dissected the resource consumption of database systems in the framework of modern CPUs.\\n\\nThe following figure clearly shows that the CPU time for computation is not greater than 50% in sequential scans, index-based scans, and JOIN queries. On the contrary, the CPU spends quite an amount of time (50% on average) waiting for resources, due to memory or resource stalls. Plus the cost of branch mispredictions, the percentage of CPU time for computation is often far less than 50%. For example, the minimum percentage of CPU time for computation in index-based scans is less than 20%.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306497765.jpg)\\n\\nCPU time breakdown for SQL execution\\n\\nUnlike the traditional volcano model which iterates data row by row, the vectorized engine adopts vector iteration to pass multiple rows of data at a time from one operator to another.\\n\\nIn other words, vectorization makes a great leap from single-cell operations to vector operations.\\n\\n# Why Vectorized Engine?\\n\\nThere are two advantages of the vectorized engine.\\n\\n1\\\\. A cache-friendly method that returns vector data with fewer function calls\\n\\nTo further improve CPU utilization and reduce the memory/resource stalls during SQL execution, vectorized engines are introduced and applied to the design of modern databases.\\n\\nSimilar to the traditional volcano model, a vectorized engine also pulls data from the root node of an operator tree. The difference is that the vectorized engine passes vector data at a time and keeps the data as compact as possible in the memory, rather than calling the next() function to pass one row at a time. Since the data is contiguous, the CPU can quickly load the data to the level-2 cache (L2 cache) by instruction prefetching, which reduces memory stalls and thus improves CPU utilization. The contiguous and compact data in the memory also make it possible to process a set of data at a time by running a SIMD instruction. This brings the computing power of modern CPUs into full play.\\n\\nThe vectorized engine drastically reduces the number of function calls. Assuming that you want to query a table of 100 million rows of data, a database of the volcano model must perform 100 million iterations to complete the query. If you use a vectorized engine and set the vector size to 1024 rows, the number of function calls to execute the query is significantly reduced to 97,657, which is calculated by dividing 100 million by 1024. Inside an operator, the function crunches a chunk of data by traversing the data in a loop instead of nibbling one row at a time. Such vector processing of contiguous data is more friendly to the dCache and iCache of CPUs and reduces cache misses.\\n\\n2\\\\. Higher CPU capabilities to process an instruction stream with fewer branch predictions\\n\\nThe paper \u201cDBMSs On A Modern Processor: Where Does Time Go?\u201d also indicates that branch mispredictions have a serious impact on the database performance because the CPU halts the execution of an instruction stream and refreshes the pipeline upon a misprediction. The paper \u201cMicro Adaptivity in Vectorwise\u201d released at the 2013 ACM SIGMOD Conference on Management of Data (SIGMOD\u201913) also elaborates on the execution efficiency of branching at different selectivities. A figure is provided below for your information.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306505841.jpg)\\n\\nImpact of branching on execution\\n\\nThe logic of the SQL engine of a database is complicated. Therefore, conditional logic is inevitable for the volcano model. In contrast, the vectorized engine can keep conditionals at the minimum in an operator. For example, the vectorized engine can avoid an IF statement within a FOR loop by overriding data writes by default, thus protecting the CPU pipeline from branch mispredictions and greatly improving CPU capabilities.\\n\\n3\\\\. Faster computation accelerated by SIMD instructions\\n\\nThe vectorized engine handles contiguous data in the memory, hence can easily load a set of data into a vector register. It then sends a single instruction, multiple data (SIMD) instruction to perform vector computation instead of using the traditional scalar algorithm. Note that the SIMD instruction is closely related to the CPU architecture, and corresponding instruction sets are provided for x86, ARM, and PPC architectures. At present, the Intel x86 architecture supports the most instructions. The figure below shows SIMD instructions for the x86 architecture and the data types that each instruction supports. For more information, see the official manual of Intel.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306515132.jpg)\\n\\nData types supported by Intrinsic instructions of Intel\\n\\n# Implementing a Vectorized Engine in OceanBase\\n\\nThis section details the implementation of the OceanBase vectorized engine from the following aspects: storage, data organization, and SQL operators.\\n\\n## Storage Vectorization\\n\\nOceanBase Database stores data in microblocks, the minimum unit of I/O operations. The size of each microblock is 64 KB by default and can be resized.\\n\\nIn each microblock, the data is stored in columns. During a query, sets of data in a microblock are projected to the memory of the SQL engine by columns. Thanks to the compact structure, the data can be easily cached, and the projection process can be accelerated by using SIMD instructions. Since the vectorized engine does not maintain physical rows, it fits well with the data storage mode in a microblock. This makes data processing simpler and more efficient. The data storage and projection logic is illustrated in the following figure.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306524742.jpg)\\n\\nVectorized storage of OceanBase Database\\n\\n## Data Organization in a Vectorized SQL Engine\\n\\n**Memory orchestration**\\n\\nIn the SQL engine, all data is stored in expressions. Expressions are managed by a Data Frame, a contiguous piece of memory of no more than 2 MB in size. In other words, Data Frame holds the data of all expressions involved in SQL queries, and the SQL engine allocates the required memory from the Data Frame.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306532988.jpg)\\n\\nMemory orchestration in the OceanBase SQL engine\\n\\nIn a non-vectorized engine, an expression processes only one row of data cells at a time, as shown in the left part of the above figure. In a vectorized engine, an expression stores multiple rows of compactly structured data cells, as shown in the right part of the above figure.\\n\\nThis way, the data in an expression is computed as a vector, which is more friendly to the CPU cache. The compact data structure also allows easy computation acceleration by using SIMD instructions.\\n\\nIn addition, the number of cells allocated to each expression, or the vector size, can be adjusted based on the size of the L2 cache of the CPU and the number of expressions in an SQL statement. The principle of vector resizing is to ensure that all cells involved in the computation are stored in the L2 cache of the CPU to reduce the impact of memory stalls.\\n\\n**Design of filter representations**\\n\\nThe filter representations of vector engines need to be redesigned. This is because a vector engine returns vector data at a time, and only a part of the data is output. Other data is filtered out. It is important to efficiently identify the data to be output or the valid data. The paper \u201cFilter Representation in Vectorized Query Execution\u201d compares the following two common strategies in the industry:\\n\\n- Identifying rows by bitmaps. In this strategy, a bitmap is created to include a number of bits that equals the size of the returned data vector. The bit of a valid row is set to 1, whereas the bit of an invalid row is set to 0.\\n- Recording valid rows by using an additional selection vector. In this strategy, the subscripts of valid rows are stored in a selection vector.\\n\\nOceanBase Database uses the bitmap strategy because, among other advantages, it occupies a small memory space. This prevents the out-of-memory (OOM) error especially when a query involves too many operators or extra-large vectors.\\n\\nThe data identified by a bitmap is sparse when the data selectivity is low, which may lead to unsatisfactory performance. Some databases tackle this by adding sorting methods to densify the data. However, we have found in practice that the SQL execution under HTAP workloads often involves blocking operators, such as Sort, Hash Join, and Hash Group By, or transmission operators. These operators intrinsically output dense data. Extra data sorting only causes unnecessary overheads. Therefore, the OceanBase vectorized engine does not provide a method to modify the bitmap structure.\\n\\n## Implementing Operators in a Vectorized Engine\\n\\nThe vectorization of operators is an essential part of the OceanBase vectorized engine. To support the vectorized engine, all query operators are redesigned to fit its characteristics. In accordance with the working principles of the vectorized engine, each operator fetches vector data from the lower-level operator through the vector interface, and the data in each operator is engineered by following guidelines such as branchless programming, memory prefetching, and SIMD instructions. This allows the database to maximize performance improvement. As a large number of operators are involved, I would like to take Hash Join and Sort Merge Group By as examples.\\n\\n**Hash Join**\\n\\nHash Join implements a Hash lookup of two tables, say tables R and S, by creating and probing a Hash table. When the Hash table is larger than the L2 cache of the CPU, the random access of the Hash table will cause memory stalls and greatly reduce the execution efficiency. Therefore, cache optimization is an important part of Hash Join vectorization, where the impact of cache misses on performance is addressed as a top priority.\\n\\nIt is worth mentioning that the vectorized Hash Join operator of OceanBase Database does not implement hardware-conscious Hash Joins such as Radix Hash Join. Cache misses and memory stalls are avoided by vector-based Hash value computation and memory prefetching.\\n\\nRadix Hash Join effectively reduces the cache and Translation Lookaside Buffer (TLB) misses, but it needs to scan table R twice and incurs the cost of creating histograms and additional materialization. The vectorization of Hash joins is more streamlined in OceanBase Database. First, a Hash table is created based on the partitions of tables S and R. At the Hash table probe operation, the vectorized Hash value is first obtained by vector computation. Then, the data in the Hash bucket corresponding to that vector is prefetched and loaded into the CPU cache. Finally, the results are compared based on the Join conditions. By controlling the vector size, it is ensured that the prefetched vector data can be loaded into the L2 cache of the CPU. This way, cache misses and memory stalls can be kept at a minimum during data comparison, thereby improving CPU utilization.\\n\\n**Sort Merge Group By**\\n\\nSort Merge Group By is a common aggregation operation. It sorts the data in order, uses the Group By operator to find the grouping boundaries based on data comparison, and then computes the data in the same group.\\n\\nFor example, the data in column c1 is ordered, as shown in the figure below.\\n\\n![oceanbase database](https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306542965.jpg)\\n\\nVectorization of the Sort Merge Group By operator\\n\\nIn the volcano model that iterates only one data row at a time, eight comparisons are required for Group 1, and sum(c1) needs to perform eight additions to get the result.\\n\\nThe vectorized engine performs the comparison and aggregation separately. In other words, it first performs eight comparisons to determine the number of data rows in Group 1. As the data values in the group are the same, sum() and count() can be further optimized. In this example, sum(c1) can be performed by 1 x 8, and count(c1) can be directly added by 8.\\n\\nIn addition, vectorization can also speed up computation by introducing methods such as dichotomization. Assuming that a 16-row vector is defined for column c1 in the figure above. Under the dichotomization strategy, the engine takes the first step of 8 rows to compare the data from row 0 to row 7. If the data is equal, it sums up the first 8 data rows. In the second step of 8 rows, the engine compares the data from row 7 to row 15. If the data are not equal, it goes back by 4 rows and compares the data again, until it finds the grouping boundary. After that, the engine repeats the preceding steps to find the next grouping boundary. Under the dichotomization strategy, duplicate data can be skipped during comparison, which predicates faster computation. Dichotomization delivers poor performance if the column contains few duplicate data. We can decide whether to enable dichotomization during the execution based on statistics such as the number of distinct value (NDV).\\n\\nThe OceanBase vectorized engine is under constant upgrade. For example, the current SIMD instructions of OceanBase Database are written for the AVX-512 instruction set, which is applicable to the x86 instruction set architecture. With the increase of ARM-based applications, we will provide SIMD instructions for ARM-based systems. Furthermore, as a lot of operators can be optimized to support the vectorized engine, the OceanBase team will continue to work on this and integrate more new operators and technical solutions with the vectorized engine to better support users in their businesses."},{"id":"boss-zhipin","metadata":{"permalink":"/blog/boss-zhipin","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/user-boss.md","source":"@site/blog/user-boss.md","title":"BOSS Zhipin \u2014\u2014 How to save 70% storage cost through OceanBase with an archive database of 1 billion rows per day?","description":"Author: Zhang Yujie, Database Engineer with BOSS Zhipin","date":"2024-06-04T09:30:23.000Z","tags":[{"inline":true,"label":"User Case","permalink":"/blog/tags/user-case"}],"readingTime":12.56,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"boss-zhipin","title":"BOSS Zhipin \u2014\u2014 How to save 70% storage cost through OceanBase with an archive database of 1 billion rows per day?","tags":["User Case"]},"unlisted":false,"prevItem":{"title":"Implementing a Vectorized Engine in a Distributed Database","permalink":"/blog/vectorized-engine"},"nextItem":{"title":"DMALL \u2014\u2014 a summary of database selection experience in SaaS scenarios","permalink":"/blog/dmall"}},"content":"Author: Zhang Yujie, Database Engineer with BOSS Zhipin\\n\\n## I. Background\\n\\nBOSS Zhipin is the first \\"direct-hiring\\" online recruitment service in the world, and has become China\'s largest job searching platform. An important job of my team is to store the conversation records during the recruitment process into a database, which has held a tremendous amount of data and is taking in 500 million to 1 billion data records on a daily basis. However, these conversation records are rarely or never accessed or updated after they are written to the database. The growing volume of online data, especially the cold historical conversation records, has occupied petabytes of storage space of the online business database, resulting in serious waste of hardware resources and escalating IT costs. In addition to bloating the online business database, the increasing data volume also reduces the query efficiency, which hinders data changes and system scaling.\\n\\nTo address these issues, we need to separate hot data from cold historical conversation records. The hot data is stored in a sharded online database of multiple MySQL clusters. Expired data is regularly migrated to the archive database every month.\\n\\n\x3c!-- truncate --\x3e\\n\\n## II. Database Selection\\n\\nTo build an archive database with a humongous capacity, we compared MySQL, ClickHouse, OceanBase Database, and an open-source distributed database (let\'s call it \\"DB-U\\") in terms of storage costs and high availability.\\n\\n### (I) Storage costs\\n\\nWe need to retain historical conversation data for three to five years and must control the cost of massive storage. First, we created a table in each of the databases to store historical messages. The table schemas are the same. See the following figure.\\n\\n![1706241250](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241250230.png)\\n\\nThen, we wrote the same 100 million data rows to each of the tables, and compared their disk usage. See the following figure.\\n\\n![1706241310](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241310742.png)\\n\\nApparently, compared with MySQL and DB-U, columnar storage-based ClickHouse and OceanBase Database boasting an ultra-high compression ratio incur significantly lower storage costs. We then delved deeper into the storage engines of the two winners.\\n\\n#### ClickHouse storage engine\\n\\nClickHouse uses smaller storage simply because of its column-based storage engine. Compared with a row-based storage engine, the data stored in the same column of the ClickHouse database is of the same type, and thus can be compressed more compactly. Generally, the compression ratio of columnar storage can reach 10 or higher, saving a lot of storage space and reducing the storage costs significantly.\\n\\n![1706241332](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241332521.png)\\n\\nHowever, as an archive database often involves more writes and is rarely read, a pure column-based storage engine such as ClickHouse cannot bring its great query performance into full play. Instead, its lame write performance becomes intolerable.\\n\\n#### OceanBase Database storage engine\\n\\n**1. Architecture**\\n\\nBased on the LSM-tree architecture, the storage engine of OceanBase Database stores baseline data in baseline SSTables and incremental data in MemTables and incremental SSTables. The baseline data is read-only and cannot be modified. Incremental data supports read and write operations.\\n\\n![1706241405](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241405527.png)\\n\\nIn OceanBase Database, DML operations such as INSERT, UPDATE, and DELETE are first written into MemTables in memory. This means that the data write performance is equivalent to that of an in-memory database, which agrees with the write-intensive scenario of our archive database. When the size of a MemTable reaches the specified threshold, data in the MemTable is dumped to an incremental SSTable on the disk, as shown by the red chevrons in the preceding figure. The dumping process is performed sequentially in batches, delivering much higher performance compared with the discrete random writing of a B+ tree-based storage engine.\\n\\nWhen the size of an incremental SSTable reaches the specified threshold, incremental data in the SSTable is merged with the existing baseline data to produce new baseline data. This process is referred to as a major compaction. Then, the new baseline data remains unchanged until the next major compaction. OceanBase Database automatically performs a major compaction during off-peak hours early in the morning on a daily basis.\\n\\nA downside of the log-structured merge-tree (LSM-tree) architecture is that it causes read amplification, as shown by the green arrows in the preceding figure. Upon a query request, OceanBase Database queries SSTables and MemTables separately, merges the query results, and then returns the merged result to the SQL layer. To mitigate the impact of read amplification, OceanBase Database implements multiple layers of caches in memory, such as the block cache and row cache, to avoid frequent random reads of baseline data.\\n\\n**2\\\\. Data compression technology**\\n\\nGiven the optimized storage architecture, OceanBase Database compresses data during the major compaction, when data is written to the baseline SSTable. This way, online data updates are independent of the major compaction.\\n\\nOceanBase Database supports both compression and encoding. Compression does not consider data characteristics, while encoding compresses data by column based on data characteristics. The two methods are orthogonal, meaning that we can first encode a data block and then compress it to achieve a higher compression ratio.\\n\\nOceanBase Database also supports batch persistence. This feature allows it to adopt more aggressive compression strategies. OceanBase Database uses the Partition Attributes Cross (PAX) storage mode, which features a hybrid row-column storage architecture based on microblocks. In a microblock, a group of rows are stored and encoded based on data characteristics by column, fully leveraging the locality and type characteristics of data in the same column. Variable-length data blocks and batch-compressed continuous data allow OceanBase Database to guide the compression of the next data block by using the prior knowledge derived from the compressed data blocks in the same SSTable, so that it can compress as many data rows as possible into a data block by selecting a better encoding algorithm.\\n\\n![1706241431](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241431761.png)\\n\\nUnlike some database implementations that specify data encoding on the schema, OceanBase Database employs adaptive data encoding, which works without manual intervention, reducing the workload of users and storage costs. We just need to adjust a couple of compression and encoding parameters for the archive database to start working.\\n\\n### (II) High availability and stability\\n\\nWe also compared the high availability and stability of ClickHouse and OceanBase Database.\\n\\n#### ClickHouse\\n\\nWe fully tested the automatic data synchronization performance of the ClickHouse database through data replication across different servers in the cluster to ensure high availability and fault tolerance. ZooKeeper was used to coordinate the replication process, track the status of replicas, and guarantee data consistency among them. This way, multiple data replicas are hosted on different servers to minimize the risk of data loss.\\n\\n![1706241453](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241453557.png)\\n\\nHowever, we noticed some drawbacks of this ClickHouse-based high availability solution in handling massive amounts of data. During data replication between multiple replicas, a lot of information was stored on ZooKeeper. The problem is that, ZooKeeper does not support linear scaling, which means its performance is limited by the capacity of a server. As more data was written to the archive cluster, ZooKeeper services soon became unavailable.\\n\\nIn fact, when working with ClickHouse, ZooKeeper is more like a multitasker than just a coordination service. For example, it works as a log service and stores information such as behavior logs. It also plays the role of a catalog service, verifying the schema information of tables. As a result, the data volume handled by ZooKeeper increases linearly with that of the database. Given the expected data growth rate of our archive database, this ClickHouse + ZooKeeper solution cannot survive the full data archiving over a time span of three to five years.\\n\\nFurthermore, data replication in the ClickHouse database depends heavily on ZooKeeper, which, as an external coordination service, introduces additional complexity in terms of system configuration and maintenance. Exceptions of ZooKeeper are likely to affect the replication process in the ClickHouse database. This high availability solution also prolongs troubleshooting paths, making it more difficult to locate a fault. The recovery process becomes more complicated, requiring manual intervention. Data loss was common in the test.\\n\\n#### OceanBase Database\\n\\nOceanBase Database is a native distributed database system that guarantees the data consistency between multiple replicas based on the Paxos distributed consensus protocol. The Paxos protocol ensures that a unique leader is elected to provide data services only when the majority of replicas in the OceanBase cluster reach a consensus. In other words, OceanBase Database guarantees high database availability by using multiple replicas and the Paxos protocol.\\n\\n![1706241467](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241467288.png)\\n\\nCompared with MySQL and ClickHouse, the high availability solution based on OceanBase Database makes our database O&M and business updates easier. OceanBase Database also supports multireplica and multiregion architecture, so that data replicas can be stored in IDCs in the same city or different regions for the purpose of geo-disaster recovery.\\n\\n![1706241475](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241475029.png)\\n\\nFeaturing a distributed architecture, OceanBase Database inherently supports dynamic data storage scaling. As the data volume of the archive database keeps growing, our database administrators (DBAs) only need to run a command or two to scale up a node by modifying the quota of hardware resources, or scale out a cluster by adding more nodes. After a new node is added to the cluster, OceanBase Database automatically balances the workload among the new and old nodes. The scaling process is smooth without business interruptions or downtime. This feature also saves the costs of database scale-out and data migration in response to business surges, leading to a great reduction of the risk due to insufficient database capacity.\\n\\nThe best part is, when we want to increase the capacity of a single node, add more nodes into a zone, or add a new zone to reach higher availability, we can perform these operations on OceanBase Cloud Platform (OCP), a GUI-based OceanBase Database management tool. The following figure shows a page of OCP in the process of scaling a single-zone cluster to a three-zone one.\\n\\n![1706241482](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241482608.png)\\n\\nCompared with a command-line tool, OCP is more user-friendly for the deployment and O&M of OceanBase Database, according to our DBAs, who recommended OCP.\\n\\nTo sum up, unlike MySQL and ClickHouse, OceanBase Database natively guarantees strong storage consistency. It does not compromise the eventual consistency for other capabilities, or rely on miscellaneous complex peripherals to ensure data consistency. Multi-replica disaster recovery is applicable to individual clusters. Transaction logs are persisted, and logs are synchronized between multiple replicas. The Paxos protocol guarantees log persistence for the majority of replicas. OceanBase Database provides high availability with a recovery point objective (RPO) of 0 and a recovery time objective (RTO) of less than 8s when a minority of replicas fail. In the test, OceanBase Database outperformed MySQL, ClickHouse, and DB-U, demonstrating higher system stability.\\n\\nBased on a comprehensive assessment, factoring in the storage costs, high availability, and O&M difficulty, we decided to build our archive database by using OceanBase Database.\\n\\n## III. Implementation\\n\\nOur online database is a MySQL cluster deployed in primary/standby mode. We use it to store hot data, mainly user conversation records in the last 30 days. Our archive database consists of several OceanBase clusters, which are managed by OCP. We regularly migrate expired data from the MySQL database to the archive database on a monthly basis using our internal data transmission service (DTS) tool. The following figure shows the overall architecture.\\n\\n![1706241500](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241500535.png)\\n\\nSo far, eight OceanBase archive clusters, comprising more than 20 tenants, are managed on OCP. And, our app keeps writing data by user ID hash to the MySQL database, which has hosted more than 10,000 table shards. Expired data is directly imported into the OceanBase archive database.\\n\\n![1706241507](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241507211.png)\\n\\nOur old ClickHouse archive cluster is still in use to support reads of some historical data. However, considering the stability and security issues of ClickHouse, we will gradually replace it with OceanBase Database.\\n\\n## IV. Benefits\\n\\nTo begin with, the powerful compression capability of OceanBase Database helps us archive cold data with ease, and saves storage resources by more than 70%.\\n\\n![1706241521](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241521817.png)\\n\\nIn addition, OceanBase Database is a native distributed system with great scalability. It provides high availability with an RPO of 0 and an RTO of less than 8s when a minority of replicas fail, making the database more stable.\\n\\nLast but not least, OceanBase Database comes with GUI-based OCP, which is a lifesaver for our DBAs in handling deployment and O&M tasks. OCP allows us to manage objects such as clusters, tenants, hosts, and software packages over their entire lifecycle, including their installation, O&M, performance monitoring, configuration, and upgrade. OCP of the latest version supports custom alerts. For example, we can set custom disk and memory usage thresholds. Once the usage exceeds the thresholds, we receive alerts immediately. OCP also supports backup and restore, as well as automated diagnostics during O&M.\\n\\n![1706241530](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241530556.png)\\n\\n## V. Outlook\\n\\n**1. Support for online distributed database capabilities**\\n\\nAs mentioned above, we still use a MySQL online database. Compared with using a single table in a distributed database, performing database and table sharding in the MySQL database is more complex. If the data in multiple data tables or databases is associated, the difficulty of maintaining data consistency increases drastically.\\n\\n![1706241545](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241545453.png)\\n\\nIn addition to data consistency challenges, the complexity of O&M and management of multiple database and table shards also makes system troubleshooting and maintenance a headache. As data is stored in multiple database and table shards, it is hard to trace historical data in this online MySQL database.\\n\\nAt present, many of our upper-level business modules rely on the online MySQL database, and it may take some time for us to replace it with OceanBase Database.\\n\\nThe good news is, after the introduction of OceanBase Database, we have improved our capabilities to support natively distributed database tables, and developed more convenient and feasible solutions for transforming business modules with large storage capacity and complex sharding logic.\\n\\n![1706241553](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241553431.png)\\n\\n**2\\\\. ODC and Binlog service**\\n\\nWe learned that since V4.2.2, OceanBase Developer Center (ODC), a GUI-based database development tool tailored for OceanBase Database, supports data archiving from MySQL to OceanBase Database and within OceanBase Database. It also supports configuration of automatic tasks at multiple levels. Considering the high compression ratio of OceanBase Database and the data archiving capabilities of ODC, it is quite easy to build an archive database based on OceanBase Database.\\n\\nCurrently, our business team mainly uses the internal RDS platform to call the DTS tool for data archiving. ODC is used as a supplement to the DTS tool. We will continue to learn other features of ODC to strengthen our capabilities in database O&M.\\n\\nOceanBase Database V4.2.1 and later provide the Binlog service, which allows downstream services, such as data warehouses, of the sharded MySQL database to subscribe to binlogs in a unified way, rather than separately subscribing to binlogs of each MySQL shard, making the binlog subscription easier.\\n\\n**3\\\\. Exploration of best practices**\\n\\nThe introduction of the new database system brings more challenges to our DBAs. To better tap the potential of OceanBase Database, they must select hardware of proper specifications and continuously optimize service configurations, while maintaining the stability of the database. We will keep working with the OceanBase team to figure out the most efficient and cost-effective way in using OceanBase Database, and provide strong support for the rapid and stable development of our business."},{"id":"dmall","metadata":{"permalink":"/blog/dmall","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/user-dmall.md","source":"@site/blog/user-dmall.md","title":"DMALL \u2014\u2014 a summary of database selection experience in SaaS scenarios","description":"Feng Guangpu, head of the Dmall database team, is responsible for the stability of OceanBase, TiDB, MySQL, Redis, and other databases of Dmall and the construction of its database platform as a service (PaaS) model. Feng has a wealth of experience in multi-active database architecture and data synchronization schemes.","date":"2024-06-04T09:30:23.000Z","tags":[{"inline":true,"label":"User Case","permalink":"/blog/tags/user-case"}],"readingTime":15.855,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"dmall","title":"DMALL \u2014\u2014 a summary of database selection experience in SaaS scenarios","tags":["User Case"]},"unlisted":false,"prevItem":{"title":"BOSS Zhipin \u2014\u2014 How to save 70% storage cost through OceanBase with an archive database of 1 billion rows per day?","permalink":"/blog/boss-zhipin"},"nextItem":{"title":"Sichuan Hwadee \u2014\u2014 The practice of lightweight data warehouse construction of health big data based on OceanBase","permalink":"/blog/sichuan-hwadee"}},"content":"> Feng Guangpu, head of the Dmall database team, is responsible for the stability of OceanBase, TiDB, MySQL, Redis, and other databases of Dmall and the construction of its database platform as a service (PaaS) model. Feng has a wealth of experience in multi-active database architecture and data synchronization schemes.\\n\\n> Yang Jiaxin, a senior DBA at Dmall, is an expert in fault analysis and performance optimization and loves exploring new technologies.\\n\\nAs the largest retail cloud solution and digital retail service provider in Asia and the only end-to-end service provider of omnichannel retail cloud solutions in China, Dmall conducts business in six countries and regions. Therefore, its model has been widely verified. The development of Dmall is a microcosm of the retail digitalization process in China and even the world.\\n\\nHowever, the rapid business growth and impressive business achievements have been accompanied by many challenges in retail software as a service (SaaS) scenarios and business system bottlenecks. This article takes a look at the data processing pain points in retail SaaS scenarios from the business perspective. It gives a glimpse into how Dmall improves its read and write performance and lowers storage costs while ensuring the stability and reliability of its business database.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Characteristics and pain points of retail SaaS scenarios\\n\\nDmall markets its services across borders. In China, our customers include large supermarkets, such as Wumart and Zhongbai, as well as multinational retailers, such as METRO AG and 7-Eleven. Dmall also serves many Chinese and international brands. It links brand owners, suppliers, and retailers with smooth data and information flows so that they can better support and serve consumers.\\n\\nThe long service chain, from manufacturers, brand owners, and suppliers, to retailers in various shopping malls and stores, and finally to consumers, generates massive data volumes. The system complexity increases exponentially with the data volume. As a result, the retail SaaS system of Dmall faces three major challenges:\\n\\n- High O&M complexity\\n\\n  Dmall uses a microservice architecture that involves many business links in the overall process and a large system application scale. Dmall already has more than 500 databases. Moreover, as our system continue to iterate, the data scale continues to increase, and O&M management is becoming more and more difficult.\\n\\n- Fast business growth and frequent need for horizontal scaling\\n\\n  Dmall formulated a global expansion strategy to cope with its business growth. According to the requirements of regional data security laws, we need to deploy a new system to undertake business traffic outside China. It is hard to predict future business scale and data growth in the initial deployment phase. Therefore, database resource allocation in the initial deployment phase is quite difficult. The common practice is to deploy the system with limited resources at low costs. However, rapid business growth and exponential data increase will require quick scaling.\\n\\n- The need to serve a large number of merchants in the same cluster\\n\\n  The number of stock keeping units (SKUs) of different convenience stores and supermarket chains ranges from thousands to tens of thousands. Therefore, it is impossible for us to deploy an independent system for each customer. This means that our SaaS system must support hundreds of small and medium-sized business customers, and the data generated by all merchants share database resources at the underlying layer. Moreover, we have massive individual tenants in our system, such as large chain stores. We want to isolate the resources for these tenants from those for others.\\n\\nIn short, our database needs to support a huge data scale and cope with rapid data growth.\\n\\n## Why we chose a distributed database\\n\\nTo address the aforementioned issues and requirements, we started looking for a new database solution. A distributed database provides larger capacity, transparent scaling, financial-level data security, higher development efficiency, and lower O&M costs. Therefore, it can better support our business development. Due to these benefits and advantages, we believe that distributed databases will become the prevalent trend in the data field. This is why we only looked at distributed database products during our database selection process.\\n\\n### Business-based database selection considerations\\n\\nFirst of all, we have many MySQL databases each with over 4 TB of data, and the data size is still growing rapidly. After we migrated our largest MySQL database to a distributed database, the data size increased to 29 TB. Our DBAs are very concerned about the capacity bottleneck of MySQL:\\n\\n- We have a few choices. We can continue to push R&D for data cleanup and data archiving, but this may delay other R&D jobs such as upgrades to better address our business needs.\\n- Alternatively, we can continue to expand the disks. This method may be easier to carry out on the cloud. The size of a single cloud storage disk can reach 32 TB or even larger. However, as the data volume continues to grow, this method simply puts the problem off, rather than solving it. In the end, the problem will simply become more and more difficult to solve.\\n\\nWe can also choose the database and table sharding solution, but this is an intricate and highly risky solution, and requires several months for code reconstruction to guarantee SQL capabilities.\\n\\nTherefore, we want to utilize the transparent scaling capabilities of distributed databases to smoothly support rapid business growth.\\n\\nFirst, we aim to reduce the O&M complexity and costs while ensuring system stability. If we think of a MySQL database as an egg and a MySQL instance as a basket, how many MySQL instances should we deploy to house 1000 databases? Which databases should we put on the same instance? If we put two resource-demanding databases in the same instance, resource preemption may occur. In addition, some workloads have special requirements. For example, although payment transactions generate only a small amount of data, they have high business requirements. Therefore, a payment transaction database cannot be deployed in the same instance as other databases. Because of different businesses, different priorities, different data growth rates, and different QPS requirements, DBAs often need to \\"move eggs from one basket to another\\". This creates major O&M challenges, not to mention high resource costs. We hope that we can solve this problem with distributed databases, enabling automatic \\"egg relocation\\".\\n\\n![](https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/64d2cad9b954a7dde0bfe3b57ebb85c3c3a3b61a)![1693993792](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993792381.png)\\n\\nSecond, we expect distributed databases to help us achieve high cluster availability. We mainly use MHA and Orchestrator to implement high availability for MySQL clusters. However, they are all in the form of \\"plug-ins\\", which does not solve the split-brain issue caused by network partitions. The database and high-availability component are two independent pieces of software. Therefore, they lack consistency and coordinated control. High-availability architectures like MySQL Group Replication are reliable because they are based on the Paxos or Raft distributed consensus protocol just like distributed databases such as OceanBase Database and TiDB. Such an architecture can achieve a recovery point objective (RPO) of 0 and a recovery time objective (RTO) of less than 30s.\\n\\n![](https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/6566a6165303bd34aca1fc2adaf2db473d4c5900)![1693993807](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993807766.png)\\n\\n### Database selection based on test data\\n\\nBased on the above database selection considerations, we chose the native distributed database OceanBase Database. We then compared the storage costs and QPS performance of OceanBase Database with MySQL Database in terms of table reading and writing, table reading, and table writing. The following table shows the configurations used for this test.\\n\\n|                             | OceanBase Database                                                                                                                                             | MySQL Database                                                                                                                                                 |\\n| --------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| Community Edition           | v4.1.0                                                                                                                                                         | v5.7.16                                                                                                                                                        |\\n| Memory                      | Tenant memory size: 16 GB                                                                                                                                      | innodb_buffer_pool_size: 16 GB                                                                                                                                 |\\n| Single-node configuration   | 32C RAID10 SSD                                                                                                                                                 | 32C RAID10 SSD                                                                                                                                                 |\\n| Disk flush configuration    | Forcible disk flush by default                                                                                                                                 | sync_binlog = 1 and innodb_flush_log_at_trx_commit = 2                                                                                                         |\\n| Degree of parallelism (DOP) | 5, 10, 20, 30, 60, and 120                                                                                                                                     | 5, 10, 20, 30, 60, and 120                                                                                                                                     |\\n| Test modes                  | read_write, read_only, and write_only                                                                                                                          | read_write, read_only, and write_only                                                                                                                          |\\n| Duration of a single test   | 300s, for a total of 18 tests (Concurrency x Number of test modes)                                                                                             | 300s, for a total of 18 tests (Concurrency x Number of test modes)                                                                                             |\\n| Test method                 | Run the `obd test sysbench` command (that comes with OBD), which will run the `sysbench prepare`, `sysbench run`, and `sysbench cleanup` commands in sequence. | Run the `obd test sysbench` command (that comes with OBD), which will run the `sysbench prepare`, `sysbench run`, and `sysbench cleanup` commands in sequence. |\\n\\nGiven the same configurations, MySQL Database performs slightly better than OceanBase Database in the Sysbench test with ten tables, each containing 30 million data rows, when the DOP is less than 200. However, in terms of both QPS and latency, the performance of OceanBase Database approaches that of MySQL Database when the DOP increases.\\n\\n#### OceanBase Database in different configurations\\n\\nIn the standalone deployment mode, the read and write performance is also affected by the mode of accessing the OBServer nodes.\\n\\nThe performance is 30% to 50% higher when the OBServer nodes are directly accessed than when they are accessed through OBProxy.\\n\\nTherefore, for the standalone deployment mode, we recommend that you directly connect to OBServer nodes to avoid the extra costs of accessing OBServer nodes through OBProxy.\\n\\n![](https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/1194670243918372120898947e2d916ebbf8df2e)![1693993851](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993851203.png)\\n\\nPerformance also varies with tenant memory configurations. We can see that the performance of a tenant with 32 GB of memory is 14% higher than that of a tenant with 16 GB of memory.\\n\\n![](https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/6f219cc28f657a56eaf7a3cff84dc4d448adbc12)![1693993879](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993879936.png)\\n\\n#### Tablespace comparison between OceanBase Database and MySQL Database\\n\\nIn the monitoring snapshot scenario in the production environment, after we migrated 20 tables with a total of 0.5 billion data rows from MySQL Database to OceanBase Database, the tablespace usage was reduced by a factor of six.\\n\\n![1693993904](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993904435.png)\\n\\n![](https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/ecb2401843a0b597b3c9999138346056c2e10681)\\n\\nBased on the above test results, we decided to deploy OceanBase Database for the following considerations:\\n\\n- In the monitoring snapshot scenario in the production environment, OceanBase Database shows excellent data compression performance, with the space required for a single replica in OceaBase storage being 1/6 that in MySQL storage.\\n- When OceanBase Database is deployed in standalone mode and is connected through OBProxy, it achieves a minimum QPS value of over 10,000 and a minimum average latency of 3 ms, which are slightly inferior to those of MySQL Database. However, the query performance of OceanBase Database increases as the memory size increases. Performance indicators of OceanBase Database show greater improvement than those of MySQL Database as the DOP increases. When the DOP exceeds 200, the performance of OceanBase Database approaches and can even exceed that of MySQL Database.\\n- The MySQL architecture has only one layer, while OceanBase has two, including the OBProxy layer and the OBServer node layer. In standalone deployment, the performance of OceanBase Database is 30% to 50% higher when the OBServer nodes are directly accessed than when they are accessed through OBProxy. This is because each additional layer incurs additional network latency.\\n\\n## Why did we choose OceanBase Database and what sets it apart?\\n\\nFirst, OceanBase Database provides an integrated architecture that supports both standalone and distributed deployment modes. What does that mean? A standalone database, like MySQL, can achieve low latency and high performance, while a distributed database supports easy scaling. What problems can these two types of databases address?\\n\\nIn the early business stage, when your data volumes are relatively small, a standalone database like MySQL can offer outstanding performance. Then, when your business starts growing rapidly, a distributed database that supports transparent scaling with almost unlimited capacity can allow you to easily migrate data without code changes or downtime while ensuring high performance.\\n\\nSecond, OceanBase Database, as a native distributed database, naturally supports automatic sharding and migration, load balancing, and other scaling capabilities, enabling transparent scaling without interrupting or affecting your business. Based on the Paxos protocol, OceanBase Database V4.x is further optimized to achieve an RPO of 0 and an RTO of less than 8s, ensuring high system availability.\\n\\nThird, compared to MySQL Database, OceanBase Database ensures high database performance by minimizing the overhead of the distributed architecture and reduces storage costs by over 80% with a high compression ratio. In addition, the multitenancy feature of OceanBase Database is perfectly suited to SaaS customers, offering easy resource isolation and capacity scaling.\\n\\nIn a distributed database, data processing involves the interaction of memory, disks, and networks. The latency of data reading and writing in memory is 0.1 microseconds, the latency of SSD reading and writing is about 0.1 milliseconds, and the network latency within an IDC is about 0.1 milliseconds, while the network latency across IDCs in the same city is about 3 milliseconds. On the whole, the latency of SSD reading and writing as well as the network latency are about 100 to 1000 times greater than the in-memory read and write latency. The in-memory read and write throughput can reach 100 Gbit/s, the SSD read and write throughput is about 1 Gbit/s to 2 Gbit/s, and the read and write throughput over a 10 Gigabit network is about 1.2 Gbit/s. The SSD and network read and write throughputs are about 100 times less than the in-memory read and write throughput, a difference of two orders of magnitude.\\n\\nAs a standalone database, MySQL places the InnoDB and Server layers in the same process, and therefore offers highly efficient data interaction, making it the undisputed leader in performance and latency. However, for distributed databases in a computing-storage separated architecture, network I/O overheads between the computing and storage layers are inevitable. It is very difficult to mitigate the resulting performance restriction. The unique architecture design of OceanBase Database implements the SQL engine, storage engine, and transaction engine in one process. That is, an OBServer node does both computing and storage. When an application is connected to the OceanBase cluster by using OBProxy, the OBServer nodes report the data routing information to OBProxy. After OBProxy receives an SQL statement from the application layer, it directly forwards the SQL statement to the most appropriate OBServer node for execution based on the routing information. If the data is on one OBServer node, the SQL statement is executed in standalone mode, just like MySQL. This minimizes the network I/O overhead.\\n\\n![](https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/9fb9fc45c8819738607f8b3daabf23a16c12308f)![1693993921](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993921469.png)\\n\\n### Why does OceanBase Database perform better?\\n\\nOceanBase Database significantly outperforms MySQL Database when working with large data volumes or high concurrency. To find out why, we conducted an in-depth study of its architecture. Here are the key points that contribute to the high performance of OceanBase Database:\\n\\nFirst, OceanBase Database offers both low latency and high throughput. For production business data, the proportion of single-server transactions in OceanBase Database can reach more than 80%. This is because, in OceanBase Database, data sharding is performed at the table or partition granularity. Therefore, if we update a non-partitioned table, or a single partition of a partitioned table, OceanBase Database can implement single-server transactions with low latency. Moreover, transactions involving multiple tables in the same server are also executed in single-server mode.\\n\\nSecond, OceanBase Database allows users to use table groups to turn cross-server join operations into single-server transactions. The sound partition granularity design can ensure that 80% of transactions are single-server transactions. The proportion can be further improved by optimizing high-frequency cross-server join operations with table groups. The performance of the database will be superb if more than 90% of all business transactions are single-server transactions.\\n\\nThird, OceanBase Database distinguishes query priorities. Small queries are given the top priority. Large queries occupy up to a percentage of the worker threads as defined in the `large_query_worker_percentage` parameter. When there are no small queries, large queries can take 100% of the worker threads. The overall mechanism is similar to highway traffic rules, with large vehicles allowed to take the rightmost lane only, allowing other vehicles to overtake them in the passing lanes. This mechanism can prevent slow SQL queries and large queries from congesting the system or causing it to crash.\\n\\nThese architecture designs and practical optimization ensure the high performance of OceanBase Database. So how does OceanBase Database offer higher performance at lower costs?\\n\\n### Why does OceanBase Database cost less while offering superior performance?\\n\\nOceanBase Database uses the log-structured merge-tree (LSM-tree)-based storage engine, which supports both compression through encoding and general compression to offer a high compression ratio. As you can see in the following figure, our test data shows that it uses 75% less cluster storage space than MySQL Database.\\n\\n![](https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/c8607d66d96e86236d749dbaa9e3769f9a0e32b7)![1693993956](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993956548.png)\\n\\nAs the business of Dmall grows, our data is increasing rapidly, and the number of nodes is increasing exponentially. The costs of a MySQL system will soon exceed those of OceanBase Database. As shown in the figure below, the compression ratio of 6:1 is verified by the test conducted in our production environment. Our business data will continue to grow in the future. As OceanBase Database supports unlimited new nodes, its storage costs will grow much slower than those of MySQL Database.\\n\\n![](https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/41f4c6bb40ce230e482c27e6385fb746232085ef)![1693993976](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993976054.png)\\n\\n### Multitenancy and resource isolation capabilities of OceanBase Database\\n\\nThe multitenancy feature of OceanBase Database perfectly suits the SaaS scenario because it provides resource isolation among tenants and fast elastic scaling of tenants.\\n\\nResource isolation among tenants: OceanBase Database tenants are physically isolated from each other in terms of CPU, memory, and I/O resources. This ensures zero resource preemption among different businesses and ensures that issues in one business will not affect other tenants.\\n\\nFast elastic scaling for tenants: Assume that a tenant has three zones, with each zone having two nodes. The tenant has a total of six nodes, and each node has one resource unit. To scale up the tenant, you only need to execute one SQL statement. For example, you can add Zone 4 and Zone 5 to scale the tenant up from 6 resource units to 10, implementing horizontal scaling with ease. Vertical scaling is simple to carry out as well. For example, if you start out with 2 CPU cores and 8 GB of memory and do not want to add nodes, you can scale up the tenant to 6 CPU cores and 12 GB of memory without adding any nodes. The whole process is dynamic and lossless without affecting or interrupting your business. Vertical scaling requires the DBA to run only a single SQL statement, greatly reducing the workload. Therefore, the multitenancy feature perfectly meets our need for a new system for SaaS businesses that is cost-saving and easy to expand.\\n\\n![](https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/c75160228d41e619d04531f05b1b71937a3a64a3)![1693993992](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993992089.png)\\n\\n## Summary\\n\\nAs a SaaS service provider, Dmall faces many pain points, such as massive databases, fast data growth, high resource costs, and complex O&M. Distributed databases provide excellent support for our business growth, improve development efficiency, and alleviate the DBA workload. Distributed databases are the path of advancement for database technology. We verified the advantages of OceanBase Database in scalability, performance, and cost effectiveness by running our own tests. Based on our current business development needs and the multitenancy capability that suits our retail SaaS scenario, we are sure that we will continue to expand our cooperation with OceanBase in the future."},{"id":"sichuan-hwadee","metadata":{"permalink":"/blog/sichuan-hwadee","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/user-hwadee.md","source":"@site/blog/user-hwadee.md","title":"Sichuan Hwadee \u2014\u2014 The practice of lightweight data warehouse construction of health big data based on OceanBase","description":"Introduction: This article introduces Sichuan Hwadee\'s practice of migrating its data computing platform from Hadoop to OceanBase Database. This case demonstrates the advantages of OceanBase Database in terms of performance and storage costs. With OceanBase Database, hardware costs are reduced by 60% and the O&M work is significantly cut down, relieving maintenance personnel from responsibility for the many Hadoop components. OceanBase Database enables Hwadee to meet the needs of hybrid transaction/analytical processing (HTAP) scenarios with just one system, simplifying O&M for the company.","date":"2024-06-04T09:30:23.000Z","tags":[{"inline":true,"label":"User Case","permalink":"/blog/tags/user-case"}],"readingTime":12.875,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"sichuan-hwadee","title":"Sichuan Hwadee \u2014\u2014 The practice of lightweight data warehouse construction of health big data based on OceanBase","tags":["User Case"]},"unlisted":false,"prevItem":{"title":"DMALL \u2014\u2014 a summary of database selection experience in SaaS scenarios","permalink":"/blog/dmall"},"nextItem":{"title":"KUAYUE EXPRESS \u2014\u2014 In the real-time warehouse scenario, utilizing OceanBase to make up for the shortcomings of MySQL and StarRocks","permalink":"/blog/kuayue-express"}},"content":"> **Introduction:** This article introduces Sichuan Hwadee\'s practice of migrating its data computing platform from Hadoop to OceanBase Database. This case demonstrates the advantages of OceanBase Database in terms of performance and storage costs. With OceanBase Database, hardware costs are reduced by 60% and the O&M work is significantly cut down, relieving maintenance personnel from responsibility for the many Hadoop components. OceanBase Database enables Hwadee to meet the needs of hybrid transaction/analytical processing (HTAP) scenarios with just one system, simplifying O&M for the company.\\n\\n> **About the author:** Xiang Ping, the Technical Director of the Smart Healthcare and Elderly Care R&D Department of Sichuan Hwadee Information Technology Co., Ltd., is responsible for big data and AI architecture design and team management in the smart healthcare and elderly care sector.\\n\\n\x3c!-- truncate --\x3e\\n\\n## I. Mining the value of medical data with a data computing platform\\n\\nAs the Chinese population grows older, care for the elderly is an increasingly important topic for our society. Providing healthcare for the elderly is a demanding job calling for wisdom, resources, and effort from society as a whole.\\n\\nWe, Sichuan Hwadee Information Technology Co., Ltd. (hereinafter referred to as \\"Hwadee\\"), have created a big data public service platform for smart medical care called Qijiale by integrating and innovating next-generation information technologies such as big data, cloud computing, Internet of Things, AI, and mobile Internet. We cooperate with the local governments and competent government departments with jurisdiction over project demo sites to explore and create a new smart healthcare model that joins efforts of institutions, communities, and enterprises. Through these efforts, we hope to provide professional, efficient, convenient, and safe healthcare services for the elderly by drawing on the collective efforts of residential communities, relatives of the elderly, elderly care institutions, medical institutions, medical schools, local governments, science and technology enterprises, life service institutions, and other groups.\\n\\n![1697622574](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622574673.png)\\n\\nThe Qijiiale platform is a resource integration platform that brings together medical and nursing services. It provides a work, service, and publicity platform for health information and warnings, assistance in chronic and geriatric disease diagnosis, integrated medical and nursing services, health knowledge learning, and other features. It establishes comprehensive and professional public service networks for healthcare and elderly care at the provincial, municipal, county (district), township (street), and village (community) levels and creates a multi-level intelligent healthcare service system that covers health data collection, big data analysis and early warning, intervention services, and performance evaluation. Qijiiale establishes a multi-scene healthcare service model that focuses on the elderly by combining efforts of families, communities, medical care institutions, hospitals, and governments. By providing intelligent modern community healthcare services in real time in a dynamic and continuous manner based on real names, the platform links healthcare information with healthcare resources by connecting people, data, and devices.\\n\\nWe have created a data resource pool based on the medical data, elderly care data, and industrial data collected by hospitals and governments, and uses a big data system to store and compute resources. We have also established a data computing platform with powerful processing capabilities and high scalability. The data computing platform can store, cleanse, process, model, and analyze massive data, making full use of every piece of data in the resource pool to aggregate data according to a wide range of metrics. The platform provides ample reference data for statistical decision analysis, algorithm analysis services, big data prediction, and other applications, helping the company mine more value from its medical data resources.\\n\\n## II. Technology selection to address pain points of Hadoop applications\\n\\nCurrently, we have accumulated a total of about 20 TB of data. The following figure shows our early-stage data computing platform that we built based on the Hadoop ecosystem.\\n\\n![1697622688](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622688331.png)\\n\\nWe encountered many problems in using and operating this data computing platform, such as excessive components, complex builds, and high O&M costs. The most critical problem was that this complex environment was difficult to promptly troubleshoot when a fault occurred.\\n\\n![1697622700](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622700907.png)\\n\\nTo solve these pain points, we looked into distributed databases. By reading the OceanBase official documentation along with blogs and Q&A in the open source community, we learned that OceanBase Database supports thousands of OBServer nodes in a single cluster. It can support nearly a petabyte of data in a single cluster and trillions of rows in a single table. Real-time online transactions and real-time data analysis can be supported with the same set of data and the same engine. Multiple replicas of a dataset can be stored in different formats for different workloads. Moreover, OceanBase provides an automatic migration tool, OceanBase Migration Service (OMS), which supports migration assessment and reverse synchronization to ensure data migration security.\\n\\nWe found that the open source ecosystem products of OceanBase could meet our data scale and data computing platform needs, so we conducted preliminary tests. We immediately took note of the HTAP capabilities of OceanBase Database, and also the following five capabilities, which are critical to us.\\n\\n- Easy O&M\\n\\n  The OBServer nodes are peer-to-peer, and each contains a storage engine and a computing engine. OceanBase Database boasts a small number of components, simple deployment, and easy O&M. With OceanBase Database, we no longer need to add other components to the database to achieve high availability and automatic failover.\\n\\n- High compression ratio\\n\\n  OceanBase Database organizes data based on the Log-structured merge-tree (LSM-tree) architecture, where the full data consist of baseline data plus incremental data. Incremental data is first written to a MemTable in memory, achieving a write performance comparable to that of an in-memory database. The baseline data is static and will only change at a major compaction. Therefore, a more aggressive compression algorithm can be used, reducing the storage space required by at least 60%. After we migrated the data from Oracle Real Application Clusters (RAC) to OceanBase Database, the disk capacity required is only 1/3 the original size, even when we adopt a three-replica storage mode. For more information about the core data compression technologies of OceanBase, you can read [Save at least 50% of the storage costs for history databases with core data compression technologies of OceanBase](https://open.oceanbase.com/blog/5269500160).\\n\\n- High compatibility\\n\\n  At present, OceanBase Database Community Edition is almost perfectly compatible with MySQL syntaxes and features. Most statistical analysis tasks can be completed by using SQL statements. OceanBase Database also supports advanced features that we commonly use, such as stored procedures and triggers.\\n\\n- High scalability\\n\\n  OceanBase Database provides linear scalability. DBAs can simply execute a single command to add server nodes and achieve linear performance scale-out. After new nodes are added to an OceanBase cluster, the system automatically rebalances the data among nodes. DBAs no longer need to migrate data manually.\\n\\n- High availability\\n\\n  OceanBase Database natively supports high availability. It uses Paxos to achieve high availability at the partition level. When a minority of nodes fail, it can still provide services, so your business is not affected.\\n\\n## III. Benefits and issues arising from migration from Hadoop to OceanBase Database\\n\\n### i. Changes in architecture\\n\\nOur original data computing platform was a Hadoop environment that was deployed on 10 servers and used more than 20 different open source components. These components carried out tasks such as data import and export, data cleansing, and analytical processing. We first used the extract-transform-load (ETL) tool to transfer raw data to Hadoop Distributed File System (HDFS), then used Hive commands to load the data, and finally used Spark SQL for data analysis.\\n\\nUnder this architecture, data needed to be transferred back and forth. Moreover, we needed professionals to work intensively on version adaptation and performance tuning for these components. The key problem lay in troubleshooting. Due to the excessive components and long links, we had a hard time quickly finding the faulty component.\\n\\nAt first, we just wanted to use OceanBase Database to integrate and cleanse data: We used a dedicated line to pull data to the front-end machine (Oracle RAC) and then used the ETL tool DataX to pull data from the front-end machine to OceanBase Database. Then, we decrypted, cleansed, and integrated the data in OceanBase Database. Finally, we pulled the cleansed data from OceanBase Database to Hadoop for analytical processing. You can see the full process in the figure below.\\n\\n![1697622871](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622871684.png)\\n\\nLater, from the OceanBase official website, we learned that OceanBase Database supported HTAP capabilities. So we tried analytical processing on the data directly in an OceanBase cluster with three nodes. We were surprised to see that even if the data was not partitioned in OceanBase and parallel execution was not used, we were able to complete analytical processing on 0.5 billion rows of data in less than a minute.\\n\\nAnalytical processing in an OceanBase cluster with three nodes outperformed Hadoop that had 10 servers of the same specification. To our surprise, OceanBase Database supports SQL syntax directly. We do not need to load the data into Hive and then use Spark SQL for analysis, nor do we need to use various open-source components. This simplifies the data computing platform link, as shown in the following figure.\\n\\n![1697622887](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622887380.png)\\n\\nWe initially planned to import data into OceanBase Database for data integration, and then import it back to Hadoop. However, we found that the entire downstream Hadoop cluster was useless. Moreover, as a distributed database, OceanBase Database supports horizontal scaling. This led us to abandon Hadoop completely.\\n\\nWe haven\'t yet done any performance tuning for OceanBase Database. In the future, we will use it to perform analytical processing on 10 billion data rows in more data environments, and then further study its partitioning and parallel execution features.\\n\\nAlso, we have changed our data computing platform to an OceanBase cluster consisting of 4 servers, with one OceanBase Cloud Platform (OCP) server and three OBServer nodes with the following specifications.\\n\\n| Server          | Operating system  | Memory | CPU      | Disk space |\\n| --------------- | ----------------- | ------ | -------- | ---------- |\\n| OCP             | CentOS 7 (64-bit) | 64 GB  | 20 vCPUs | 2 TB       |\\n| OBServer node 1 | CentOS 7 (64-bit) | 64 GB  | 20 vCPUs | 2 TB       |\\n| OBServer node 2 | CentOS 7 (64-bit) | 64 GB  | 20 vCPUs | 2 TB       |\\n| OBServer node 3 | CentOS 7 (64-bit) | 64 GB  | 20 vCPUs | 2 TB       |\\n\\nAfter data is imported to OceanBase Database by using the ETL tool, data decryption, cleansing, aggregation, and analytical processing are all completed in OceanBase Database, and some performance improvements are made for analytical processing. The OceanBase cluster consists of only the deployed cluster management tool OCP and OBServer nodes, and the OBServer nodes are completely equivalent to each other, greatly reducing the O&M complexity.\\n\\n![1697622917](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622916987.png)\\n\\n> Terms:\\n\\n> 1\\\\. Operational data store (ODS): The ODS layer stores and directly loads raw data without processing the data.\\n\\n> 2\\\\. Data warehouse (DW): The DW layer stores structured and aggregated clean data to support decision-making and data analysis services for enterprises.\\n\\n> 3\\\\. Data warehouse detail (DWD): The DWD layer cleanses data at the ODS layer by removing empty values, dirty data, and data that does not meet the metadata standards. It is used to store detailed, complete data that is used for cross-departmental and cross-system sharing and enterprise data query.\\n\\n> 4\\\\. Data warehouse summary (DWS): The DWS layer provides business data summary and analysis services, and computes, aggregates, and processes raw data for enterprise decision-making.\\n\\n> 5\\\\. Application data service (ADS): The ADS layer stores custom statistical metrics and report data for data products.\\n\\n> 6\\\\. Data mart (DM): The DM is a collection of data that is focused on a single subject and separated from the data warehouse for a specific application. It provides a clear, targeted, and scalable data structure.\\n\\n![1697622998](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622998311.png)\\n\\nFigure: Overall technical architecture\\n\\n### ii. Storage costs\\n\\nWe tested the data import and export performance of an Oracle cluster and an OceanBase cluster each deployed on five servers of the same specifications, with a 372 GB data file containing 0.5 billion rows of data. For data import, we compared the storage space required by the imported data:\\n\\n- The data file occupies 220 GB of storage space after it is imported to the Oracle cluster.\\n- After the same data file is imported to the OceanBase cluster by using OBLOADER, it occupies only 78 GB of storage space, even when it is stored in three replicas.\\n\\nIn short, even when storing data in three replicas, OceanBase Database requires only about 1/3 to 1/4 of the storage space required by Oracle Database.\\n\\n### iii. Ecosystem tools\\n\\nOceanBase has a robust ecosystem that provides more than 400 upstream and downstream ecosystem products and in-house tools such as OCP and OBLOADER & OBDUMPER.\\n\\n![1697623053](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697623053024.png)\\n\\n**1\\\\. OCP**\\n\\nOCP is an O&M management tool that provides visual performance monitoring.\\n\\n![1697623067](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697623067245.png)\\n\\n**2\\\\. OBLOADER & OBDUMPER**\\n\\nOBLOADER & OBDUMPER is a data import and export tool for logical backup and restore of data. It takes OBDUMPER only several dozen minutes to generate a backup file for 0.5 billion rows of data, with the backup file sized about 400 GB. OBLOADER & OBDUMPER is easy to get started with, and allows you to import and export data with commands.\\n\\n![1697623086](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697623086269.png)\\n\\n**3\\\\. Integration and fusion with other open source products**\\n\\nOceanBase Database Community Edition has implemented in-depth integration and fusion with more than 400 upstream and downstream products in the ecosystem, such as Flink, Canal, Otter, and DataX. We find this very convenient. For example, we can use DataX to extract data to an OceanBase cluster. Our cluster runs a total of 168 ETL tasks in real time.\\n\\n![1697623102](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697623102230.png)\\n\\n**4\\\\. Some usage issues encountered**\\n\\n- When we manage stored procedures in OceanBase Developer Center (ODC), the `REPLACE PROCEDURE` command is provided but unavailable. To replace a stored procedure, we have to drop it and then create a new one. However, we often need to modify only a small part of a large stored procedure, so we hope that ODC can support `REPLACE PROCEDURE` to make it easier to modify stored procedures.\\n- The earlier OceanBase Database Community Edition V3.x does not support DBlinks and does not allow access to and manipulation of tables, views, and data in another database. DBlinks are supported in OceanBase Database Community Edition V4.x.\\n- The deployment was cumbersome because OceanBase Database Community Edition did not provide the one-click installation package OceanBase All in One at that time.\\n\\n## IV. Summary\\n\\nWe have been paying close attention to OceanBase since 2021 and have used OceanBase Database Community Edition since its initial open source version V3.x. We have verified the feasibility of using OceanBase Database to directly analyze 20 TB of data in real business scenarios. A task that originally required 10 servers can now be completed with only 4 servers, reducing our hardware costs by 60% and freeing engineers from the demanding O&M work caused by the many components of Hadoop.\\n\\nOceanBase Database supports online analytical processing (OLAP) and online transaction processing (OLTP) workloads with one set of engines. It not only meets our analytical processing performance requirements, but also simplifies O&M and greatly reduces the costs. The advantages of OceanBase Database in performance and storage costs are proven in our business environment.\\n\\n![1697623167](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697623167264.png)\\n\\nIn the future, we will strengthen our cooperation with OceanBase and try to use the wide array of peripheral tools in the OceanBase open source ecosystem to create enterprise-level products."},{"id":"kuayue-express","metadata":{"permalink":"/blog/kuayue-express","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/user-kuayue-express.md","source":"@site/blog/user-kuayue-express.md","title":"KUAYUE EXPRESS \u2014\u2014 In the real-time warehouse scenario, utilizing OceanBase to make up for the shortcomings of MySQL and StarRocks","description":"Introduction: This article is based on the speech made by Zhang Jie, Big Data Architect of KUAYUE EXPRESS (hereinafter referred to as KYE), at the DTCC conference. The speech introduced the pain points faced by KYE in data analysis and the company\'s ideas and practices in the development of its query engine solution.","date":"2024-06-04T09:30:23.000Z","tags":[{"inline":true,"label":"User Case","permalink":"/blog/tags/user-case"}],"readingTime":14.765,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"kuayue-express","title":"KUAYUE EXPRESS \u2014\u2014 In the real-time warehouse scenario, utilizing OceanBase to make up for the shortcomings of MySQL and StarRocks","tags":["User Case"]},"unlisted":false,"prevItem":{"title":"Sichuan Hwadee \u2014\u2014 The practice of lightweight data warehouse construction of health big data based on OceanBase","permalink":"/blog/sichuan-hwadee"},"nextItem":{"title":"Momo \u2014\u2014 Exploration and practice of persistent cache based on OceanBase KV storage","permalink":"/blog/momo"}},"content":"Introduction: This article is based on the speech made by Zhang Jie, Big Data Architect of KUAYUE EXPRESS (hereinafter referred to as KYE), at the DTCC conference. The speech introduced the pain points faced by KYE in data analysis and the company\'s ideas and practices in the development of its query engine solution.\\n\\nHello everyone. I\'m Zhang Jie, a Big Data Architect at KYE. Today, I want to talk about how we selected our query engine for HTAP scenarios. My talk will be divided into four parts:\\n\\n1. Business background and the pain points encountered in daily data analysis in the logistics industry\\n2. Our considerations and product testing for query engine selection\\n3. Our benefits from using OceanBase Database\\n4. Our experience in using OceanBase Database Community Edition V4.0\\n\\n\x3c!-- truncate --\x3e\\n\\n**Business background of daily data analysis in the logistics industry**\\n\\nIn the Internet industry, data may be directly oriented to consumers and end users. But at KYE, we mainly serve our internal employees. Every day, more than 100 BI developers intensively use our big data platform for development. We have built more than 10,000 data service interfaces, which are called by various production systems through gateways. We handle tens of millions of calls per day, and the various data services we provide support the daily work of more than 50,000 employees throughout the entire group. To ensure a good user experience, we have high requirements for interface latency. Basically, the 99th percentile latency is required to be less than 1 second.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/1a31d7a2-e247-43fb-bce9-46ca43585e11/image/2022-12-28/1258e2ef-7ee1-456c-bc95-5284d117052b.png)\\n\\nIn addition to online analytical processing (OLAP) scenarios, our data services also provide key data in core business processes, such as performance and salary query, waybill cost allocation, and real-time waybill tracking. The actual revenue of each waybill is not fixed. Therefore, a large number of users, such as drivers and operators, query their daily revenue by using our interfaces, including the costs of each waybill and the real-time tracking information of each waybill. All of this data comes from the big data platform.\\n\\nIn the logistics industry, core analysis scenarios all concern waybills. Waybill analysis requires data from dozens of upstream enterprise resource planning (ERP) systems, including transportation management, customer management, and performance management systems. This data is converged on the big data platform. The big data platform collects, merges, and calculates all upstream data related to waybills, qualifies the data for waybill domains of our data warehouse through complex analysis and processing, and then provides the data to the users of various platforms and services.\\n\\nWith the rapid development of the company business, database application scenarios are becoming more and more complex. In the early stage, users queried results based on fixed parameters, for example, by day, week, or month, or with fixed conditions. Back then, we could preprocess data and place the data aggregation results in MySQL Database so that users could directly find the results they were looking for. Today, user needs are becoming increasingly complex. They want to query data by any field and any time range. In this case, MySQL Database no longer meets our needs.\\n\\n**Our business scenario has changed in the following aspects:**\\n\\n1. More table joins\\n\\n   In the past, we could prepare a single wide table in advance to serve users. As requirements change more frequently, a query interface may involve dozens of table joins.\\n\\n2. Higher requirement for real-time performance\\n\\n   In the past, users could accept our offline and batch data updates. Now they require real-time data updates, including the real-time status of each waybill. This imposes higher requirements on real-time data processing for downstream databases.\\n\\n3. Higher requirements for latency\\n\\n   Previously, users could wait 3 to 4 seconds for data results. Now users require a response to data requests within one second.\\n\\nTo address these new business requirements, we need to select a more suitable database solution. To improve user experience and real-time performance, the new database must provide **exceptional query performance** and **support real-time writes, updates, and deletion**. The database must also be easy to use, so **standard SQL support and rich functions** are essential. It would be even better if it was **highly compatible with MySQL**. In addition, the database must allow **data to flow in and out**, and its data integration feature must support data sources in our usage scenarios. **Database stability and maintainability** are also important factors that concern us.\\n\\n**Database tests: performance and feature comparison of five databases**\\n\\nThe following performance testing and feature testing results explain why we chose OceanBase Database from among other distributed databases.\\n\\n**Performance testing**\\n\\nA unified test comparison standard is essential for database evaluation and selection. Common benchmark tests in the industry include TPC-H, TPC-DS, and SSB. Every database product provides these benchmark test results in the official documents. We used these test results for reference in database selection. However, we cannot solely rely on these test results because they do not account for our business characteristics and the test sets are usually specially optimized for the tests. Therefore, we built a set of benchmark standards based on our own data analysis requirements. This set of benchmark standards adopts a unified test model and environment, and defines unified tests based on the SSD disks of Alibaba Cloud.\\n\\nWe prepared a standard data set of more than a dozen tables related to waybills, and compiled a set of standard SQL statements in accordance with our daily application scenarios based on actual waybill analysis cases. We also developed a feature test set based on our actual needs. Then we carried out benchmark tests with these test data sets on different databases available on the market.\\n\\nAfter preliminary screening, we selected five popular commercial query engines for testing and comparison. They were TiDB, OceanBase, StarRocks, Doris, and Trino. The following figure shows the performance test results.\\n\\n| SQL No | SQL description                                                             |\\n| ------ | --------------------------------------------------------------------------- |\\n| SQL 21 | SQL heavily relying on CPU for computation                                  |\\n| SQL 19 | Multi-table join aggregation with high cardinality in Snowflake schema      |\\n| SQL 18 | Multi-table join aggregation with high cardinality in Star schema           |\\n| SQL 17 | Multi-table join point query in Snowflake schema                            |\\n| SQL 16 | Multi-table join point query in Star schema                                 |\\n| SQL 15 | Large table join aggregation with high cardinality                          |\\n| SQL 14 | Large table join aggregation with low cardinality                           |\\n| SQL 13 | Large table and small table join aggregation with high cardinality          |\\n| SQL 12 | Large table and small table join aggregation with low cardinality           |\\n| SQL 11 | Large table and small table join point query with index miss                |\\n| SQL 10 | Large table and small table join point query with index hit                 |\\n| SQL 9  | Range scan on a non-indexed datetime column                                 |\\n| SQL 8  | Aggregation on high cardinality column                                      |\\n| SQL 7  | Aggregation on low cardinality column                                       |\\n| SQL 6  | Window functions                                                            |\\n| SQL 5  | ORDER BY with LIMIT                                                         |\\n| SQL 4  | ORDER BY                                                                    |\\n| SQL 3  | Point query with filter condition as IN clause containing a thousand values |\\n| SQL 2  | Point query with index miss                                                 |\\n| SQL 1  | Point query with index hit                                                  |\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/08b1c279-4af3-4f55-aa09-9e4e98c56bb0/image/2022-12-28/6d1cb039-34c7-48f4-a75d-669e358e0eee.png)\\n\\nWe can see that, in pure OLAP scenarios, StarRocks has the best performance. To our surprise, OceanBase Database came in second. Before we tried OceanBase Database, we tested TiDB, whose performance was far inferior to StarRocks. Therefore, we didn\'t expect much from the performance of HTAP databases. However, after the testing on OceanBase Database was completed, we unexpectedly found that its performance in OLAP scenarios was also good.\\n\\n**Feature testing**\\n\\nIn addition to the performance tests, we also tested and verified the general features of the databases. The test results show that HTAP databases have richer features, and one HTAP database can be basically considered a collection of several distributed MySQL databases. Despite their slight differences, these HTAP databases all support updates, deletion, indexed column customization, and consistency guarantees.\\n\\n- **Big data ecosystem integration**. Our business scenarios primarily involve waybill analysis. Therefore, we must consider the integration of databases with big data platforms. In this regard, StarRocks and Doris are the clear winners.\\n- **Maintainability.** We are primarily concerned with online expansion, online upgrade, automatic balancing, resource isolation, and management tools. HTAP databases perform better in these areas. Among them, OceanBase and TiDB were the best. For example, OceanBase provides OceanBase Cloud Platform (OCP), and TiDB provides TiUP. Both can help us easily deploy, upgrade, monitor, and maintain the corresponding databases.\\n\\nAfter comprehensive comparison, we concluded that MySQL is a transaction processing (TP) database that is widely used in our business, and its strengths are stability, transaction processing, and concurrency capabilities. StarRocks is an analytical processing (AP) database that is widely used in our analysis scenarios. StarRocks boasts a distributed architecture, good AP analysis performance, and a high compression ratio due to columnar storage. StarRocks provides a good solution to our requirements in pure analysis business scenarios. However, in addition to TP scenarios and AP scenarios, we have many businesses that require databases with both TP and AP capabilities, such as real-time waybill analysis. HTAP databases, such as OceanBase Database and TiDB, can support these scenarios.\\n\\nWe carried out **a further round of testing and comparison between OceanBase Database and TiDB** and found that:\\n\\n- The performance of OceanBase Database is 4x to 5x higher than that of TiDB in all scenarios.\\n- OceanBase Database has a simpler architecture and integrates storage and computing. It needs only observer and obproxy processes. However, TiDB requires multiple services such as PD, TiDB, TiKV, and TiFlash, resulting in higher complexity during subsequent maintenance and troubleshooting.\\n- OceanBase Database is more advantageous in data storage space because it supports hybrid row and column storage and supports AP and TP scenarios with the same set of data. In contrast, TiDB needs another set of data stored in the columnar format generated with TiFlash to support AP scenarios, so the data storage space is doubled.\\n\\nIn view of this, we chose OceanBase Database to help us solve business pain points in real-time analysis scenarios.\\n\\n**Our benefits from using OceanBase Database**\\n\\nAfter we decided to use OceanBase Database, we carried out the following tasks.\\n\\n**First, we verified the data integration links**\\n\\n**Verification of offline data synchronization.** We use the Sqoop tool to synchronize data from Hive to OceanBase Database in batches. When we write data to a 220-field wide table in a 3-node OceanBase cluster at a degree of parallelism (DOP) of 50, the write performance is 2 million rows per minute, which meets our requirements for offline batch data synchronization.\\n\\n**Verification of real-time data writing.** Based on our real-time computing platform, we use Flink JDBC Connector to write upstream data from Kafka and other sources into OceanBase Database in real time. The real-time write performance is 0.15 million rows per minute for a 220-field wide table with 10 partitions. Limited by the implementation of the JDBC connector, the real-time performance is far inferior to offline performance, but can still meet our requirements for real-time synchronization of incremental data. The OceanBase community will launch Flink OceanBase Connector in January 2023, which has been comprehensively optimized in terms of write and concurrency performance. We will verify it as soon as possible. We believe that its performance will be better than that of the native Flink JDBC Connector.\\n\\nOceanBase also provides OceanBase Migration Service (OMS), which can synchronize incremental data to Kafka in the Canal format in real time, so that downstream systems can consume the incremental data. The AP database that we are currently using does not support this feature. It can be of great help to us in secondary real-time data computing as well as data backup and disaster recovery.\\n\\n**Second, we explored upgrading the waybill analysis architecture**\\n\\nAfter data link verification, we run a trial to upgrade the waybill analysis architecture. The following figure shows our original waybill analysis architecture, which was basically an offline processing architecture. The basic upstream waybill data was synchronized from binlogs to Kafka through Canal. The data of multiple topics was written into different Hive tables through our in-house platform. The tables were merged and processed on a two-hourly or daily basis to generate a final waybill wide table for business teams.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/f981915a-ab34-4739-b160-01db66a38944/image/2022-12-28/6a627b5a-8bd0-40a3-9231-1dd51abedfa0.png)\\n\\nThis architecture had two main problems: First, the timeliness of data was poor. Data was processed offline, so it was already at least two hours old. Second, the analysis performance was poor. We carried out waybill analysis based on Presto. The analysis generally took about 1 to 10 seconds, and even longer for more complex analysis. This was hard on our business teams, but we were limited by the technology at that time.\\n\\nCurrently, we use an upgraded architecture based on this earlier version, with HBase carrying out real-time aggregation of waybill wide tables and the CDC capability of HBase synchronizing the merged data to StarRocks in real time. This is a significant improvement. Offline data is turned into real-time data, and the performance of real-time waybill analysis based on StarRocks has also been greatly improved.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/3ec17c7c-652f-4964-8a6f-058a22537e64/image/2022-12-28/1d94b7d2-204d-4f90-8874-426b107ca4f3.png)\\n\\nHowever, we encounter several problems after the launch of the upgraded architecture:\\n\\n- The solution is too complex and hard to reuse.\\n- The data link is long, resulting in difficulties in troubleshooting data synchronization problems.\\n- The maintenance costs are high.\\n\\nTherefore, we tried to optimize the current architecture with the AP capabilities of OceanBase Database. We use OceanBase Database to help us further solve problems in real-time waybill analysis. During the test, we used Flink JDBC Connector to directly write the upstream waybill data of multiple tables into OceanBase Database, and merged the multi-field data of the waybill wide tables in OceanBase Database in real time.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/52ff144b-0eae-4c4a-9ab3-caad91f4a341/image/2022-12-28/15bd01c5-0e8b-46b1-a4cf-8a85c15843e3.png)\\n\\nBased on the excellent AP capabilities of OceanBase Database, we connected the waybill wide tables to the external analysis system. The analysis performance was unchanged. The analysis of a 220-field wide table with 20 million rows by using a complex SQL statement took about 4 seconds, and could be further accelerated. Upon testing, we found that we no longer need a complete set of HBase systems, so we were able to **greatly simply the architecture for real-time waybill analysis, reducing the number of components by a third and cutting costs by half.**\\n\\n- The number of components required for data merging is significantly reduced. This shortens the data link by half and increases the timeliness of data processing. For example, a query that took 5 seconds before now takes only 2 seconds in OceanBase Database.\\n- The data synchronization link is also shortened, facilitating maintenance and troubleshooting.\\n- Server costs are also reduced because a complete set of HBase clusters is no longer required.\\n- The reproducibility of the whole solution has increased, and we can quickly reuse it in other similar analysis scenarios.\\n\\n**Our experience in using OceanBase Database Community Edition V4.0**\\n\\nAfter OceanBase Database Community Edition V4.0 was launched on November 3, 2022, KYE tried out its many new features. This version implements a new distributed cost model and distributed query optimization framework, applies a comprehensive set of parallel pushdown techniques, and enables the development of adaptive techniques. The new version outperforms OceanBase Database Community Edition V3.x significantly, as shown in the following TPC-DS 100 GB benchmark test results.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/92a3f581-1bf7-4e93-92fa-9804bf7ea772/image/2022-12-28/e47eb7bd-9ac2-4d83-83a2-7cccc2da9e1f.png)\\n\\nKYE has also tested OceanBase Database Community Edition V3.2. The following figure shows the test results and comparison of these two versions.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/10f21275-ffec-45e0-bacb-26200484c28a/image/2022-12-28/a4888a1b-47d6-4a13-8c10-2cfb627d69b0.png)\\n\\nWe can see that almost all SQL sets see performance improvements in the new version, and some are improved by 4x to 5x. In some high-base aggregation scenarios, the performance improvement is 3x to 4x. The overall AP performance is improved significantly.\\n\\nCompared with OceanBase Database Community Edition V3.2, the new version delivers optimal performance without requiring much parameter optimization. You only need to take the following simple steps:\\n\\n- Manually perform a major compaction to merge the data and flush it to disk.\\n- Manually collect table statistics one time to increase the accuracy of the execution plan.\\n- Set a reasonable number of partitions and DOP based on the number of CPU cores of the OBServer nodes to give full play to the CPU capacity.\\n\\n**Suggestions for OceanBase**\\n\\nWhen using OceanBase, we also found some issues and reported them to the OceanBase community. The first issue is the compatibility of peripheral tools. Tools such as OBLOADER and OMS are not fully adapted to OceanBase Database Community Edition V4.0. The second issue is the performance in some aggregation scenarios. After analysis, we found that we could solve this issue with hints. According to the OceanBase community, these issues will be resolved in the next version.\\n\\nFinally, I would like to express our gratitude to the OceanBase community team for their help and support.\\n\\nThat\'s all for me. Thank you.\\n\\n> Company Profile: Established in 2007, Kuayue Express Group Co., Ltd. is a modern comprehensive express delivery company specializing in time-sensitive delivery services. It provides delivery on the same day, the next day, the third day, the same day in the same city, the next day in the same city, land transportation, and fresh goods delivery services, as well as 24-hour pickup and delivery and one-to-one exclusive services.\\n\\nFollow us in the [OceanBase community](https://open.oceanbase.com/blog). We aspire to regularly contribute technical information so we can all move forward together.\\n\\nSearch\ud83d\udd0d DingTalk group 33254054 or scan the QR code below to join the OceanBase technical Q&A group. You can find answers to all your technical questions there.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/f4d95b17-3494-4004-8295-09ab4e649b68/image/2022-08-29/00ff7894-c260-446d-939d-f98aa6648760.png)"},{"id":"momo","metadata":{"permalink":"/blog/momo","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/user-momo.md","source":"@site/blog/user-momo.md","title":"Momo \u2014\u2014 Exploration and practice of persistent cache based on OceanBase KV storage","description":"About the author: Ji Haodong, head of the database division at Momo, part of the Hello Group. He is now responsible for the database teams of Momo and Tantan and the database storage and operation throughout the Hello Group. He has a wealth of professional experience and practical know-how in the fields of large-scale data source stability construction, team building, cost optimization, and IDC migration.","date":"2024-06-04T09:30:23.000Z","tags":[{"inline":true,"label":"User Case","permalink":"/blog/tags/user-case"}],"readingTime":13.43,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"momo","title":"Momo \u2014\u2014 Exploration and practice of persistent cache based on OceanBase KV storage","tags":["User Case"]},"unlisted":false,"prevItem":{"title":"KUAYUE EXPRESS \u2014\u2014 In the real-time warehouse scenario, utilizing OceanBase to make up for the shortcomings of MySQL and StarRocks","permalink":"/blog/kuayue-express"},"nextItem":{"title":"Work with developers to create an all-in-one database","permalink":"/blog/all-in-one"}},"content":"About the author: Ji Haodong, head of the database division at Momo, part of the Hello Group. He is now responsible for the database teams of Momo and Tantan and the database storage and operation throughout the Hello Group. He has a wealth of professional experience and practical know-how in the fields of large-scale data source stability construction, team building, cost optimization, and IDC migration.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Business scenario characteristics of social media applications\\n\\nHello Group launched Momo in August 2011. This open-style mobile video social media application is based on geographical location services, unique among the social media platforms in China. As the mainstream social applications that allow strangers to interact, Momo and Tantan involve a variety of core business modules, including livestreaming, nearby events, instant messaging (IM), and value-added services. Each business scenario has its own unique characteristics and challenges.\\n\\nIn this article, Ji Haodong, the head of the database team of Momo, will focus on their experience in choosing a KV system architecture for Momo, giving an in-depth analysis of the decision-making process. He will further share the trials that the Momo team conducted in choosing and using OBKV of OceanBase and discuss their practical experience.\\n\\nLivestreaming holds a prominent place among these business modules. The main characteristic of this module is the possibility of sudden traffic spikes. Due to the requirements for low latency and high concurrency, this module places high demands on the real-time processing capabilities of the database system. The platform needs to ensure that data is processed and distributed in a timely and accurate manner when a large number of users are simultaneously viewing content and interacting online.\\n\\nThe nearby events module involves complex data, including geographic location information, activity trajectories, and social relationships of users. This kind of data accumulates quickly and forms large-scale data sets over time. Data can be classified into hot data and cold data. For example, some data may become hot at a certain time, for example, when a user\'s post creates a lively discussion. This requires the system to effectively manage and quickly respond to hot data access needs.\\n\\nThe core features of IM business scenarios are highly concurrent communication requiring low latency. The delivery time of information must be accurate. Therefore, the requirements for real-time performance are extremely high. To ensure a sound user experience, the application needs to ensure that messages are delivered to users instantly and reliably.\\n\\nValue-added services mainly focus on data consistency and real-time performance. When processing operations such as purchasing and giving virtual gifts or exercising member privileges, the system needs to ensure data accuracy and update user account statuses in a timely manner. At the same time, real-time performance of data is also essential for high-quality value-added services, such as real-time calculation of bonus points, grades, and benefits.\\n\\nWhen operating these businesses, both Momo and Tantan need a powerful data processing and management system to deal with various characteristics and challenges in order to deliver an efficient, stable, and personalized social media experience to users. How should we choose an appropriate KV system for these business scenarios?\\n\\n## KV storage architecture in different business stages\\n\\nCompanies usually have different requirements for storage systems in different development stages.\\n\\nIn the initial stage, the main goal of a company is to start business operations. In the startup stage, the company often needs to carry out rapid iteration on a newly developed app. As a result, they do not have high requirements for the storage system. The storage system is only expected to meet the basic technical needs of the business and then evolve gradually. In this stage, the common choices include the Redis master-slave architecture, Redis Cluster, and other native architectures.\\n\\nThe advantage of the Redis master-slave cluster architecture is that you can quickly build master-slave clusters or sharded clusters and carry out many designs directly on the client. However, this simple operation mode may lead to a high degree of coupling between the design and the client service code, causing difficulties in elastic scaling in the future.\\n\\nIn contrast, the Redis Cluster architecture supports dynamic scaling and high availability. However, with Redis Cluster, the business relies on clients to perceive node changes. If the client fails to handle node changes correctly, service interruption or business performance degradation may occur. Therefore, for error-sensitive business, Redis Cluster may introduce additional complexity. Although Redis Cluster has the advantages of decentralization, fewer components, smart clients, and support for horizontal scaling, they also have some downsides, such as unfriendly batch processing and lack of an effective flow control mechanism.\\n\\nIn the second stage, as the company grows and gains more users, the architecture must support rapid scaling. Basic Redis sharding architectures, such as Codis and Twemproxy, are popular choices for companies in this stage. Among them, Codis provides server-side sharding, centralized management, automatic failover, horizontal node scaling (with 1024 slots), dynamic capacity scaling, and support for pipelines and batch processing. However, Codis only offers an outdated official version V3.2.9. Any updates require much repair and adaption work, and can consume many resources due to the large number of components involved.\\n\\nIn the third stage of company development, as the business further develops and becomes relatively stable, the company may identify issues introduced in the previous stages, such as excessive memory usage and lack of hot and cold data separation. These issues need to be re-examined and addressed. So in the third stage, optimization is the focus. In this stage, common choices are persistence architectures, including oneStore-Pika, Tendis, and Pika.\\n\\nFinally, in the fourth stage, the business and technology of the company become more complex and advanced. Simple optimization and adjustments may no longer provide significant improvements, and further optimization may not be possible. At this point, a more stable architecture or solution may be introduced to respond to these challenges. We recommend that companies at this stage adopt a multi-modal architecture, which can accommodate multiple data types and workload types and thus provide greater flexibility and potential for optimization.\\n\\nIn general, companies must choose a storage system that suits their business needs, technology maturity, cost-effectiveness requirements, and future scalability and optimization demands in their development stages. As companies grow and business complexity increases, storage architectures need to evolve and adapt to ensure system stability, efficiency, and sustainability.\\n\\n## Momo\'s in-house KV storage architecture oneStore\\n\\nIn view of the current business situation of Momo, the most crucial challenge we face is the continuously growing cluster size. When the number of shards in a single cluster exceeded 1000, the data volume exceeded 10 TB, and the QPS exceeded 1 million, the Codis architecture and Redis Cluster architecture no longer met our growing capacity requirements.\\n\\nIn order to break through this bottleneck, we developed a proprietary storage product called oneStore. The following figure shows its architecture. This architecture has undergone a phased optimization and improvement process, aiming to overcome the original limitations to accommodate more shards, larger data volumes, and more intensive query requests. We strive to use the oneStore architecture to achieve seamless business expansion and performance improvement.\\n\\n![1706061557](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061557148.png)\\n\\n**In the first phase, this architecture provides a server proxy scheme.** Our proprietary oneStore Watcher sentinel component is used to streamline the architecture. In this way, only one sentinel cluster needs to be deployed to effectively manage a business domain.\\n\\n**In the second phase, the client SDK solution is provided.** Although the server proxy solution performed well, as the company business stabilized, we aimed to reduce costs and increase efficiency throughout the company. The client SDK solution can be used directly to perceive cluster topology changes and directly connect to the backend Redis endpoint. In this way, the server proxy component can be eliminated, reducing our costs. However, we have not completely abandoned the server proxy scheme. This is because our client SDK solution currently only supports Java and C++, and users of other languages such as PHP and Python still need to access the data source through the server proxy. The successful application of these two solutions helped us unify access to Redis at the company level and significantly improved IDC migration efficiency.\\n\\nAs the business has further stabilized, we began to optimize the architecture in terms of cost. We used Pika to replace some Redis clusters with a low request volume, and then improved the persistence capability of the architecture, as shown in the following figure, while reducing storage costs. However, in this stage, Pika was mainly used to store some relatively cold data, and its performance in processing hot data needed to be further improved. We hope to further improve performance in this aspect.\\n\\n![1706061595](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061595578.png)\\n\\nIn short, the scenarios to be addressed and optimized include:\\n\\n1\\\\. Reciprocal impact among multiple instances on a single server: We urgently need to eliminate the reciprocal impact among different instances on a single server to ensure the stable operation of and efficient cooperation among these instances. Such impact undermines the overall stability and synergy of the system, and must be addressed by targeted optimizations and adjustments.\\n\\n2\\\\. Data persistence support: We plan to enhance data persistence support to form a comprehensive data persistence solution, to ensure data integrity and reliability not just for cold data, but also for a wider range of data types. It will be an important guarantee for the long-term stability of the system.\\n\\nWe need a simple, reliable, and scalable KV system to solve the above problems.\\n\\n## Reliable distributed KV system \u2014 OBKV of OceanBase\\n\\n![1706061629](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061629024.png)\\n\\nOBKV is a module that OceanBase provides for accessing the table and HBase models over APIs. We chose OBKV because it offers the following major advantages:\\n\\n**1\\\\. Better performance**\\n\\nOBKV is built based on a table model and matches the typical table model of the Redis data structure persistence solution. It has better performance than traditional persistent storage architecture and can build richer data structures.\\n\\nThe following figures show the performance of OBKV in different aspects in a massive data writing scenario (with a TPS value of 17000). The TPS curve is quite steep because tasks write data at different stages, and the response delay is within 2 milliseconds. The response time details of the transactions are as expected.\\n\\n![1706061648](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061648498.png)\\n\\nThe following figures show the CPU performance. We can see the CPU utilization remains below 10% in a relatively stable state. The MemStore usage is also within the normal range of below 24% with small fluctuations as expected.\\n\\n![1706061656](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061656861.png)\\n\\nOn the whole, OBKV achieves low fluctuations and stable resource usage in a production environment.\\n\\n**2\\\\. High stability**\\n\\nOBKV is based on OceanBase Database. The storage engine has been verified in a variety of large-scale TP scenarios and can provide high concurrency and low latency. The multitenancy feature ensures system stability, as shown in the following figure. The black curve represents the TPS of a tenant for which OBKV is implemented, while the blue curve represents the TPS of a regular MySQL tenant of OceanBase Database. After the stress test was initiated around 11:30, the tenant for which OBKV is implemented responded normally, and the MySQL tenant was not affected. At the server layer, the CPU load increased due to the stress test, while the MySQL tenant was not affected.\\n\\n![1706061668](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061668943.png)\\n\\nTherefore, it can be concluded that the multitenancy feature can effectively solve reciprocal impact among different instances on the same server.\\n\\nThe following figures show the performance of the online MySQL tenant in a production environment. The TPS value was 5000, and the overall performance was quite stable. The fluctuations in CPU utilization and memory usage were mild and in line with expectations.\\n\\n![1706061680](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061679951.png)\\n\\nIn addition, we can easily use the KV interface to store data in the database and use SQL statements to query data. OBKV further enhances this convenience by supporting secondary indexes and service-side TTL features to help simplify the upper-level service architecture. Nevertheless, OBKV has some limitations. For example, it only provides local transaction processing capabilities. If distributed transaction processing is enabled, it may affect the performance and increase the response latency of the system in a highly concurrent environment. However, in view of the current requirements of Momo\'s business, we believe that the local transaction processing capability of OBKV fully meets our needs. Therefore, we have built a storage solution that combines OBKV and Redis.\\n\\n## OBKV-Redis cluster architecture\\n\\nWe worked with the OceanBase open source team to create a project internally named Modis. The overall architecture of the project covers multiple layers, including the access layer, data structure layer, cache layer, storage layer, and management plane, as shown in the following figure. It is worth noting that the cache layer will be used to effectively cope with the challenges of hot reading and large-key issues in our future plans. At the storage layer, we will build standard storage structures based on the principles of standardization and abstraction, to allow flexible access to a variety of storage solutions, including OBKV.\\n\\n![1706061706](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061706135.png)\\n\\nDuring the test and evaluation process, after we successfully migrated about 158 GB of data from Pika to the OBKV-Redis cluster, the occupied storage space was significantly reduced to 95 GB. The migration cut storage costs by about 40%.\\n\\nTo assess the performance, we built a special test environment with the specifications shown in the figure to simulate various thread concurrency scenarios.\\n\\n![1706061754](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061754200.png)\\n\\nBased on the multitenancy management concept, we do not allocate excessive resources to a single tenant. Instead, we check for performance bottlenecks in each tenant in the test process to calculate the corresponding QPS value of a single core. Currently, the standard specification is 12 CPU cores and 40 GB of memory. To better adapt to future changes in business needs, we may release an architecture with lower specifications, such as 4 CPU cores and 8 GB of memory, or 8 CPU cores and 16 GB of memory, depending on the specific needs of the actual business.\\n\\nThe following figure shows the performance of OBKV-Redis given 128 threads and a QPS value of 70000:\\n\\n- P90 response latency: 1.9 ms;\\n- P95 response latency: 2.2 ms;\\n- P99 response latency: 6.3 ms;\\n\\nOn average, the single-core read-write ratio is 4:1, and the single-core capacity is close to 6000 QPS.\\n\\n![1706061776](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061776775.png)\\n\\nWe also compared the O&M differences among OBKV, Pika, and TiKV. Currently, only OBKV provides native multitenancy support, which effectively solves the problem of reciprocal impact among different instances deployed on a single server. Notably, OBKV is a thoughtful and efficient solution for database O&M personnel because it provides GUI-based management tools and supports immediate effectiveness of parameter changes.\\n\\nIn summary, OBKV-Redis substantially improves performance, reduces disk usage, and greatly simplifies O&M management.\\n\\nThis is made possible by the following advantages of the solution:\\n\\n- **Multi-tenant isolation**, which solves reciprocal impact among multiple instances on a single server.\\n- **Lower storage costs** from the encoding framework and common compression algorithms used for the storage of table models.\\n- **Higher performance** because request filters are pushed down directly to the storage, without serialization or deserialization, and the Time To Live (TTL) feature is supported for servers.\\n\\n## Vision for the future\\n\\nAt present, OBKV-Redis is integrated with strong support for core data structures, such as strings, hashes, and sets. It now supports 93% of Redis KV database commands, and is expected to provide full compatibility with common management commands and support containerized deployment and level-2 caches by the end of the first quarter of 2024. Moreover, we also plan to implement in-depth integration with the Capture Data Change (CDC) scheme. In the second quarter of 2024, our R&D team will further develop data migration features and introduce data tiering technology.\\n\\nWe have great confidence and high hopes for OBKV-Redis. We hope it will allow us to implement more refined data management throughout the data life cycle. As the company enters the fourth stage of our business development, where business and technological challenges deepen, we see stability and innovation as our key mission. Therefore, we have high anticipation of the multi-model ecosystem, and we believe it will empower enterprises by solving legacy problems and reducing costs."},{"id":"all-in-one","metadata":{"permalink":"/blog/all-in-one","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/feat-all-in-one.md","source":"@site/blog/feat-all-in-one.md","title":"Work with developers to create an all-in-one database","description":"1713958543","date":"2024-06-03T13:23:07.000Z","tags":[{"inline":true,"label":"Release","permalink":"/blog/tags/release"},{"inline":true,"label":"OceanBase","permalink":"/blog/tags/ocean-base"}],"readingTime":29.515,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"all-in-one","title":"Work with developers to create an all-in-one database","tags":["Release","OceanBase"]},"unlisted":false,"prevItem":{"title":"Momo \u2014\u2014 Exploration and practice of persistent cache based on OceanBase KV storage","permalink":"/blog/momo"},"nextItem":{"title":"OceanBase provides users with sufficiently flexible and simple I/O resource isolation experiences.","permalink":"/blog/io-isolation"}},"content":"![1713958543](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958543012.png)\\n\\nGood morning, OceanBase developers! I\'m so glad to meet you again here in Shanghai for our second developers conference, after our first acquaintance in OceanBase DevCon 2023 in Wangjing, Beijing.\\n\\n![1713958832](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958831452.png)\\n\\nLet\'s first have a brief review of the architecture of OceanBase Database. The research and development of OceanBase Database started in 2010. It has been nearly 14 years. Throughout this period, we have made two major upgrades to the technical architecture and a key product upgrade.\\n\\n![1713958840](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958839876.png)\\n\\n\x3c!-- truncate --\x3e\\n\\nThe first upgrade to the technical architecture took place in 2016, where the original OceanBase Database V0.5 in the single-write multi-read architecture was upgraded to OceanBase Database V1.0 in a fully distributed architecture. The second architecture upgrade was carried out in OceanBase Database V4.0 in 2022. This version introduced an integrated architecture that supports both standalone and distributed deployment modes. This way, OceanBase Database can be applied not only to large enterprises (SMEs), but also to small and medium-sized enterprises and even start-ups.\\n\\nIn the second half of 2023, we proposed the concept of all-in-one database based on the integrated architecture, enabling OceanBase Database to incorporate capabilities such as OLTP, OLAP, KV, OBKV, and even AI in the future, to cope with a wide variety of workloads.\\n\\n## **I. The path to open source**\\n\\nOceanBase Database was officially available as an open source project on the Children\'s Day \u2014 June 1, 2021. Before OceanBase Database was launched open source, the open source community in China already had a prevailing native distributed database. Then, why another one?\\n\\nSimply because we believed that users and developers would love it, for it has showed high stability, high performance, and high cost effectiveness in more than ten years\' service in all core scenarios of Ant Group in Double 11 shopping festivals.\\n\\nAt that time, we were serious on turning OceanBase Database into open source, and real open source. Now I have a deeper understanding of \\"real open source\\". Is OceanBase Database really open-source? Statistics speak louder than our words.\\n\\nOceanBase Database has been available as an open source project for two years. The last two years have seen rapid development. Up to now, the number of clusters deployed with OceanBase Database Community Edition has exceeded 10,000. We have seen phenomenal growth in OceanBase users and clusters after the release of OceanBase Database V4.0.\\n\\n![1713958891](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958890484.png)\\n\\nToday, mainstream Internet companies, including Ctrip, Kuaishou, Zhihu, Vivo, and NetEase, are using OceanBase Database Community Edition in various scenarios. In the open source community, OceanBase is recognized as the best open source distributed database with top technical feature performance.\\n\\nOceanBase Database is widely recognized and has more than 1000 customers. According to the China Distributed Relational Database Vendors Report 2023 by IDC (Document number:# CHC50734323), OceanBase ranked in the industry leaders and was in a leading position in product capabilities. OceanBase also received an Honorable Mention in the 2023 Gartner Magic Quadrant for Global Cloud Database Management Systems, and has topped Modb China Database Rankings for 14 consecutive months.\\n\\n![1713958904](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958903766.png)\\n\\n### **i. OceanBase Database Community Edition in distributed OLTP scenarios**\\n\\nOceanBase Database was initially designed for mission-critical core business scenarios such as transaction processing and payment. Therefore, early open source users apply OceanBase Database to core business scenarios.\\n\\nCR Vanguard originally used the sharding solution of MySQL Database, which does not support scaling. By deploying OceanBase Database, a native distributed database solution, in place of MySQL Database, Vanguard realizes on-demand scaling, reduces the required storage size by 60% from 15 TB to 6 TB, and achieves a recovery point objective (RPO) of zero.\\n\\n![1713958918](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958917144.png)\\n\\nWith its infrastructure, including the databases, built in Baidu Cloud, Alibaba Cloud, and Tencent Cloud, Zuoyebang is in dire need of a system that can tackle this multi-infrastructure environment. Against this backdrop, it replaces MySQL with OceanBase. Backed by the high availability and disaster tolerance capabilities of OceanBase Database, Zuoyebang now sets up a multi-active architecture in a multi-infrastructure environment, handling hybrid transaction/analytical processing (HTAP) loads with just one system. Moreover, with OceanBase Database, the number of servers required is reduced from 24 to 9, reducing hardware costs by 60%.\\n\\n![1713958948](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958947472.png)\\n\\nDmall is a digital retail solution provider that serves many supermarkets in various sizes, such as Wumart. Dmall once used a large number of scattered MySQL servers, which do not support scaling and incur high O&M costs. The multitenancy capability of OceanBase Database enables database consolidation for Dmall, greatly reducing O&M costs. OceanBase Database also achieves a compression ratio of 6:1 while delivering higher performance even in standalone mode.\\n\\n![1713958985](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958984052.png)\\n\\nCtrip is one of the earliest customers of OceanBase Database Community Edition. Ctrip had a history database that used the sharding solution of MySQL Database, which did not support dynamic online scaling, failing to cope with the drastic data volume increase of the history database. Therefore, they migrated their history database to OceanBase Database. The native distributed database solution of OceanBase avoids database and table sharding, reduces the required storage space by 85%, and improves write performance several times.\\n\\n![1713958999](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958998115.png)\\n\\n### **ii. OceanBase Database Community Edition in real-time AP and multi-model scenarios**\\n\\nAlthough OceanBase Database was initially designed to handle mission-critical core business scenarios, our developers also use OceanBase Database in real-time analytical processing (AP) and multi-model scenarios.\\n\\nKuayue Express once used the \\"HBase + Kafka + StarRocks\\" architecture for its analytics scenario. They developed a system called HBase CDC, which pulls data from HBase to Kafka and then synchronizes the data to StarRocks. This solution features high development costs and a complex link, which also results in a long data processing period. With OceanBase Database, Kuayue Express now needs only one system, reducing hardware costs by 50% while improving data processing efficiency by 50%.\\n\\n![1713959021](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959020294.png)\\n\\nBeike has a dictionary service that was once established on HBase, which is frequently criticized for two major issues:\\n\\n\u25cb Massive complex components due to its dependence on the Hadoop system\\n\\n\u25cb Lack of support for secondary indexes\\n\\nIn view of this, Beike replaced HBase with OBKV, which improves the query performance by 2 to 5 times and the write performance by 5 times with a simplified system complexity.\\n\\n![1713959058](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959057480.png)\\n\\n### **iii. All-in-one database driven by developers**\\n\\nSometimes I wonder why developers would use a distributed database that was initially designed for mission-critical core business scenarios in real-time AP or multi-model scenarios as an all-in-one database. I believe the answer lies in the two initial design concepts of OceanBase Database.\\n\\n\u25cb Distributed architecture: The distributed architecture of OceanBase Database enables it to process massive data and support automatic scaling.\\n\\n\u25cb Log-structured merge-tree (LSM-tree)-based storage: This achieves a high compression ratio, enabling processing of massive data at a low cost.\\n\\nWith these capabilities, OceanBase Database is particularly suitable for scenarios involving massive data, including not only TP scenarios, but also AP and multi-model scenarios, which naturally involve large amounts of data.\\n\\nTherefore, it is fair to say that it is the demand of developers and users that has driven OceanBase Database to grow from the early distributed TP system to the distributed AP system, and then into the present-day all-in-one database. With this integrated architecture, OceanBase Database will surely rise up to the future challenges posed by multi-model and AI scenarios, achieving lower IT costs for users.\\n\\nWhat lies behind the all-in-one database is the integrated architecture, which includes not only the integrated storage engine, integrated transaction engine, and integrated SQL engine in the kernel, but also the multi-model engine built on the integrated architecture, and the capability to adapt to multiple infrastructures.\\n\\nOceanBase Database is also a multi-cloud native all-in-one database, which can natively support both public and private clouds with different architectures in one system, including Huawei Cloud, Tencent Cloud, Alibaba Cloud, Amazon Web Services (AWS), Google Cloud Platform (GCP), and Azure. This means that developers can use OceanBase services in different clouds with the same experience.\\n\\n## **II. Significance of integration for developers**\\n\\n### **i. Transparent deployment in standalone and distributed modes**\\n\\nOceanBase Database supports an integrated architecture for standalone and distributed modes. The deployment is transparent to users. Today, the first question facing developers when choosing a database system is whether to choose a centralized or distributed one.\\n\\nOceanBase Database resolves the dilemma by providing an integrated architecture that supports both standalone and distributed modes. This integrated architecture enables OceanBase Database to support smooth on-demand scaling and small-specification deployment, making OceanBase Database a stand-out choice for large enterprises, SMEs, and even start-ups. Moreover, OceanBase Database also supports three replicas based on Paxos and primary/standby synchronization, and achieves an RPO of 0 and a recovery time objective (RTO) of less than 8 seconds.\\n\\n![1713959351](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959350210.png)\\n\\n### **ii. OBKV: adding a query API rather than a database**\\n\\nSQL and non-SQL models are integrated to achieve multi-model integration. I believe that multi-model integration is not just about providing a new data model, but more about integrating different data models to combine their respective strengths.\\n\\n![1713959361](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959360450.png)\\n\\nTake HBase, which has been well-explored by many developers, as an example. The write interface of HBase is relatively simple, efficient, and easy to use. However, HBase does not support SQL syntax, and therefore its query interfaces have limited features and are inconvenient to use. With OBKV, we can write data to OceanBase Database in an HBase-compatible way and query data from OceanBase Database by using standard SQL syntaxes, giving full play to technical advantages of both SQL and NoSQL systems.\\n\\n### **iii. HTAP = OLTP Plus**\\n\\nSpeaking of the integration of TP and AP systems, we often mention HTAP. As I have said in many occasions, HTAP is not omnipotent.\\n\\n![1713959394](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959393533.png)\\n\\nIn most cases, HTAP is merely a plus version of online transaction processing (OLTP) that supports online analytical processing (OLAP) capabilities on the basis of OLTP. The following two deployment modes are supported:\\n\\n(1) Based on the multi-replica distributed architecture of OceanBase Database, all replicas use the same storage architecture, either row-based storage or hybrid row-column storage, and the leader directly provides services. This deployment mode achieves zero data latency while ensuring data consistency, but provides only moderate support for AP capabilities due to the lack of columnar storage. Therefore, this deployment mode is suitable for \\"OLTP + lightweight OLAP\\" scenarios.\\n\\n(2) The leader uses row-based storage or hybrid row-column storage, while one or multiple followers use columnar storage. This service mode incurs data latency between the leader and the followers, but can better support OLAP capabilities. Therefore, it is suitable for \\"OLTP + medium-load OLAP\\" scenarios.\\n\\nTo wrap up, HTAP is a solid option in scenarios involving hundreds of GB to hundreds of TB of data, but it is not omnipotent even with these two flexibly deployment modes. Many large companies with a larger data amount usually deploy separate TP and AP systems.\\n\\nTake Haidilao as an example. Haidilao originally used PolarDB and PolarDB-X for OLTP and AnalyticDB for OLAP, and used Data Transmission Service (DTS) to synchronize data from PolarDB to AnalyticDB.\\n\\n![1713959411](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959410909.png)\\n\\nHaidilao often do promotions in holidays and need to make recommendations according to customer preferences in real time. The HTAP capabilities of OceanBase Database implement both TP and AP based on the same set of data, helping Haidilao reduce the total cost of ownership (TCO) by 35% and improving AP performance by 30%.\\n\\n## **III. Use TP & AP integration to fuse core capabilities of distributed TP systems into AP systems**\\n\\nI\'ve mentioned earlier that traditional HTAP is not omnipotent. Here is a new concept that I\'d like to share with you: using TP & AP integration to fuse core capabilities of distributed TP systems into AP systems.\\n\\n![1713959637](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959636626.png)\\n\\nWhat does it mean? Instead of providing TP and AP services with one system, we can directly integrate TP capabilities into an AP system to build a new real-time analytical database that is easier to use for developers.\\n\\nWe all know that traditional OLAP systems often provide strong large query capabilities, while AP systems feature robust ecological adaptation but do not support real-time writing due to the lack of support for row storage. Therefore, traditional AP systems do not support real-time point queries or real-time serving. Moreover, AP systems are no match for TP systems in terms of syntax compatibility and functionality. Regardless of their diversity, AP systems today are rarely applied in core business scenarios, and therefore fall short of adequate testing in terms of reliability and stability.\\n\\nOceanBase Database offers sound distributed capabilities, high reliability, and high availability with an RPO of zero. It also provides favorable TP capabilities and supports real-time writing and table access by secondary index primary key for point queries, achieving dynamic serving in AP scenarios. It is also compatible with MySQL Database. Besides, OceanBase also provides a GUI-based O&M tool called OceanBase Cloud Platform (OCP).\\n\\nOceanBase combines these capabilities with traditional AP capabilities to offer you a new-generation real-time AP system. This system offers better realtimeness and allows you to directly perform both large queries and serving. It is compatible with MySQL Database and is easy to use. Moreover, you can directly use OceanBase control tools for integrated control.\\n\\n## **IV. Release of OceanBase Database V4.3.0**\\n\\nToday, I\'m honored here to announce the official release of OceanBase Database V4.3.0. This version launches three core technological upgrades:\\n\\n\u25cb Columnar storage engine\\n\\n\u25cb Further strengthened TP & AP integration\\n\\n\u25cb Near-petabyte-scale real-time analytical database that supports data sized \\\\[1TB, 1PB)\\n\\nLet\'s dive into the core capabilities of OceanBase Database V4.3.0.\\n\\n![1713959735](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959734334.png)\\n\\n(1) Enhanced TP & AP integration and OLTP capabilities\\n\\nIn KV scenarios, OceanBase Database V4.3.0 greatly outperforms V4.2.1, with 70% higher single-row reading and writing performance and 80% to 220% higher batch reading and writing performance. This version is also optimized in terms of SQL, transaction, and log features.\\n\\nThis version also comes with the fast tenant cloning feature. I believe you would love it. The feature allows you to clone a tenant to generate a snapshot before performing risky operations, so that you can quickly restore the tenant if these operations go wrong.\\n\\n(2) Enhanced real-time AP capabilities\\n\\nThe new version supports bypass import, external tables, columnar storage, and dynamic conversion between row-based storage and columnar storage.\\n\\nMany features useful in OLAP scenarios are also provided, such as materialized views, federated queries, window functions, common table expressions (CTEs), hierarchical queries, and operator pushdown.\\n\\nThe distributed computing engine is also significantly enhanced with a Massively Parallel Processing (MPP) architecture and supports the vectorized engine and auto DOP. Moreover, this version also enhances support for semi-structured data, such as JSON and geographic information system (GIS) data. OceanBase Database V4.3.0 is compatible with most mainstream streaming databases in the industry, such as Kafka and Flink.\\n\\n(3) Improved OLAP performance\\n\\nCompared with OceanBase Database V4.2.1, the new version offers 25% higher TPC-H 1 TB test performance, 111% higher TPC-DS 1 TB test performance, and six times higher bypass import performance. Some may argue that the 25% TPC-H performance increase is but a mild one. I must remind you here that OceanBase Database has topped the TPC-H benchmark test. So, it\'s a laudable climb from the top.\\n\\n(4) Improved AP compatibility\\n\\n(5) Greater ease of use\\n\\nThis version provides AP parameter templates and scenario-based AP documents, greatly improving the ease of use. We believe that the scenario-based documents can be a great help during your exploration of the new version.\\n\\nAlthough OceanBase Database V4.3.0 is officially released today, articles about experiencing the new version are already available on the Internet. You are welcome to learn more about OceanBase Database V4.3.0 in our exhibition area and on the official website.\\n\\n### **i. Benchmarking based on analytical loads: OceanBase Database versus top-notch columnar databases in wide-table queries**\\n\\nAs a routine of OceanBase launch events, here comes the benchmarking part, which is also one of my favorite parts of this conference. Now we will do the analytical load-based benchmark tests together.\\n\\nIn fact, before this event, we\'ve already run benchmark tests on OceanBase Database and some mainstream real-time analytical databases in the industry and compared the test results. We will use ClickBench, a standard-bearer benchmark for analytical databases in the industry proposed by ClickHouse (which also tops the ClickHouse benchmark rankings), to evaluate the performance of OceanBase Database and ClickHouse.\\n\\nWe have run a benchmark test on OceanBase Database V4.3.0 Beta and ClickHouse 23.11 last year. We later commercialized and evolved OceanBase Database V4.3.0 Beta into V4.3.0. ClickHouse also issued a new release, namely release 24.4, in April this year, which outperforms its predecessor 23.11. Fortunately, we have OceanBase Database V4.3.x Beta.\\n\\nSo today, we will have four systems for the test: OceanBase Database V4.3.0 and V4.3.x Beta, as well as ClickHouse 23.11 and 24.4, which are respectively the latest two versions of OceanBase and ClickHouse.\\n\\nNow the benchmark test has started. As you know, ClickBench has 43 queries, respectively named Q0 to Q42. In the test, each query is executed 3 times, and the result of the execution that takes the shortest time is taken as the final result of the query.\\n\\nThe systems in the test are displayed either in green or red. A green background indicates that the system runs faster, while a red one indicates slower running. Now that the benchmark test is done, you can see that OceanBase Database V4.3.0 takes 14.5 seconds, ClickHouse 23.11 takes 14.8 seconds, OceanBase Database V4.3.x Beta 12.85 seconds, and ClickHouse 24.4 14.26 seconds. In summary, OceanBase Database V4.3.0 outperforms ClickHouse 23.11, but is outshined by ClickHouse 24.4, while OceanBase Database V4.3.x Beta surpasses all, including ClickHouse 24.4. They are in close tie though.\\n\\nAccording to the test results, we can see that under the same hardware conditions, OceanBase Database V4.3.0 achieves a wide table query performance comparable with that of ClickHouse, which is best in class in the real-time analytical database industry.\\n\\n![1713959830](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959829606.png)\\n\\n### **ii. Aspiring to build an omnipotent real-time analytical database for sub-petabyte-scale scenarios**\\n\\nI\'ve mentioned earlier that the all-in-one architecture of OceanBase Database enables it to tackle various scenarios such as TP, AP, multi-model, KV, and AI scenarios. It sounds as if OeanBase Database suits all purposes. Then what are the scenarios that OceanBase Database is truly suitable for? The scenarios can be classified into the following categories:\\n\\n![1713959859](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959858594.png)\\n\\n(1) Express OLTP\\n\\nSuch scenarios generally involve simple read and write operations at high concurrency in the Internet industry. MySQL Database usually fits these scenarios.\\n\\n(2) Complex OLTP\\n\\nSuch scenarios involve some complex queries, batch operations, and stored procedures in addition to simple read and write operations. These scenarios are often encountered in conventional industries for which commercial databases such as Oracle are well-suited.\\n\\n(3) OBKV\\n\\nSystems such as HBase and Redis are apt for these scenarios.\\n\\n(4) HTAP\\n\\nAn HTAP system processes both OLTP and real-time AP loads. In my opinion, HTAP is applicable to scenarios involving hundreds of GB to hundreds of TB of data.\\n\\n(5) Real-time AP scenarios with larger data amounts\\n\\nThe difference between real-time AP and HTAP is that the data source for HTAP is a system, while the data source for real-time AP is not only the TP system of OceanBase Database, but also may be other databases, Kafka or Flink instances, files, or storage systems. The real-time AP system will further extend the AP capabilities of OceanBase Database.\\n\\nReal-time AP is suitable for scenarios with a data amount of 1 TB to 1 PB. A scenario with a data amount of more than 1 PB can be taken as a large enterprise scenario. Large enterprises, such as Alibaba and Tencent, have their own in-house data lakes or big data analysis systems, which fall out of the application scenarios of OceanBase Database.\\n\\nWe can build a lightweight real-time analytical database directly based on the real-time AP capabilities of OceanBase Database. This analytical database supports diverse data sources, real-time writes, batch writes, and some batch updates. After the data arrives at OceanBase Database, it will be processed in layers, first by the operational data store (ODS) layer, and then by the data warehouse detail (DWD), data warehouse summary (DWS), and application data service (ADS) layers in sequence.\\n\\n![1713959902](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959901104.png)\\n\\nThe ODS layer stores raw data, and the real-time data warehouse usually uses columnstore tables. DWD and DWS process raw data in two layers, usually by using materialized views. Then, the ADS layer carries out serving. It uses row-based storage for simple point queries and table access by index primary key. However, when the data amount of each query is large, it uses columnar storage for serving.\\n\\nIn OceanBase Database V4.3.0, we can carry out both large queries on the data warehouse and serving at the same time in one system, providing users with better real-time capabilities. In addition, the enhanced compatibility with MySQL makes it easier to use for developers.\\n\\nWe aspire to forge the real-time AP system of OceanBase Database into an omnipotent real-time analytical database for sub-petabyte-scale scenarios.\\n\\n![1713959925](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959924580.png)\\n\\nFirst, it is suitable for scenarios with a data amount of 1 TB to 1 PB. It is designed for users with real-time demands, whereas users with pure offline analytics demands have many other options. OceanBase Database provides strong TP capabilities, supports real-time writing, and directly provides AP services through point queries. OceanBase Database also has sound MySQL compatibility and good distributed capabilities for high availability. Moreover, it also offers high stability, with basically no bugs.\\n\\nSecond, OceanBase Database provides exceptional real-time AP capabilities, and realizes columnar storage and vectorization. Notably, it has topped both the ClickBench and TPC-H benchmark rankings. I believe that the combination of these two capabilities will make OceanBase Database an excellent choice of real-time analytical database for sub-petabyte-scale scenarios.\\n\\n### **iii. What\'s next**\\n\\nWe have a lot to do next. In the first quarter, we released OceanBase Database V4.3.0 in this developers conference. This version enhances the capabilities of columnar storage and the vectorized engine. In the months to come, we will continue to tap the potentials of OceanBase All-in-one, to improve its integration with and support for multi-model, search, and AI capabilities.\\n\\n![1713959951](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959950394.png)\\n\\nIn the second quarter, we will develop full-text indexing and JSON multi-value indexing capabilities and strengthen database search capabilities. In the third quarter, we will further support vector database capabilities by using components. With support for vector databases, you can directly develop your own large model applications based on OceanBase Database. In the fourth quarter, we will implement storage-compute separation capabilities based on Amazon Simple Storage Service (S3).\\n\\nOceanBase Database has many users in public clouds. As we all know, storage and computing are separated in a public cloud. However, currently, OceanBase Database implements storage-compute separation based on expensive cloud disks of public clouds. With the object storage capability based on S3, we can greatly improve the cost performance of OceanBase Database.\\n\\n## **V. Improved ease of use**\\n\\nMany developers speak to me highly of OceanBase Database in the following aspects:\\n\\nThe first is its powerful technological capabilities. We enjoy running benchmark tests in our launch events. Sometimes we run TPC-C benchmark, sometimes TPC-H or ClickBench, and other times ClickHouse. It\'s a bit like having your excellent child competing for top universities and showing off their achievements in your neighborhood. This is the core technological capabilities of OceanBase Database.\\n\\nThe second is that OceanBase Database realizes high O&M efficiency for proficient developers or DBAs. One Alipay DBA can operate and maintain thousands of servers. However, you can fully explore the powerful features of OceanBase Database only after you gain profound knowledge about it.\\n\\nTherefore, we have made a lot of effort to make OceanBase Database easier to use and get started with.\\n\\nNext, let\'s take a look at some video clips from our users and developers concerning the ease of use of OceanBase Database.\\n\\nWe would like to extend our gratitude to Chunlei, Guangming, and Baishan. Thank you for your praise and encouragement. Your constructive comments are also highly appreciated. We will spare no efforts to meet your high expectations for OceanBase Database.\\n\\nSpeaking of ease of use, I often think of the technical books I have read. Such books are often titled something like \\"From Beginners to Experts\\", \\"Seven Days to Learn xxx\\", and \\"Learn a programming language in 21 days\\". That is why I named this presentation \\"OceanBase \u2013 From Beginners to Experts\\"\\n\\n![1713960011](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960010018.png)\\n\\nThe ease-of-use feature is helpful for the following two types of users:\\n\\nThe first type is beginners. These users are more concerned about how to quickly deploy OceanBase Database, whether a GUI or CLI tool is available for quickly performing benchmark tests and basic demo tests, and whether reference documents are available.\\n\\nThe other type is users who have been using OceanBase Database for a while. For these users, all their requirements can be summarized into one question: What do I do when something goes wrong? This seemingly simple question is hard to answer. A situation going wrong may be caused by various exceptions, such as common exceptions like server failures, network failures, and disk failures, or underlying issues like system suspension or jitter caused by unknown reasons. Therefore, experienced users tend to look for answers in the debug logs of OceanBase Database.\\n\\nIn the previous videos, we can see some of the most basic GUI-based features of OceanBase Database. OCP is quite a standout in the industry. Guangming even said that OCP is the best ecological tool that he has ever used throughout his career. I\'m thrilled to hear that. The credit goes to our OCP team.\\n\\nHowever, OceanBase Database still entails some drawbacks. For example, the quickstart tutorial needs to be optimized. Many users may find it difficult to come by the relevant documents when they install and deploy OceanBase Database. In addition, no effective tools are available for diagnosing some underlying issues such as jitters and system suspension. The debug logs of OceanBase Database are also inferior to those of Oracle Database in terms of observability and ease of use.\\n\\nIn fact, we\'ve invested considerable efforts to make relevant improvements last year, though some expectations are yet to be fulfilled.\\n\\n### **i. Ease to learn: lowering the barrier for getting started**\\n\\nWe have put in a great deal of work to lower the barrier for novice users from the following aspects:\\n\\n![1713960042](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960041671.png)\\n\\n(1) Installation and deployment\\n\\nChunlei has mentioned some issues related to installation and deployment in the video earlier. Last year, we achieved quick installation and deployment of OceanBase Database within two minutes by using OBD. However, OBD offers limited features. Though boasting rich and powerful features, OCP supports the installation of OceanBase Database only in an environment that meets specific hardware conditions. Neither of the two tools can fully meet the needs of developers.\\n\\nIn light of this, we combine OBD and OCP. You can use OBD to install OceanBase Database and then use OCP to run the database, so that you can tap the rich management and control capabilities of OCP after installation. This resolves the installation and deployment issues mentioned by Chunlei.\\n\\n(2) Performance tests\\n\\nOceanBase Database is renowned for its performance. Yet many developers told me that they must become OceanBase experts to bring out its full potential.\\n\\nTherefore, we provide parameter templates based on scenarios, enabling developers to build OceanBase databases that can yield remarkable benchmark test results.\\n\\n(3) Documentation\\n\\nOceanBase documentation is voluminous. There are only slightly over 1,900 topics for OceanBase Database V2.x, and over 2,900 for V3.x, but over 3,900 for V4.x. Over 1,000 topics are added in each version upgrade.\\n\\nDespite the substantial efforts invested into documentation optimization last year, there is still much room for improvement. We hope to receive more insights about documentation and ease of use from more developers like Baishan.\\n\\nWe position OceanBase Database as a globally popular database. To achieve this goal, we must deliver product documentation that meets world-class standards.\\n\\nBesides documentation, the following two features can also be useful for developers:\\n\\nOne is online experience. It would be more constructive if we can experience first-hand the features described in the documentation. Therefore, we provide the online experience feature, which provides an environment for you to experience online the OceanBase knowledge described in the articles. With the online experience feature, you can copy an operation and check the effect directly in the cloud environment. We also have a documentation area here. You are welcome to try it out.\\n\\nThe other is the knowledge base. We enhanced the OceanBase knowledge base last year and added more than 1000 cases. These cases are based on many years\' experience of OceanBase and Ant Group DBAs in mining and solving problems in customer scenarios. We hope it can be helpful for developers.\\n\\n### **ii. Ease of diagnosis: improved diagnostic capabilities**\\n\\nWe have also further improved the diagnostic capabilities of OceanBase Database.\\n\\n![1713960096](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960095220.png)\\n\\nOceanBase Database excels in handling simple exceptions based on OCP, such as some simple network failures, disk failures, and simple compaction issues. This is because OCP can detect the causes for such exceptions. However, as for deep-seated issues whose resolution requires profound understanding of OceanBase Database, OCP is found inadequate. Therefore, we provide the following tools as a complement:\\n\\n(1) Active Session History (ASH): the perf-like tool of OceanBase Database\\n\\nWe all know the perf tool of Linux. When something goes wrong, we turn to perf to find the cause.\\n\\nLast year, we developed a tool called ASH, an OceanBase version of perf. You can locate causes of issues by referring to ASH reports. Seemingly easy, developing ASH was actually very challenging for us.\\n\\nWe\'ve set up a dedicated performance diagnostics team made up of many senior OceanBase kernel developers. They spent over a year developing a time model that ensures time accuracy for all backend tasks, locks, wait events, queue entry, queue exit, and so on.\\n\\n(2) OceanBase Autonomy Service (OAS): for root cause analysis\\n\\nOAS incorporates rules that are formulated based on years of customer service experience of Ant Group and OceanBase. After you locate the issue based on ASH reports, you can use OAS to identify the root cause.\\n\\n(3) `alert.log`\\n\\nOceanBase Database has long been criticized for its debug logs, which contain excess content, making it difficult for developers to find useful information.\\n\\nThis year, we launched the `alert.log` file based on suggestions from developers. This file is an extract from the debug logs, and records the common system events that occur during the operation of OceanBase Database. Developers can resolve 80% of the issues that they encounter by referring to `alert.log`, without the need to dig through the long-winded `observer.log` file.\\n\\n### **iii. On-demand use of serverless instances, with a 1-month free trial**\\n\\nThe concept of \\"serverless\\" is intrinsically linked with cloud native. Compared with common instances, serverless instances support more flexible scaling methods and full on-demand use, making them a cost-effective choice for developers.\\n\\n![1713960147](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960146102.png)\\n\\nOceanBase Database serverless instances are now supported in ApsaraDB for OceanBase. A one-month free trial is provided for serverless instances with a specification of 1C4G on Alibaba Cloud and Huawei Cloud. We have a free trial demo here in the exhibition area. I was told that it is so popular that the quota is used up. Don\'t worry. We are applying for more quota.\\n\\n## **VI. A more open technology ecosystem**\\n\\n### **i. Smooth integration of binlog service with over 20 downstream services**\\n\\nLast year, we also developed a useful feature, a MySQL-compatible binlog service, which has been integrated with more than 20 downstream services, including some MySQL subscription tools and binlog-based cloud services.\\n\\n![1713960191](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960190610.png)\\n\\nI\'d like to share some untold stories behind this feature. The OceanBase product R&D department will present a \\"Breakthrough Award\\" at the end of each year. It\'s the top award in our department. The final candidates of the award last year were the columnar storage project team and the binlog project team.\\n\\nWell, as it turned out, the binlog project team won the award. I am glad that our department leaders voted for the binlog project team that is dedicated to improve ease of use, instead of the columnar storage project team that is set out to enhance core product capabilities. This is a testament that our production and research team has realized that improved ease of use means more to developers than modest performance enhancement.\\n\\n### **ii. OceanBase landscape: from basic ecological adaptation to open technology ecosystem**\\n\\nThis is the landscape of OceanBase ecological tools.\\n\\n![1713960206](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960205995.png)\\n\\nOceanBase ecological tools have gone through two main stages in their development.\\n\\nThe first stage is ecological adaptation, including the adaption of ecological tools to the OceanBase Database kernel and adaption of OceanBase Database as a database ecosystem to other database ecosystems, such as Kubernetes and big data.\\n\\nThe second stage is joint construction of the ecosystem based on open APIs. At present, more than 750 mainstream products have joined the OceanBase ecosystem, and joined forces with OceanBase to build the open ecosystem.\\n\\n### **iii. Sustained efforts to lower entry barriers for developers based on open source**\\n\\nFinally, I\'d like talk about the open source community of OceanBase.\\n\\n![1713960235](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960234174.png)\\n\\nThe OceanBase open source community was originally positioned as a \\"responsive\\" community where user feedbacks will be responded in a timely manner. Now we have upgrade the community to be an interactive one.\\n\\nI\'ve always believed that open source is not just about opening up and sharing some source code of products or technologies, but more about the open source community being a bridge that brings together people interested in a certain product. This is one of the key insights I gained last year.\\n\\nGladly, our user organization OUG finally went on track last year. We\'ve held a series of OUG city visit and enterprise visit events in concert with a bunch of enterprises such as 58, Zhihu, and Vivo in the open source community.\\n\\nMany developers have shared their thoughts in the open source community. More than 118 developers opened their blog accounts and published more than 1,000 technical blogs. The community also boasts an impressive collection of co-developed open source projects, including six warehouses containing over 50,000 code lines built jointly by OceanBase and other companies or developers.\\n\\n## **VII. Huge kudos to developers, who built a bridge for communication with inspired initiatives**\\n\\nOur special thanks go to the 108 developers entitled \\"Star of the Month\\". Your active participation has made our community a better place to learn about OceanBase. We also highly appreciate the efforts of the application developers and 315 OceanBase contributors for jointly building the OceanBase ecosystem and making the open source endeavor more fruitful.\\n\\n![1713960305](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960304441.png)\\n\\nLast but not least, I\'d like to share with you a project case from the OceanBase community. It is an initiative made by a developer to build a vector engine plug-in to integrate SQL and AI in OceanBase Database.\\n\\n![1713960317](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960316156.png)\\n\\nThough OceanBase Database currently does not support vector databases, he still spent months in writing a vector engine plug-in based on the open source code of OceanBase Database. He also built a personal knowledge base based on the vector engine plug-in that he wrote. This knowledge base supports natural language retrieval.\\n\\nThis initiative is fascinating. We look forward to more initiatives like this coming up in the OceanBase community. We sincerely hope that our open source community will become a warm haven where people can have fun and make friends.\\n\\nThat\'s all for my sharing today. Look forward to seeing you again at the 2024 Release Conference."},{"id":"io-isolation","metadata":{"permalink":"/blog/io-isolation","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/feat-io-isolation.md","source":"@site/blog/feat-io-isolation.md","title":"OceanBase provides users with sufficiently flexible and simple I/O resource isolation experiences.","description":"Sun Jianyun, an OceanBase technical expert","date":"2024-06-03T13:23:07.000Z","tags":[],"readingTime":9.065,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"io-isolation","title":"OceanBase provides users with sufficiently flexible and simple I/O resource isolation experiences."},"unlisted":false,"prevItem":{"title":"Work with developers to create an all-in-one database","permalink":"/blog/all-in-one"},"nextItem":{"title":"OceanBase 4.0 interpretation: Reduce the threshold of distributed database use, talk about our thinking on small specifications","permalink":"/blog/miniaturization"}},"content":"**Sun Jianyun, an OceanBase technical expert**\\n\\nHe used to be a member of the TPC-C project and the technical support team for Double 11 shopping festivals, and is now engaged in the design, research, and development of I/O scheduling, DDL capabilities, and other work related to storage engines.\\n\\nIn [Why resource isolation matters to HTAP? ](https://open.oceanbase.com/blog/10900412), we talked about why hybrid transaction/analytical processing (HTAP) relies on resource isolation and how to implement it. Resource isolation is a capability. Many scenarios can be derived from it, such as HTAP, multitenancy, and pay-as-you-go. Based on resource isolation and cloud-based resource pools, all kinds of resources can be allocated on demand. Isolation of resources such as CPU and memory is already supported in OceanBase Database V4.0. OceanBase Database V4.1 supports enhanced disk I/O isolation and provides users a simple and flexible way to use this feature.\\n\\nWe believe that disk I/O isolation is an essential part of resource isolation. Disk I/O isolation enhances and completes the resource control capabilities for users. This article describes some thoughts of the OceanBase team on disk I/O isolation, how it is configured in OceanBase Database, and the disk I/O isolation performance testing of OceanBase Database V4.1.\\n\\n\x3c!-- truncate --\x3e\\n\\n## I. Why is disk I/O isolation necessary?\\n\\nSome may ask, \\"Is resource isolation, especially disk I/O isolation, really necessary?\\" Why not directly divide the loads among different servers? For example, transaction processing (TP) and analytical processing (AP) loads can be routed to different replicas on different servers, and different tenants can be deployed on different servers to implement physical isolation. As I see it, that is truly a simple and convenient solution. However, it has many limitations, and cost is the biggest concern. Assume that a game company has two tenants A and B, where tenant A processes services outside China and tenant B processes services inside China. The load peaks of one tenant coincide with the load troughs of the other and vice versa due to the time zone difference. Although each of them can exclusively occupy separate server resources, half of the resources are wasted.\\n\\nFor disk I/O resources, loads whose data is tightly coupled cannot be simply divided among different servers. For example, operations such as backup, migration, and reorganization in a database strongly depend on intensive data reads and writes. Without disk I/O isolation, these tasks can affect the service throughput and response time. Actually, it is difficult to divide TP and AP loads on different servers as desired. TP and AP loads cannot be clearly demarcated sometimes. Even loads of the same type, such as TP loads, have different priorities based on services. What can we do in this situation?\\n\\nDisk I/O is a type of flexible resources, and loads can contend for disk I/O resources. Resources such as memory are rigid and described as scalars. A memory block occupied by Load A cannot be simultaneously allocated to Load B. Disk I/O is a type of flexible resources and described as the processing capability within a unit time. Loads A and B can read data from and write data to the disk at the same time. Rigid resources can be clearly isolated like cutting a cake. However, for flexible resources, contention between loads must be considered. Assume that you have two fields A and B irrigated by the same river. When the water that flows to Field A is reduced, the water that flows to Field B can be increased.\\n\\n## II. What is good disk I/O isolation?\\n\\nTo answer this question, we need to figure out customers\' expectations of disk I/O isolation, which vary from one customer to another.\\n\\n- Some customers want to implement exclusive resource usage through I/O isolation, such as an exclusive disk bandwidth of 200 Mbit/s.\\n- Some customers want to limit the resource usage of some loads to specified thresholds through disk I/O isolation.\\n- Others only want to allocate resources by weight when resources are insufficient. Resource isolation is not a concern when resources are sufficient.\\n\\nIn the technical field of resource isolation, the preceding three types of requirements correspond to three isolation semantics: reservation, limitation, and proportion. They are also what disk I/O isolation is supposed to implement in OceanBase Database.\\n\\n## III. How do we configure disk I/O isolation in OceanBase Database?\\n\\nOceanBase Database allows you to configure disk I/O isolation between tenants or between loads in a tenant.\\n\\nThe former is quite easy. For input/output operations per second (IOPS), you can specify the `MIN_IOPS`, `MAX_IOPS`, and `IOPS_WEIGHT` parameters for a tenant in the unit config to meet the foregoing three types of isolation requirements. Here is an example.\\n\\n```SQL\\nalter resource unit set tp_unit min_iops=20000, max_iops=40000, iops_weight=500;\\n```\\n\\nThen, how to configure disk I/O isolation between loads in a tenant? OceanBase Database extends the ResourceManager package of Oracle to adapt to the use habits of users.\\n\\nThe following example shows you how to use ResourceManager to isolate the disk I/O resources for TP and AP loads.\\n\\n- First, create a resource management plan named `htap_plan` and two resource consumer groups named `tp_group` and `ap_group`.\\n- Second, bind `tp_group` and `ap_group` to `htap_plan`. Allocate more resources to `tp_group` and fewer resources to `ap_group`. The value of each of `MIN_IOPS`, `MAX_IOPS`, and `WEIGHT_IOPS` is a resource percentage of the unit config of the tenant.\\n- Third, set the mapping rule between the loads and resource consumer groups. In this example, loads are mapped to consumer groups by username. For example, all loads of the `trade` user use resources of the `tp_group` resource consumer group.\\n\\n```SQL\\n# Create a resource management plan\\nBEGIN DBMS_RESOURCE_MANAGER.CREATE_PLAN(\\n  PLAN => \'htap_plan\');\\nEND; /\\n\\n# Create resource consumer groups\\nBEGIN DBMS_RESOURCE_MANAGER.CREATE_CONSUMER_GROUP(\\n  CONSUMER_GROUP => \'tp_group\',\\n  COMMENT => \'resource group for oltp applications\');\\nEND;/\\n\\nBEGIN DBMS_RESOURCE_MANAGER.CREATE_CONSUMER_GROUP(\\n  CONSUMER_GROUP => \'ap_group\',\\n  COMMENT => \'resource group for olap applications\');\\nEND;/\\n\\n# Allocate resources\\nBEGIN DBMS_RESOURCE_MANAGER.CREATE_PLAN_DIRECTIVE(\\n  PLAN => \'htap_plan\',\\n  GROUP_OR_SUBPLAN => \'tp_group\' ,\\n  COMMENT => \'more resource for tp_group\',\\n  MGMT_P1 => 100,\\n  MIN_IOPS => 60,\\n  MIX_IOPS => 100,\\n  WEIGHT_IOPS => 100);\\nEND; /\\n\\nBEGIN DBMS_RESOURCE_MANAGER.CREATE_PLAN_DIRECTIVE(\\n  PLAN => \'htap_plan\',\\n  GROUP_OR_SUBPLAN => \'ap_group\' ,\\n  COMMENT => \'less resource for ap_group\',\\n  MGMT_P1 => 20,\\n  MIN_IOPS => 0,\\n  MIX_IOPS => 80,\\n  WEIGHT_IOPS => 20);\\nEND; /\\n\\n# Map loads to resource consumer groups\\nBEGIN\\nDBMS_RESOURCE_MANAGER.SET_CONSUMER_GROUP_MAPPING\\n  (\'FUNCTION\', \'CAOPACTION_HIGH\', \'background_group\');\\nEND;/\\n\\n# Map specific SQL statements to a resource consumer group\\nBEGIN\\n  DBMS_RESOURCE_MANAGER.SET_CONSUMER_GROUP_MAPPING\\n    (\'COLUMN\', \'test.t1.c1 = 3\', \'big1_group\');\\nEND;/\\nBEGIN\\n  DBMS_RESOURCE_MANAGER.SET_CONSUMER_GROUP_MAPPING\\n    (\'USER\', \'trade\', \'tp_group\');\\nEND;/\\n\\nBEGIN\\n  DBMS_RESOURCE_MANAGER.SET_CONSUMER_GROUP_MAPPING\\n    (\'USER\', \'analysis\', \'ap_group\');\\nEND;/\\n```\\n\\nMapping rules for resource consumer groups also support function names and column names. In function name-based mapping, the resource usage of backend tasks can be controlled by using ResourceManager. In column name-based mapping, resource isolation can be refined to the SQL statement level. Here is an example.\\n\\n```SQL\\n# Map backend tasks to a resource consumer group\\nBEGIN\\n  DBMS_RESOURCE_MANAGER.SET_CONSUMER_GROUP_MAPPING\\n      (\'FUNCTION\', \'CAOPACTION_HIGH\', \'background_group\');\\nEND;/\\n\\n# Map specific SQL statements to a resource consumer group\\nBEGIN\\n  DBMS_RESOURCE_MANAGER.SET_CONSUMER_GROUP_MAPPING\\n      (\'COLUMN\', \'test.t1.c1 = 3\', \'big1_group\');\\nEND;/\\n```\\n\\n## IV. Disk I/O isolation performance testing of OceanBase Database V4.x\\n\\n### **Verify the disk I/O isolation capability**\\n\\nCreate four tenants for a simulation test. Each tenant starts 64 threads to send I/O requests that perform 16 KB random reads. The loads of tenants 1, 2, and 4 last for 20 seconds, and the load of tenant 3 begins from the 10th second and lasts for 10 seconds. In this test, the maximum IOPS is about 60,000. Without limitations, any tenant can use up the disk resources.\\n\\nWe first verified the disk I/O isolation between tenants. Table 1 describes the resource configurations of the tenants and Figure 1 shows the test results of the tenants.\\n\\n- When the disk resources are used up, the newly joined tenant 3 still has an IOPS of 10,000, which is reserved by using the `MIN_IOPS` parameter.\\n- The IOPS of tenant 4 does not exceed 5,000 because its maximum IOPS is limited by using the `MAX_IOPS` parameter.\\n- Regardless of the load changes, the IOPS ratio between tenant 1 and tenant 2 is always 2:1 as defined.\\n\\n![1683280561](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1683280561708.png)![1683280562](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1683280562322.png)\\n\\nThen, we verified the disk I/O isolation between loads in a tenant. Set four types of loads in tenant 2. Table 2 describes the resource configurations of the loads. Figure 2 shows the test results.\\n\\n- The IOPS of Load B remains about 2,000, even if its weight is 0. This is because 97% of the minimum IOPS resources of the tenant is reserved for Load B by using the `MIN_PERCENT` parameter.\\n- The IOPS of Load A remains about 1,000. This is because the `MAX_PERCENT` parameter is set to `1` for Load A. In this way, Load A can use only 1% of the maximum resources of the tenant.\\n- The IOPS ratio between Load C and Load D is always 2:1, which conforms to their weight ratio of 50:25.\\n\\n![1683280606](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1683280606307.png)![1683280606](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1683280606875.png)\\n\\nThe preceding tests show that OceanBase Database supports disk I/O isolation between tenants and between loads in a tenant, and meets the three isolation semantics of reservation, limitation, and proportion.\\n\\n### **Adjust disk I/O isolation configurations in real time**\\n\\nSome may have noticed that the disk I/O isolation configurations remain unchanged in the preceding tests. Does OceanBase Database support real-time adjustment of the isolation configurations? The answer is \\"Yes\\". The following test will prove it.\\n\\nPrepare a large table and perform a full-table scan with a parallel query. During the scan, change the value of the `MAX_IOPS` parameter for the tenant repeatedly as the administrator. The video shows that the IOPS monitored by the operating system changes constantly.\\n\\nYou may have noticed that the IOPS monitored by the operating system is always lower than the value specified by the administrator. This is because OceanBase Database normalizes the overhead of I/O requests.\\n\\nFor example, the overhead of 64 KB random reads is different from that of 4 KB random reads. The baseline IOPS overhead specified in the unit config of the tenant is 16 KB random reads. However, the actual size of I/O requests is about 20 KB. After overhead calculation, the IOPS monitored by the operating system is different. For more information, see the related code of ob_io_manager.\\n\\n## V. Afterword\\n\\nThe resource isolation capability of OceanBase Database V4.x allows you to flexibly control the resources allocated to different loads. We will make every effort to improve this capability to address user concerns, such as the unit config and number of resource units of the tenant. OceanBase Database is devoted to providing a better resource isolation capability and user experience. When the business traffic changes, OceanBase Database can automatically allocate the required resources, like a standalone database with unlimited resources. It must be a long haul to reach that goal, but we are resolved and ready to push through all the challenges.\\n\\nFinally, feel free to share with us your comments on disk I/O isolation."},{"id":"miniaturization","metadata":{"permalink":"/blog/miniaturization","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/feat-miniaturization.md","source":"@site/blog/feat-miniaturization.md","title":"OceanBase 4.0 interpretation: Reduce the threshold of distributed database use, talk about our thinking on small specifications","description":"Author | Zhao Yuzhong, a senior technical expert of OceanBase. He joined Alipay in 2010 to help with the R&D of the distributed transaction framework, and has engaged in the R&D of storage engines as an OceanBaser since 2013.","date":"2024-06-03T13:23:07.000Z","tags":[],"readingTime":12.32,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"miniaturization","title":"OceanBase 4.0 interpretation: Reduce the threshold of distributed database use, talk about our thinking on small specifications"},"unlisted":false,"prevItem":{"title":"OceanBase provides users with sufficiently flexible and simple I/O resource isolation experiences.","permalink":"/blog/io-isolation"},"nextItem":{"title":"OceanBase 4.3 - Milestone release for real-time AP analysis","permalink":"/blog/ob-430"}},"content":"> **Author | Zhao Yuzhong, a senior technical expert of OceanBase.** He joined Alipay in 2010 to help with the R&D of the distributed transaction framework, and has engaged in the R&D of storage engines as an OceanBaser since 2013.\\n\\nWith the emergence of more scenarios and the growth of data volume in recent years, distributed databases have rapidly spread across a variety of sectors, providing great solutions for data-intensive and high-concurrency applications with their technical capabilities such as data consistency, high availability, and elastic scaling. A distributed database is often deployed on multiple servers to ensure high availability and performance. Therefore, to handle small-scale simple scenarios in the early days of their business, users tend to deploy a centralized database that costs less and exhibits higher performance if small specifications. The problem is, sooner or later, the centralized database will be bottlenecked as the business size grows, and adjustments or restructuring of the database architecture by then can be extremely challenging and costly.\\n\\n\x3c!-- truncate --\x3e\\n\\nOceanBase Database Community Edition V4.0 was released at the Apsara Conference 2022. It is the industry\'s first MySQL-compatible integrated database that supports both standalone and distributed deployment modes. This version provides many much-expected capabilities, such as enhanced online analytical processing (OLAP) capabilities. Featuring an integrated architecture, it can be deployed in standalone mode with a few clicks and can stably run in a production system with small hardware specifications, such as four CPU cores and 16 GB of memory (4C16G). This reduces the deployment costs and improves its usability. We hope that the dual technical advantages of the integrated architecture can bring perpetual benefits for database users.\\n\\nAccording to their feedback, users are highly interested in the integrated architecture of OceanBase Database Community Edition V4.0 and its support for small-specification deployment. We believe that small-specification deployment is not only about providing all necessary features in standalone mode. More importantly, it delivers higher performance with the same hardware configuration. In this article, we will, from the following three perspectives, share our thoughts on small-sized distributed databases, and our innovative ideas and solutions about the integrated architecture that supports both standalone and distributed deployment:\\n\\n- Reasons for choosing a small-specification distributed database\\n- Key techniques for small-specification deployment\\n- Performance of OceanBase databases with small specifications\\n\\n## I. Why a distributed database with small specifications?\\n\\nOver the past decade or so since its founding in 2010, OceanBase has broken the world records in TPC-C and TPC-H tests, empowered the Double 11 shopping festival every year, and ensured that every transaction was safely and efficiently executed. Pushing through all kinds of challenges, OceanBase Database, as a fully self-developed native distributed database, has proved its scalability and stability. From OceanBase Database V2.2 that topped the TPC-C ranking for the first time with 203 Elastic Compute Service (ECS) servers, to a later version that took the crown again with 1,554 ECS servers, the performance of OceanBase Database rises linearly with the number of servers.\\n\\nOn the other hand, as OceanBase Database caught the attention from industries other than the financial sector, we realized that not all users were faced by the amount of data comparable to the Double 11. In fact, standalone databases are just enough to tick all the boxes of many users in the early days of their business, when the data volume is rather small. Therefore, it is a great help to provide minimal database specifications for users to begin with. In this way, users are able to break in at very low costs. Also, with the great scalability of OceanBase Database, users can flexibly scale out their database systems later to take care of the increasing data volume and performance requirements.\\n\\n**\u258b From small to large: Basic database requirements in a business that grows**\\n\\nThe latest OceanBase Database V4.0 supports a minimum deployment specification of 4C8G. What does 4C8G mean? It\'s just a typical configuration of a nice laptop. In other words, OceanBase Database V4.0 can be deployed and stably run on a personal computer.\\n\\nAs user business grows, OceanBase Database V4.0 can be scaled out to support changing needs over the entire lifecycle of the business. OceanBase Database V4.0 helps users find better solutions to cost reduction, efficiency improvement, and business innovation.\\n\\n- In its early days, user business handles small amount of data and has few requirements on disaster recovery. The user can deploy and run OceanBase Database V4.0 on a single server and perform cold backup regularly to protect its data system from possible disasters.\\n- As its business grows, the user can vertically scale up the specifications of the existing server. To meet its requirements on disaster recovery, the user can add another server to build a primary/standby architecture, which provides the online disaster recovery capability. (Manual intervention is still required during disaster recovery due to the limits of the primary/standby architecture.)\\n- When its business expands to certain size and data becomes more important, the user can simply upgrade to the three-replica architecture, which ensures high availability with three servers and supports automatic disaster recovery. When a server fails, the three-replica architecture of OceanBase Database V4.0 guarantees business recovery in 8s with zero data loss. In other words, the recovery time objective (RTO) is less than 8s and the recovery point objective (RPO) is 0.\\n- When user business experiences even greater growth and each server has been upgraded to the highest configurations, the user has to deal with this \\"happy trouble\\" as Taobao and Alipay did. In this case, the transparent distributed scalability of OceanBase Database allows the user to scale its cluster out from 3 to 6, 9 or even thousands of servers.\\n\\n![image.png](https://s2.loli.net/2024/06/04/gLRmXzs5VMCBT1A.webp)\\n\\nFigure 1 Deployment evolution: OceanBase Database vs conventional databases\\n\\n**\u258b Smooth transitions that ensure linear performance improvement**\\n\\nThe integrated architecture of OceanBase Database supports smooth transition from standalone to distributed multi-cluster deployment mode, keeping the performance improvement at a linear speed.\\n\\nThanks to the good vertical scalability of OceanBase Database, the configuration upgrade of the server in standalone mode usually achieves linear performance improvement. When a user scales a distributed cluster from 3 to 6 servers, for example, distributed transactions are often introduced, which, in most cases, results in performance loss. However, OceanBase Database reduces the probability of distributed transactions through a variety of mechanisms, such as the TableGroup mechanism that binds multiple tables together, and the well-designed load balancing strategies.\\n\\nThe good distributed scalability of OceanBase Database also helps maintain linear performance improvement as the number of servers increases. For example, in the TPC-C test, which involves about 10% of distributed transactions, the performance improvement of OceanBase Database remained linear as more nodes were added to the cluster.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/f419efdd-d5b7-4a33-9afe-c7e0107c71f5/image/2023-01-11/d6979385-99d3-4ed9-a378-d2a8962e3342.png)\\n\\nFigure 2 Performance of OceanBase Database with different number of nodes in the TPC-C test\\n\\nMore importantly, all operations performed in scaling from a standalone OceanBase database to an OceanBase cluster of thousands of nodes are transparent to the business. Users do not need to modify the code of their upper-level business applications, or manually migrate their operation data. If you use OceanBase Cloud, you can perform backup, scaling, and O&M operations all on the same platform, which is quite convenient.\\n\\nFrom the first day of the development of OceanBase Database V4.0, we have been thinking about how to run a distributed database on small-specification hardware, yet delivering high performance, so that users benefit from cost-effective high availability in their respective scenarios. OceanBase Database V4.0 not only provides all necessary features in standalone mode, and also delivers higher performance with the same hardware configuration.\\n\\n## II. Key techniques for small-specification deployment\\n\\nIn the fundamental software sector, it is very hard to make a database system \\"large\\" because the system will be increasingly vulnerable to failures as more nodes are added to it. In our second TPC-C test, for example, we built an OceanBase cluster of 1,554 Elastic Compute Service (ECS) servers. In such a cluster, the frequency of a single-server failure is about once a day or every other day. The point is we have to make the product sufficiently stable and highly available to keep such a jumbo-sized cluster up and running.\\n\\nIt is equally hard to make a database system \\"small\\" because it requires getting down to every detail, much like using a microscope to arrange the usage of every slice of resource. Not only that, some proper designs or configurations in a large system may be totally unacceptable in a smaller one. What\'s more challenging is that we must make the system suitable for both large and small hardware specifications. This requires us to weigh up between large and small specifications when designing the database system, so as to minimize the additional overhead of a distributed architecture while allowing the database system to make adaptive response according to hardware specifications in many scenarios. Now, let\'s talk about the technical solution of OceanBase Database V4.0 by taking the usage of CPU and memory, the two major challenges, as an example.\\n\\n**\u258b Reducing CPU utilization through dynamic control of log streams**\\n\\nTo build a small database, OceanBase Database V4.0 needs to control the CPU utilization in the first place. In versions earlier than V4.0, OceanBase Database would generate a Paxos log stream for each partition of a data table to ensure the data consistency among multiple replicas based on the Paxos protocol. This is a very flexible design because Paxos groups are based on partitions, which means that partitions can be migrated between servers. However, this design puts heavy workload on the CPU because each Paxos log stream consumes overhead for leader selection, heartbeat, and log synchronization. Such additional overhead occupies a moderate percentage of the CPU resource if servers have large specifications, or the number of partitions is small, but causes an unbearable burden for small-specification servers.\\n\\nHow do we solve that issue in OceanBase Database V4.0? We go straight-forward and reduce the number of Paxos log streams. If we can reduce the number of Paxos log streams to the same as that of servers, the overhead for Paxos log streams is roughly equal to that for logs in a conventional database in the primary/standby mode.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/a248fea2-2640-432c-8c0e-68f394e9611b/image/2023-01-11/5830c47b-96eb-4303-9398-8f1b080610a4.png)\\n\\nFigure 3 Dynamic log streams of a cluster based on OceanBase Database V4.0\\n\\nOceanBase Database V4.0 generates a Paxos log stream for multiple data table partitions and dynamically controls the log streams. As shown in the figure above, the database cluster consists of three zones, and each zone has two servers deployed. Assume that two resource units are configured for a tenant. In this case, two Paxos log streams are generated for the tenant, with one containing P1, P2, P3, and P4 partitions and the other containing P5 and P6 partitions.\\n\\n- When the two servers are not load-balanced, the load balancing module of OceanBase Database migrates the partitions between the Paxos log streams.\\n\\n- To scale out the cluster, a user can split one Paxos log stream into multiple Paxos log streams and migrate them as a whole.\\n\\n- To scale in the cluster, the user can migrate multiple Paxos log streams and merge the streams.\\n\\nWith dynamic log stream control, OceanBase Database V4.0 greatly reduces the CPU overhead of the distributed architecture and guarantees high availability and flexible scaling.\\n\\n**\u258b Achieving high concurrency with a small memory space through dynamic metadata loading**\\n\\nThe second challenge that OceanBase Database V4.0 needs to take in building a small database is to optimize memory usage. For the sake of performance, OceanBase Database of versions earlier than V4.0 stored some metadata in memory. The memory usage of this portion of metadata was not high if the total memory size was large, but unacceptable for a small-specification server. To support ultimate performance at small specifications, we have achieved dynamic loading of all metadata in OceanBase Database V4.0.\\n\\n![5D9183AF-7D02-40D3-8EAE-2173F2025126.png](https://s2.loli.net/2024/06/04/q4N9U5Vz2GAQHTM.png)\\n\\nFigure 4 SSTable hierarchical storage\\n\\nAs shown in the figure above, we store an SSTable in a hierarchical structure. To be specific, we store the microblocks of the SSTable in partitions and maintain only the handle of the partitions in memory. The requested data is dynamically loaded by using KVCache only when the partitions need to be accessed. In this way, OceanBase Database V4.0 is capable of processing highly concurrent requests for massive amount of data with a small memory size.\\n\\n## III. Performance of databases with small specifications\\n\\nTo test the actual performance of OceanBase Database with small specifications, we deployed OceanBase Database Community Edition V4.0 in 1:1:1 mode based on three 4C16G servers and compared its performance with that of RDS for MySQL 8.0, which was also deployed on 4C16G servers. The comparison was performed by using Sysbench and the results show that OceanBase Database Community Edition V4.0 outperforms RDS for MySQL 8.0 in most data processing scenarios. In particular, under the same hardware specifications, OceanBase Database Community Edition V4.0 handles a throughput 1.9 times that of RDS for MySQL 8.0 in INSERT and UPDATE operations.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/22580914-c626-4968-b2ec-14b36e73a7dc/image/2023-01-11/74429483-aee7-4cb6-9cdf-0f5c43646d01.png)\\n\\nFigure 5 Throughput performance test results of OceanBase Database Community Edition V4.0 and RDS for MySQL 8.0 on Sysbench (4C16G)\\n\\nWe also compared the two at specifications of 8C32G, 16C64G, and 32C128G, which are most popular among users. As the server specifications increase, the performance gap widens between OceanBase Database Community Edition V4.0 and RDS for MySQL 8.0. At 32C128G specifications, OceanBase Database Community Edition V4.0 achieves a throughput 4.3 times that of RDS for MySQL 8.0 with 75% less response time.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/9a3fa66e-7716-48e3-bcf2-82735c4726c9/image/2023-01-11/b4324d1e-6bd1-48cb-9ccc-c100cca6aae6.png)\\n\\nFigure 6 Throughput performance test results of OceanBase Database Community Edition V4.0 and RDS for MySQL 8.0 on Sysbench\\n\\n![lQLPJyDF79Hqr-3NBTjNDHCwRri-CRupihMGSPeKlXwaAA_3184_1336.png](https://s2.loli.net/2024/06/04/vLwU6FWcVfNKy7M.png)\\n\\nTable 1 Performance (throughput and response time) test results of OceanBase Database Community Edition V4.0 and RDS for MySQL 8.0 on Sysbench\\n\\n## Afterword\\n\\nOceanBase Database has achieved ultimate performance in the TPC-C test with a massive cluster of more than a thousand servers, and ultimate resource usage in standalone performance tests at small specifications, such as 4C16G. What\'s behind those achievements is our unshakable faith in our mission to make data management and use easier. Streamlining services for customers with every effort is the motto of every OceanBase engineer. Growing fast, OceanBase Database is not yet perfect. We still have a lot to do to optimize its performance with higher specifications and save more resources in a database with even smaller specifications. OceanBase Database Community Edition V4.0 is now available and we are looking forward to working with all users to build a general database system that is easier to use.\\n\\nWelcome to [OceanBase Community](https://open.oceanbase.com/blog). We will keep generating useful content, and pursue excellence together with tens of millions of developers.\\n\\n\ud83d\udd0dJoin us on DingTalk (Group ID: 33254054), or scan the QR code below to contact OceanBase Technical Support. We are ready to answer all questions about our products.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/f4d95b17-3494-4004-8295-09ab4e649b68/image/2022-08-29/00ff7894-c260-446d-939d-f98aa6648760.png)"},{"id":"ob-430","metadata":{"permalink":"/blog/ob-430","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/feat-ob-430.md","source":"@site/blog/feat-ob-430.md","title":"OceanBase 4.3 - Milestone release for real-time AP analysis","description":"1713848983","date":"2024-06-03T13:23:07.000Z","tags":[],"readingTime":24.205,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"ob-430","title":"OceanBase 4.3 - Milestone release for real-time AP analysis"},"unlisted":false,"prevItem":{"title":"OceanBase 4.0 interpretation: Reduce the threshold of distributed database use, talk about our thinking on small specifications","permalink":"/blog/miniaturization"},"nextItem":{"title":"Why is resource isolation important for HTAP?","permalink":"/blog/resource-isolation"}},"content":"![1713848983](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713848982839.png)\\n\\nIn early 2023, OceanBase Database V4.1 was released. It is the first milestone version of the V4.x series and supports an integrated architecture for standalone and distributed modes. Such integrated architecture reduces the recovery time objective (RTO), a database reliability indicator, to less than 8 seconds, ensuring rapid database recovery from an unexpected failure. Unlike the V3.x series, the new version does not limit the number of partitions, providing higher capacity for processing large transactions. Core features such as the arbitration replica are supported to cut costs.\\n\\nIn September 2023, OceanBase Database V4.2.1 was released. As the first long-term supported (LTS) version of the V4.x series, it augments all core features of the V3.x series, and demonstrates improved performance in many aspects such as stability, scalability, support for small specifications, and ease of diagnostics. Six months after its release, hundreds of customers have deployed this LTS version in their production environments for stable operations.\\n\\n\x3c!-- truncate --\x3e\\n\\nTo meet higher expectations on ease-of-use and capabilities of tackling miscellaneous workloads, we have released OceanBase Database V4.3.0, which is rigorously implemented on top of open design after thorough research.\\n\\nOceanBase Database V4.3.0 sets a significant milestone on our roadmap to achieve real-time analytical processing (AP). This version provides a columnar engine based on the log-structured merge-tree (LSM-tree) architecture, which implements hybrid columnar and row-based storage. The database also introduces a new vectorized engine based on column data format descriptions and a cost model based on columnar storage. This way, wide tables can be effectively processed and the query performance in AP scenarios is significantly improved without affecting transactional processing (TP) business scenarios. Overall, the new OceanBase Database version is well-suited for mixed workload scenarios involving complex analytics, real-time reporting, real-time data warehousing, and online transactions. The materialized view feature is provided. Query results are pre-calculated and stored in materialized views to improve real-time query performance, and support rapid report generation and data analysis. The kernel in the new version also extends online DDL and adds support for tenant cloning. It has optimized performance and system resource usage, and provides better system usability. In a test with the same hardware configurations, the performance of OceanBase Database V4.3.0 in wide-table queries is comparable with mainstream columnstore databases in the industry.\\n\\nNow, let\'s take a closer look at key updates of OceanBase Database V4.3.0:\\n\\n\u25cb TP and AP integration\\n\\n\u25cb High-performance kernel\\n\\n\u25cb Higher computing performance\\n\\n\u25cb Ease-of-use enhancements\\n\\n## **I. TP and AP integration**\\n\\nIn addition to features of V4.2, such as highly concurrent real-time row updates, and point queries of the primary key indexes, OceanBase Database V4.3.0 introduces more AP services. Its scalable distributed architecture also supports high availability, strong consistency, and geo-disaster recovery. The new version provides a columnar engine and enhances vectorized execution, parallel computing, and distributed plan optimization. This way, the database supports both TP and AP business.\\n\\n### **(1) Integrated columnar and row-based storage**\\n\\nColumnar storage is one of the key capabilities of AP databases in complex large-scale data analysis and ad hoc queries of massive data. Columnar storage is a way to organize data files. Different from row-based storage, columnar storage physically arranges data in a table by column. When data is stored by column, the system can scan only the columns involved in the query and calculation, instead of scanning the entire row. This way, the consumption of resources such as I/O and memory is reduced and the calculation is accelerated. Moreover, columnar storage naturally provides better data compression conditions, making it easier to achieve higher compression ratios, thereby reducing storage space and network transmission bandwidth.\\n\\nHowever, columnar engines generally assume limited random updates and attempt to ensure that data in columnar storage is static. When a large amount of data is updated randomly, system performance will inevitably degrade. The LSM-tree architecture of OceanBase Database can process baseline data and incremental data separately, and therefore can solve the performance issue. Therefore, OceanBase Database V4.3.0 supports the columnar engine based on the current architecture, implementing integrated columnar and row-based data storage on an OBServer node with only one set of code and one architecture, and ensuring the performance of both TP and AP queries.\\n\\nTo help users with AP requirements smoothly use the new version, OceanBase Database has adapted and optimized several modules, including the optimizer, executor, DDL, and transaction processing, for the columnar engine. These optimizations introduce a new cost model and vectorized engine based on columnar storage, enhancements to the query pushdown feature, and features like skip index, a new column-based encoding algorithm, and adaptive compactions.\\n\\nTo make AP queries easy, we recommend that you run the following command in a MySQL or Oracle tenant of OceanBase Database to create a columnstore table by default:\\n\\n```SQL\\n    alter system set default_table_store_format = \\"column\\"\\n```\\n\\nYou can flexibly create a business table as a rowstore table, columnstore table, or hybrid rowstore-columnstore table based on the load type. You can also create a columnstore index for a rowstore table.\\n\\n![1713849286](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713849285226.png)\\n\\n![1713849297](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713849296536.png)\\n\\nThe optimizer determines, based on estimated costs, whether to scan a hybrid rowstore-columnstore table by row or by column.\\n\\n![1713849311](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713849310310.png)\\n\\n### **(2) New vectorized engine**\\n\\nEarlier versions of OceanBase Database have implemented a vectorized engine based on uniform data format descriptions, offering performance significantly better than that of non-vectorized engines. However, the engine still has some performance deficiencies in deep AP scenarios. OceanBase Database V4.3.0 implements the vectorized engine 2.0, which is based on column data format descriptions, avoiding the memory usage, serialization, and read/write access overhead caused by ObDatum maintenance. Based on the reconstruction of data format descriptions, the new vectorized engine also reimplements more than 10 commonly used operators such as HashJoin, AGGR, HashGroupBy, and Exchange (DTL Shuffle), as well as over 20 MySQL expressions including relational operations, logical operations, and arithmetic operations. Subsequent V4.3.x versions will further improve and implement other operators and expressions based on the new vectorized engine to achieve better performance.\\n\\n### **(3) Materialized views**\\n\\nOceanBase Database V4.3.0 introduces materialized views. Materialized views are a key feature for AP business scenarios. By precomputing and storing the query results of views, real-time calculations are reduced to improve query performance and simplify complex query logic. Materialized views are commonly used for rapid report generation and data analysis scenarios.\\n\\nMaterialized views need to store query result sets to optimize the query performance. Due to data dependency between a materialized view and its base tables, data in the materialized view must be refreshed accordingly when data in any base tables changes. Therefore, the materialized view refresh mechanism is also introduced in the new version, including complete refresh and incremental refresh strategies. Complete refresh is a relatively direct method. Each time a refresh operation is performed, the system re-executes the corresponding query statement of a materialized view to recalculate and overwrite the original result set. This method is applicable to scenarios with a small amount of data. Incremental refresh, by contrast, only deals with data that has been changed since the last refresh. To achieve accurate incremental refresh, OceanBase Database implements a materialized view log feature that is similar to Oracle Materialized View Log (MLOG). The feature tracks incremental data updates in base tables and records the updates in logs. This ensures that materialized views can be refreshed incrementally in a short period. Incremental refresh is particularly useful in business scenarios with large data volume and frequent data changes.\\n\\n## **II. High-performance kernel**\\n\\nThe kernel in the new version has enhanced the cost model, added support for tenant cloning, extended online DDL, and Amazon Simple Storage Service (S3) as the backup and restore media, restructured the session management module, and optimized log stream state machine and system resource usage, to improve database performance and stability in handling key business workloads.\\n\\n### **(1) Enhanced row estimation system**\\n\\nAs the OceanBase Database version evolves, more cost estimation methods are available for the optimizer. For row estimation of each operator, a variety of algorithms, such as row estimation based on the storage layer, row estimation based on statistics, dynamic sampling, and default statistics, are supported. However, there are no clear strategies and complete control methods for using row estimation. OceanBase Database V4.3.0 reconstructs the row estimation system. Specifically, it prioritizes row estimation strategies based on scenarios and provides methods such as hints and system variables for you to manually intervene the selection of a row estimation strategy. This version also enhances the predicate selectivity and number of distinct values (NDV) calculation framework to improve the accuracy of cost estimation by the optimizer.\\n\\n### **(2) Enhanced statistics**\\n\\nOceanBase Database V4.3.0 improves the statistics feature, statistics collection performance, and the compatibility and usability of statistics. Specifically, this version reconstructs the offline statistics collection process to improve the collection efficiency, optimizes the statistics collection strategies to automatically collect information about index histograms by default and collect statistics in a deductive manner, and ensures transaction consistency for online statistics collection. It is compatible with the `DBMS_STATS.COPY_TABLE_STATS` procedure of Oracle for statistics copying, and is also compatible with the `ANALYZE TABLE` statement of MySQL. It provides a command to cancel statistics collection, enriches the monitoring on the statistics collection progress, and enhances maintenance usability. It also supports the parallel deletion of statistics.\\n\\n### **(3) Adaptive cost model**\\n\\nIn earlier versions of OceanBase Database, the cost model uses constant parameters measured by internal machines to represent hardware system statistics, and describes the execution overhead of each operator by using a series of formulas and constant parameters. However, in real business scenarios, different hardware environments may have different CPU clock frequencies, sequential or random read speeds, and NIC bandwidths, thereby resulting in cost estimation deviations. The optimizer cannot generate optimal plans in different business environments because of these deviations. The new version implements the cost model in an optimized way to support the `DBMS_STATS` package for collecting or setting system statistics coefficients, thus adapting the cost model to hardware. It also provides the `DBA_OB_AUX_STATISTICS` view to display the system statistics coefficients of the current tenant.\\n\\n### **(4) Fixed session variables for function-based indexes**\\n\\nWhen a function-based index is created on a table, a hidden virtual generated column is added to the table and defined as the index key of the function-based index. The values of the virtual generated column are stored in the index table. The results of some built-in system functions are affected by session variables. The calculation result of a function varies based on the values of session variables, even if the input arguments are the same. When a function-based index or generated column is created in this version, session variables on which the function-based index or generated column depends are fixed in the column schema to improve stability. When values of the indexed column or generated column are calculated, fixed session variable values are used. Therefore, the calculation result is not affected by variable values in the current session. OceanBase Database V4.3.0 supports fixed values of the system variables `timezone_info`, `nls_format`, `nls_collation`, and `sql_mode`.\\n\\n### **(5) Online DDL expansion in MySQL mode**\\n\\nOceanBase Database V4.3.0 supports more online DDL scenarios for column type changes, including:\\n\\n\u25cb Conversion of integer types: Online DDL operations, instead of offline DDL operations, are performed to change the data type of a primary key column, index column, generated column, column on which a generated column depends, or column with a `UNIQUE` or `CHECK` constraint to an integer type with a larger value range.\\n\\n\u25cb Conversion of the DECIMAL data type: For columns that support the DECIMAL data type, online DDL operations are performed to increase the precision within any of the \\\\[1,9\\\\], \\\\[10,18\\\\], \\\\[19,38\\\\], and \\\\[39,76\\\\] ranges without changing the scale.\\n\\n\u25cb Conversion of the BIT or CHAR data type: For columns that support the BIT or CHAR data type, online DDL operations are performed to increase the width.\\n\\n\u25cb Conversion of the VARCHAR or VARBINARY data type: For columns that support the VARCHAR or VARBINARY data type, online DDL operations are performed to increase the width.\\n\\n\u25cb Conversion of the LOB data type: To change the data type of a column that supports LOB data types to a LOB data type with a larger value range, offline DDL operations are performed for columns of the TINYTEXT or TINYBLOB data type, and online DDL operations are performed for columns of other data types.\\n\\n\u25cb Conversion between the TINYTEXT and VARCHAR data types: For columns that support the TINYTEXT data type, online DDL operations are performed to change the VARCHAR(x) data type to the TINYTEXT data type if `x <= 255`, and offline DDL operations are performed if otherwise. For columns that support the VARCHAR data type, online DDL operations are performed to change the TINYTEXT data type to the VARCHAR(x) data type if `x >= 255`, and offline DDL operations are performed if otherwise.\\n\\n\u25cb Conversion between the TINYBLOB and VARBINARY data types: For columns that support the TINYBLOB data type, online DDL operations are performed to change the VARBINARY(x) data type to the TINYBLOB data type if `x <= 255`, and offline DDL operations are performed if otherwise. For columns that support the VARBINARY data type, online DDL operations are performed to change the TINYBLOB data type to the VARBINARY(x) data type if `x >= 255`, and offline DDL operations are performed if otherwise.\\n\\n### **(6) Globally unique client session ID**\\n\\nPrior to OceanBase Database V4.3.0 and OceanBase Database Proxy (ODP) V4.2.3, when the client executes `SHOW PROCESSLIST` through ODP, the client session ID in ODP is returned. However, when the client queries the session ID by using an expression such as `connection_id` or from a system view, the session ID on the server is returned. A client session ID corresponds to multiple server session IDs. This causes confusion in session information queries and makes user session management difficult. In the new version, the client session ID generation and maintenance process is reconstructed. When the version of OceanBase Database is not earlier than V4.3.0 and the version of ODP is not earlier than V4.2.3, the session IDs returned by various channels, such as the `SHOW PROCESSLIST` command, the `information_schema.PROCESSLIST` and `GV$OB_PROCESSLIST` views, and the `connection_id`, `userenv(\'sid\')`, `userenv(\'sessionid\')`, `sys_context(\'userenv\',\'sid\')`, and `sys_context(\'userenv\',\'sessionid\')` expressions, are all client session IDs. You can specify a client session ID in the SQL or PL command `KILL` to terminate the corresponding session. If the preceding version requirements for OceanBase Database and ODP are not met, the handling method in earlier versions is used.\\n\\n### **(7) Improvement of the log stream state machine**\\n\\nIn OceanBase Database V4.3.0, the log stream status is split into the in-memory status and persistent status. The persistent status indicates the life cycle of a log stream. After the OBServer node where a log stream resides breaks down and then restarts, the system determines whether the log stream should exist and what the in-memory status of the log stream should be based on the persistent status of the log stream. The in-memory status indicates the runtime status of a log stream, representing the overall status of the log stream and the status of key submodules. Based on the explicit status and status sequence of the log stream, underlying modules can determine which operations are safe to the log stream and whether the log stream has gone through a status change of the ABA type. For backup and restore or migration processes, the working status of a log stream is optimized after the OBServer node where the log stream resides restarts. This feature improves the stability of log stream-related features and enhances the concurrency control on log streams.\\n\\n### **(8) Tenant cloning**\\n\\nOceanBase Database V4.3.0 supports tenant cloning. You can quickly clone a specified tenant by executing an SQL statement in the sys tenant. After a tenant cloning job is completed, the created new tenant is a standby tenant. You can convert the standby tenant into the primary tenant to provide services. The new tenant and the source tenant share physical macroblocks in the initial state, but new data changes and resource usage are isolated between the tenants. You can clone an online tenant for temporary data analysis with high resource consumption or other high-risk operations to avoid risking the online tenant. In addition, you can also clone a tenant for disaster recovery. When irrecoverable misoperations are performed in the source tenant, you can use the new tenant for data rollback.\\n\\n### **(9) Support for S3 as the backup and restore media**\\n\\nEarlier versions of OceanBase Database support two types of storage media for backup and restore: file storage (NFS) and object storage such as Alibaba Cloud Object Storage Service (OSS) and Tencent Cloud Object Storage (COS). The new version supports Amazon Simple Storage Service (S3) and S3-compatible object storage like Huawei Cloud Object Storage Service (OBS) and Google Cloud Storage (GCS) as the log archive and data backup destination. You can also use backup data on S3 and S3-compatible object storage for physical restore.\\n\\n### **(10) Proactive broadcast/refresh of tablet locations**\\n\\nIn earlier versions, OceanBase Database provides the periodic location cache refresh mechanism to ensure that the location information of log streams is updated in real time and consistent. However, tablet location information can only be passively refreshed. Changes in the mappings between tablets and log streams can trigger SQL retries and read/write errors with a certain probability. OceanBase Database V4.3.0 supports proactive broadcast of tablet locations to reduce SQL retries and read/write errors caused by changes in mappings after transfer. It also supports proactive refresh to avoid unrecoverable read/write errors.\\n\\n### **(11) Migration of active transactions during tablet transfer**\\n\\nIn the design of standalone log streams, data is in the unit of tablets, while logs are in the unit of log streams. Multiple tablets are aggregated into one log stream, saving the high cost of two-phase commit of transactions within a single log stream. To balance data and traffic among different log streams, tablets can be flexibly transferred between log streams. However, during the tablet transfer process, active transactions may still be handling the data, and even a simple operation may damage the atomicity, consistency, isolation, and durability (ACID) of the transactions. For example, if active transaction data on the transfer source cannot be completely migrated to the transfer destination during concurrent transaction execution, the atomicity of the transactions cannot be guaranteed. In earlier versions, active transactions were killed during the transfer to avoid transaction problems. This mechanism affects the normal execution of transactions to some extent. To solve this problem, the new version supports the migration of active transactions during tablet transfer, which enables concurrent execution of active transactions and ensures that no abnormal rollbacks or consistency issues occur in concurrent transactions due to the transfer.\\n\\n### **(12) Memory throttling mechanism**\\n\\nPrior to OceanBase Database V4.x, only a few modules release memory based on freezes and minor compactions, and the MemTable is the largest part among them. Therefore, in earlier versions, an upper limit is set for memory usage of the MemTable, enabling it to run as smoothly as possible within the memory usage limit and avoiding writing failures caused by sudden memory exhaustion. In OceanBase Database V4.x, more modules that release memory based on freezes and minor compactions are introduced, such as the transaction data module. The new version provides more refined means to control the memory usage of various modules and supports the memory upper limit control of TxData and metadata service (MDS) modules. The two modules share memory space with the MemTable. When the sum of the memory usage of the three modules reaches `Tenant memory \xd7 _tx_share_memory_limit_percentage% \xd7 writing_throttling_trigger_percentage%`, overall memory throttling is triggered for the three modules. The new version also supports freezes and minor compactions of the transaction data table by time to reduce the memory usage of the transaction data module. By default, the transaction data table is frozen once every 1,800 seconds.\\n\\n### **(13) Optimization of DDL temporary result space**\\n\\nDuring the DDL operations, many processes may store temporary results in materialized structures. Here are two typical scenarios: (1) During index creation, the system scans data in the base data table and sorts and inserts the obtained data to the index table. If the memory is insufficient during the sorting process, current data in the memory space will be temporarily stored in materialized structures to release the memory space for subsequent scanning. Data in the materialized structures is then merged and sorted. (2) In the columnar storage bypass import scenario, the system first temporarily stores the data to be inserted into each column group in materialized structures, and then obtains data from the materialized structures for insertion. These materialized structures can be used in the `SORT` operator to store intermediate data required for external sorting. When the system inserts data into column groups, the data can be cached in materialized structures, avoiding additional overhead caused by repeated table scanning. As a result, the temporary files occupy considerable disk space. The new version eliminates unnecessary redundant structures to simplify the data flow, and supports encoding and compression of temporary results for storage on disks. This greatly reduces the disk space occupied by temporary files.\\n\\n## **III. Higher computing performance**\\n\\nThe online analytical processing (OLAP) capabilities are significantly enhanced in the new version, achieving a performance boost in TPC-H 1TB and TPC-DS 1TB tests. The new version also optimizes PDML, read and write operations in OBKV, bypass import performance of LOB data, and node restart performance.\\n\\n### **(1) Increased performance in the TPC-H 1TB test**\\n\\nThe following figure shows the performance of a tenant with 80 CPU cores and 500 GB of memory of different OceanBase Database versions in the TPC-H 1TB test. Overall, the performance of V4.3.0 is about 25% higher than that of V4.2.0.\\n\\n![1713849772](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713849771931.png)\\n\\nFigure 1: Performance of V4.3.0 and V4.2.0 in the TPC-H 1TB test\\n\\n### **(2) Increased performance in TPC-DS 1TB test**\\n\\nThe following figure shows the performance of a tenant with 80 CPU cores and 500 GB of memory of different OceanBase Database versions in the TPC-DS 1TB test. Overall, the performance of V4.3.0 is about 111% higher than that of V4.2.0.\\n\\n![1713849829](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713849828408.png)\\n\\nFigure 2: Performance of V4.3.0 and V4.2.0 in the TPC-DS 1TB test\\n\\n### **(3) OBKV performance optimization**\\n\\nCompared with those in V4.2.1, the OBKV single-row read-write performance is improved by about 70%, and the batch read-write performance is improved by 80% to 220%.\\n\\n### **(4) PDML transaction optimization**\\n\\nThe new version implements optimizations at the transaction layer by supporting parallel commit, log replay, and partition-level rollbacks within transaction participants. Compared with earlier V4.x versions, the new version significantly improves the PDML execution performance and scalability in high concurrency scenarios.\\n\\n### **(5) I/O usage optimization for loading tablet metadata**\\n\\nOceanBase Database V4.x supports millions of partitions on a single machine. As the memory may fail to hold the metadata of millions of tablets, OceanBase Database V4.x supports on-demand loading of tablet metadata. OceanBase Database supports on-demand loading of metadata at the partition level and the subclass level within partitions. In a partition, metadata is split into multiple subclasses for hierarchical storage. In scenarios where background tasks require deeper metadata, the data read consumes more I/O resources. These I/O overheads are not a problem for local SSD disks, but may affect system performance when HDD disks or cloud disks are used. OceanBase Database V4.3.0 aggregates frequently accessed metadata in storage, and only one I/O operation is required to access the metadata. This greatly reduces the I/O overhead in zero load scenarios and avoids the impact on foreground query performance caused by background task I/O overhead. In addition, the metadata loading process during the restart of an OBServer node is optimized. Tablet metadata is loaded in batches at the granularity of macroblocks, greatly reducing discrete I/O reads and speeding up the restart by several or even dozens of times.\\n\\n## **IV. Ease-of-use enhancements**\\n\\nThe new version provides the index use monitoring feature to help you identify and delete invalid indexes, and allows you to import a small amount of local data from the client. Features such as LOB INROW threshold configuration, Remote Procedure Call (RPC) authentication certificate management, and parameter resetting are also provided to improve system usability.\\n\\n### **(1) Index use monitoring**\\n\\nWe usually create indexes to improve the query performance of the database. However, more and more indexes are created as data tables are used in more business scenarios by more operators. Unused indexes are a waste of storage space and increase the overhead of DML operations. In this case, you need to drop useless indexes to alleviate burden on the system. However, you can hardly identify all useless indexes by manual efforts. Therefore, OceanBase Database V4.3.0 provides the index monitoring feature. After you enable this feature and set the sampling method, the index usage information that meets the rules is recorded in the memory of a user tenant and refreshed to the internal table once every 15 minutes. You can then query the DBA_INDEX_USAGE view to find out whether an index is referenced and drop useless indexes to release space.\\n\\n### **(2) Local import from the client**\\n\\nOceanBase Database V4.3.0 supports the `LOAD DATA LOCAL INFILE` statement for local import from the client. You can use the feature to import local files through streaming file processing. Based on this feature, developers can import local files for testing without uploading files to the server or object storage, improving the efficiency of importing a small amount of data.\\n\\nNote: To import local data from the client, make sure that:\\n\\na. The version of OceanBase Client (OBClient) is V2.2.4 or later.\\n\\nb. The version of ODP is V3.2.4 or later. If you directly connect to an OBServer node, ignore this requirement.\\n\\nc. The version of OceanBase Connector/J is V2.4.8 or later if you use Java and OceanBase Connector/J.\\n\\nYou can directly use a MySQL client or a native MariaDB client of any version.\\n\\nThe `SECURE_FILE_PRIV` variable is used to specify the server paths that can be accessed by the OBServer node. This variable does not affect local import from a client, and therefore does not need to be specified for local import.\\n\\n### **(3) LOB INROW threshold configuration**\\n\\nBy default, LOB data of a size less than or equal to 4 KB is stored in INROW mode, and LOB data of a size greater than 4 KB is stored in the LOB auxiliary table. In some scenarios, INROW storage provides higher performance than auxiliary table-based storage. Therefore, this version supports dynamic configuration of the LOB storage mode. You can adjust the INROW threshold based on your business needs, provided that the threshold does not exceed the limit for INROW storage.\\n\\n### **(4) RPC authentication certificate management**\\n\\nWhen RPC authentication is enabled for a cluster, for an access request from a client, such as the arbitration service, primary/standby database, or OceanBase Change Data Capture (CDC), you need to place the root CA certificate of the client in the deployment directory of each OBServer node in the cluster, and then perform related configurations. This whole process is complicated. OceanBase Database V4.3.0 supports the internal certificate management feature. You can use the `DBMS_TRUSTED_CERTIFICATE_MANAGER` system package provided in the sys tenant to add, delete, and modify root CA certificates trusted by an OceanBase cluster. The DBA_OB_TRUSTED_ROOT_CERTIFICATE view is also provided in the sys tenant to display the list of client root CA certificates added to OBServer nodes in the cluster and the certificate expiration time.\\n\\n### **(5) Parameter resetting**\\n\\nIn earlier versions, if you want to reset a parameter to the default value, you need to query the default value of the parameter first, and then manually set the parameter to the default value. The new version provides the `ALTER SYSTEM [RESET] parameter_name [SCOPE = {MEMORY | SPFILE | BOTH}] {TENANT [=] \'tenant_name\'}` syntax for you to reset a parameter to the default value. The default value is obtained from the node that executes the statement. You can reset cluster-level parameters or parameters of a specified tenant in the sys tenant. You can also reset parameters for the current user tenant. On OBServer nodes, whether the `SCOPE` option is specified or not does not affect the implementation logic. For a parameter that takes effect statically, the default value is only stored on the disk but not updated to the memory. For a parameter that takes effect dynamically, the default value is stored on the disk and updated to the memory.\\n\\n## **V. Afterword**\\n\\nOceanBase Database V4.3.0 sets a significant milestone on our roadmap to achieve real-time AP. We will keep updating AP features of subsequent versions to overcome challenges in real-world business scenarios.\\n\\nWe would like to thank all our users and developers for their contributions to OceanBase Database V4.3.0. Their valuable suggestions are a powerful driving force that pushes OceanBase forward. We look forward to working with every user and developer in tackling critical workloads, developing modern data architectures, and building better and more user-friendly distributed databases.\\n\\nYou can visit [**Release Notes**](https://www.oceanbase.com/product/oceanbase-database-rn/releaseNote) to learn more about the new OceanBase Database V4.3.0."},{"id":"resource-isolation","metadata":{"permalink":"/blog/resource-isolation","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/feat-resource-isolation.md","source":"@site/blog/feat-resource-isolation.md","title":"Why is resource isolation important for HTAP?","description":"About the author: Xi Huafeng, an OceanBase technical expert, has been dedicated to optimizing the high availability and scalability of databases for 11 years. He helped with the implementation of the Paxos protocol in OceanBase Database and was a member of the OceanBase TPC-C project team. He is now a member of the OceanBase system group, dedicated to building HTAP infrastructure and working on isolation of resources for AP and TP tasks.","date":"2024-06-03T13:23:07.000Z","tags":[],"readingTime":18.305,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"resource-isolation","title":"Why is resource isolation important for HTAP?"},"unlisted":false,"prevItem":{"title":"OceanBase 4.3 - Milestone release for real-time AP analysis","permalink":"/blog/ob-430"},"nextItem":{"title":"Flink CDC + OceanBase integration solution for full and incremental synchronization","permalink":"/blog/flink-and-ob"}},"content":"> About the author: Xi Huafeng, an OceanBase technical expert, has been dedicated to optimizing the high availability and scalability of databases for 11 years. He helped with the implementation of the Paxos protocol in OceanBase Database and was a member of the OceanBase TPC-C project team. He is now a member of the OceanBase system group, dedicated to building HTAP infrastructure and working on isolation of resources for AP and TP tasks.\\n\\nIn [Technical thoughts on the benefits of vectorized engines to HTAP](https://open.oceanbase.com/blog/10900410), we shared views of the OceanBase team on vectorized engines and introduced our ideas of dealing with complex queries by using a vectorized engine.\\n\\nHybrid transaction and analytical processing (HTAP) is supposed to run online transaction processing (OLTP) and online analytical processing (OLAP) in the same system for better performance. It helps with making business decisions in real time and facilitates innovation at lower operational costs. However, parallel running of OLTP and OLAP tends to cause resource contention because they use database resources, such as CPU, memory, and disk, in different ways. Minimizing such contention is the key to achieve HTAP and also the issue to resolve through resource isolation \u2014 the technology to be introduced in this article.\\n\\n\x3c!-- truncate --\x3e\\n\\nWe believe that true HTAP requires complete resource isolation. A database must support logical isolation complementary to physical isolation to help users adjust resource allocation as needed. Resources for core business that requires ultimate independence can be physically isolated, while those for cost-sensitive long-tail business can be logically isolated. In this article, we will share thoughts of the OceanBase team on resource isolation, and explain why this technology is a must for HTAP and what we have done to tackle the challenges of making it work.\\n\\n- Why is resource isolation necessary for HTAP?\\n- How to implement resource isolation for HTAP?\\n- What is achieved by resource isolation in OceanBase Database?\\n\\n**Why is resource isolation necessary for HTAP?**\\n\\nResource isolation lays the groundwork for HTAP\\n\\nTo show the importance of resource isolation, we can put a database beside an operating system. The two are both complex because of their openness of functionality and nature of delivering a higher price-performance ratio. The openness of functionality denotes uncontrollable workloads. For example, a user process or an SQL statement can be used to perform any operations in the system. As for the price-performance ratio, it is important because even a teeny-tiny saving of resources means a lot, given a massive user base. Among all the ways of driving up the cost performance, resource isolation is unarguably the most straightforward one.\\n\\nAfter decades of development, modern operating systems are generally capable of supporting multiple users and Docker, a virtualized application container. Docker-based Kubernetes, for example, has become the de facto standard for business deployment. Databases, on the other hand, are also required to handle multi-tenancy and HTAP. Many companies separate their historical databases from online databases and perform OLAP in historical databases, which not only makes O&M more complicated but also downgrades OLAP efficiency, making it impossible to achieve dynamic balance between OLTP and OLAP with limited hardware resources. As more database instances are being deployed, achieving such balance will only bring more benefits.\\n\\nResource isolation is a requirement naturally derived from the grouping of different workloads. For example, backup tasks at the background and SQL processing tasks at the foreground are grouped because they obviously have different requirements on timeliness. OLTP and OLAP also involve grouping because the two use resources in different ways. In other words, as long as a software system processes objects differently, it naturally classifies them into groups to ensure the quality of service (QoS), which gives rise to the need for resource isolation.\\n\\nResource isolation is critical to the operation stability of a database. There are two typical cases of resource isolation. First, you can reserve resources for important database tasks through resource isolation to prevent the database from being overloaded and crash. Second, users may sometimes hold business with different QoS requirements in the same database. For example, they hold real-time OLTP business and a small number of less import background tasks in the same database. If users agree to expose such information to the database so that the database can isolate resources for the business, the database will be able to run more stably.\\n\\nA classic example of the second case is the isolation between OLAP and OLTP. To avoid interference between OLTP and OLAP, a conventional database tends to be built with more hardware resources so that each business is allocated with sufficient resources, which leads to inefficient resource utilization. To address this issue, we can consolidate multiple databases into one physical database to reduce the O&M complexity and hardware costs.\\n\\nMerging OLTP and OLAP databases into one HTAP database can be considered as a consolidation process. As aforementioned, operating systems have supported multi-user and Docker for long. Is it possible that databases also demand the sharing of physical resources as technology evolves? We believe that as technology evolves and databases grow larger, logical resource isolation will be applied in more scenarios. In the real world, many users run OLTP workloads in parallel with simple OLAP workloads in the same database. However, the performance may not be as expected due to the limited OLAP and resource isolation capabilities of the database.\\n\\nFor example, if the owner of an online store wants to know the best-sellers of a day, it\'s better to perform analysis in the online database. However, if the database does not support resource isolation, the analytical SQL queries may affect online transactions. To ensure the stability of online transactions, it is necessary to scale out the database by introducing more physical resources to keep the business stable. Even so, the analytical SQL queries must be strictly reviewed to prevent them from exhausting all resources.\\n\\nWhich is better: physical or logical isolation?\\n\\nResource isolation is not new. Conventionally, not sharing physical resources is taken as a physical isolation solution. In a database that adopts physical isolation, row-store-based replicas are used for OLTP and column-store-based replicas are used for OLAP under different tenants or within the same tenant. Physical resources for OLAP and OLTP are isolated. If cost is not a consideration, physical isolation is no doubt a better choice.\\n\\nIn the real world, however, costs and utilization of hardware resources are among the concerns of most customers. On the one hand, database hardware is expensive to purchase and maintain and needs to be replaced regularly. On the other hand, if database hardware is used for processing single business, only a minor portion of it is utilized on average. Inefficient use of hardware resources is absolutely a huge waste.\\n\\nTo make full use of hardware resources, logical isolation stands out because physical resources shared by OLAP and OLTP are logically isolated across different tenants or within the same tenant. Instead of a this-or-that choice, we believe that physical isolation and logical isolation are complementary. In view of the possible contention caused by shared resources, however, some worry that resource sharing impairs QoS and is therefore of limited value to users, while others are concerned about whether a perfect resource isolation solution is possible and whether the losses outweigh the benefits if the solution is too complex.\\n\\nWell, on the one hand, we should get out of the box of perfectionism and recognize the obvious customer benefits of basic resource isolation capabilities. On the other hand, let\'s look at this issue from a forward-looking perspective and admit that the logical isolation technology is getting better over time.\\n\\nTherefore, instead of making a choice between physical and logical isolation, an ideal HTAP solution is about finding a balance between absolute physical isolation and share-it-all. Infrastructure software should allow users to choose an isolation solution based on the scenario. It is necessary for database products to support physical and logical resource isolation at all levels.\\n\\n**How to implement resource isolation for HTAP?**\\n\\nBefore implementing resource isolation, we must:\\n\\n- Define resource groups and their QoS. For databases, a tenant is the most common resource group. You can also configure resource groups respectively for OLAP and OLTP.\\n- Develop and implement resource isolation strategies based on the defined QoS.\\n\\nWe will first look at the database management APIs for the database administrator (DBA), analyze the resources to be isolated (those having the greatest business impact), and then describe the isolation solution of OceanBase Database by taking CPU time, inputs and outputs per second (IOPS), and network bandwidth as examples.\\n\\nDefine resource groups and design resource plans for OLTP and OLAP\\n\\nOceanBase Database aims to realize resource isolation between tenants and that between OLTP and OLAP within one tenant.\\n\\nOceanBase Database allows users to define resource specifications of a tenant through unit configuration. Before you create an OceanBase Database tenant, you must create a resource pool and configure resource units in the pool to control the resource usage. If you are not familiar with this concept, go to OceanBase Documentation and take a look at the \\"Cluster and multi-tenant management\\" chapter.\\n\\n      create resource unit box1 max_cpu 4, max_memory 21474836480, max_iops 128, max_disk_size \'5G\', max_session_num 64, min_cpu=4, min_memory=21474836480, min_iops=128;\\n\\nFor users to define resource specifications of OLTP and OLAP within a tenant, OceanBase Database provides management APIs by referring to the classic Resource Manager service of Oracle. We have noted that customers tend to run batch processing tasks during off-peak hours, such as midnight or early morning, when OLTP is unlikely affected by OLAP, and most resources of a cluster can be allocated to OLAP with minimal resources reserved to support essential OLTP tasks. During peak hours in the daytime, the resource isolation plan can be adjusted to ensure sufficient resources for OLTP with minimal resources reserved to support essential OLAP tasks. OceanBase Database allows users to set two plans for resource management in the daytime and at night. You can activate the plans as needed to ensure isolation and maximize resource utilization.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/8834920c-824e-443d-ae10-5032f8445faf/image/2022-09-29/f2a3ed00-30b2-44df-b47e-68b793d59a67.png)\\n\\nFor example, the following syntax defines a daytime resource plan where OLTP (`interactive_group`) and OLAP (`batch_group`) are respectively allocated with 80% and 20% of the resources.\\n\\n         DBMS_RESOURCE_MANAGER.CREATE_PLAN(\\n            PLAN    => \'DAYTIME\',\\n            COMMENT => \'More resources for OLTP applications\');\\n         DBMS_RESOURCE_MANAGER.CREATE_PLAN_DIRECTIVE (\\n            PLAN             => \'DAYTIME\',\\n            GROUP_OR_SUBPLAN => \'interactive_group\',\\n            COMMENT          => \'OLTP group\',\\n            MGMT_P1          => 80,\\n            UTILIZATION_LIMIT => 100);\\n\\n         DBMS_RESOURCE_MANAGER.CREATE_PLAN_DIRECTIVE (\\n            PLAN             => \'DAYTIME\',\\n            GROUP_OR_SUBPLAN => \'batch_group\',\\n            COMMENT          => \'OLAP group\',\\n            MGMT_P1          => 20,\\n            UTILIZATION_LIMIT => 20);\\n\\nAfter the plan is ready, you can execute the following statement to activate it:\\n\\n      ALTER SYSTEM SET RESOURCE_MANAGER_PLAN = \'DAYTIME\';\\n\\nSimilarly, you can define a night resource plan and activate it during off-peak hours.\\n\\nOceanBase Database supports user-based SQL categorization, which is simple but quite effective. You can create a user dedicated to executing analytical SQL queries, so that all SQL queries initiated by this user are processed as OLAP workloads. Also, if the execution of a request does not complete in 5 seconds, OceanBase Database identifies the request as a large query and downgrades its priority.\\n\\nEnsure QoS with `min`, `max`, and `weight`\\n\\nQoS is a security mechanism that guarantees the smooth operation of critical processes when resources are overloaded. We will describe QoS through weight allocation and the definition of upper and lower limits on resources.\\n\\nAs the business traffic fluctuates over time, the QoS description should be flexible. If we use a fixed QoS description, just like specifying a fixed number of CPU cores and I/O bandwidth for Elastic Compute Service (ECS) of Alibaba Cloud, the system is prone to failure during peak hours due to insufficient database capacity.\\n\\nAssume that Tenant A and Tenant B need to share 100 Mbit/s of bandwidth based on principles of resource sharing in off-peak hours and isolation in peak hours without interfering with each other.\\n\\nHow to ensure that resources are preferentially allocated to the tenant with higher priority? We can set the weight ratio between Tenant A and Tenant B to, for example, 1:3 to control the resource allocation. When both tenants need CPU resources, the ratio of CPU time spent on Tenant A and Tenant B will be 1:3. This weight ratio is specified by the `weight` parameter.\\n\\nWhen a system has abundant physical resources, it is possible that a low-weight tenant takes up a lot of resources that it does not need. How to put a cap on it? We can specify the maximum resource usage for each tenant by setting the `max` parameter on top of the weight ratio. For example, with a weight ratio of 1:3 between Tenant A and Tenant B, Tenant A can use up to 25 Mbit/s of bandwidth. If we set the `max` parameter to 20 Mbit/s, the tenant will use no more than 20 Mbit/s of bandwidth.\\n\\nThe weight ratio will change if tenants are added or deleted. To ensure that each tenant obtains the minimum resources that it requires, we can specify the amount of reserved resources for each tenant by setting the `min` parameter. This not only guarantees the operation of basic functionality of all tenants but also describes QoS in a clearer way.\\n\\nProvide better resource isolation in OceanBase Database\\n\\nDatabase resources can be classified into rigid and elastic resources depending on their usage behaviors. Generally, elastic resources can be isolated. Rigid resources are necessary for programs to fulfill their duties and, once occupied, will not be released in a short period of time. Typical rigid resources include disk, memory, and the number of connections. After you make a static plan for such resources, the amount of resources allocated to each group is fixed. Elastic resources, such as IOPS, CPU time, and network bandwidth, have nothing to do with program functionality but are related to system performance. These resources can be preempted or quickly released. Users can schedule elastic resources for sharing in off-peak hours and isolation in peak hours. So, the sharing of elastic resources is what we need to focus on.\\n\\nOceanBase Database prioritizes the isolation of the following resources that are relatively import: memory, disk space, CPU time, IOPS, and bandwidth.\\n\\nCPU isolation\\n\\nOceanBase Database has supported CPU time isolation and will support CPU cache isolation later. CPU isolation works in real time only when CPU is in kernel mode. This is because a resource can be scheduled only if it can be divided into many smaller pieces. For example, network I/O resources are natively in form of packets, and so do disk I/O resources. The operating system divides CPU time into many slices, which are transparent for the user mode and cannot be directly scheduled. To schedule CPU time in user mode, you need to insert many checkpoints into the code to divide the CPU time of user threads into many segments, and execute the scheduling at the checkpoints. The accuracy of checkpoint insertion, however, is not guaranteed. How to insert checkpoints into functions of a static database?\\n\\nOceanBase Database adopts a kernel mode solution, where the CPU controller of cgroup is used. Currently, cgroup supports the `max` and `weight` parameters. Although the `min` parameter is not supported, it is not a problem because the total CPU time does not fluctuate. We can reserve the time slices for each group just by setting the `weight` parameter.\\n\\nCPU isolation applies not only to user workloads, but also system tasks. For example, leader election among multiple replicas is a high-priority task for OBServer nodes, and we do not want the election to be affected by the CPU resource contention with user SQL queries. Therefore, we divide resources for election and user SQL queries into two directories in the root of cgroup, and further divide the user SQL directory into subdirectories corresponding to tenants and users within tenants.\\n\\nIOPS isolation\\n\\nIf you use a solid-state disk (SSD), you can calculate the bandwidth based on this equation: Bandwidth = SSD size \xd7 IOPS. We can use normalized IOPS with an empirical formula. For example, we can take a 16 KB I/O as a normalized I/O, so that a 2 MB I/O is translated into several normalized I/Os based on the formula. Devices need to be distinguished during IOPS isolation. However, exposing the devices makes configuration more complicated. So, in most cases, multiple devices share one set of configurations.\\n\\nThese ideas are inspired by this paper about VM I/O isolation, titled \\"mClock: Handling Throughput Variability for Hypervisor IO Scheduling\\", by VMware Inc.\\n\\nWhen OceanBase Database was deployed on a public cloud, we found that the I/O throughput of the cloud disk fluctuated. However, OceanBase Database quickly adapted to such fluctuation and maintained the stability of the most important OLTP business. Also, OceanBase Database associates I/O isolation with block cache, which means OceanBase Database limits not only the I/O bandwidth of OLAP but also the cache used for OLAP. In this way, the block cache can be protected from being polluted by OLAP to eventually ensure the low latency of OLTP.\\n\\nNetwork bandwidth isolation\\n\\nOBServer nodes communicate with each other by using remote procedure calls (RPCs). RPCs are sent to OBServer nodes within the same Internet data center (IDC) for the distributed execution of SQL statements and two-phase commit, and to OBServer nodes in other IDCs for log replication and data backup to ensure high availability. Unlike intra-IDC communication, the inter-IDC communication between an OBServer node and different IDCs is performed with varying latency and bandwidth usage. Usually, the bandwidth is shared for inter-IDC communication. Therefore, the bandwidth allocation and limitation must be considered globally. The question is how to define the scope of \'global\'? If we have built multiple OceanBase clusters, do we need to consider them all? What if network partitioning is involved even we have only one OceanBase cluster? How can we get the global view?\\n\\nOceanBase Database supports region-level bandwidth control since V3.2. Next, instead of holistic resource scheduling among multiple OceanBase clusters, we want the DBA to make a static resource plan. That is, the DBA needs to configure the bandwidth available to clusters for the intra-IDC and inter-IDC communication. OceanBase Database then dynamically assigns the bandwidth to OBServer nodes within a cluster, and each OBServer node further assigns the bandwidth to different groups based on their priorities.\\n\\nFor most business, bandwidth allocation for the intra-IDC communication is more important. While bandwidth isolation is quite similar to IOPS isolation, algorithms often take the network interface card (NIC) rather than each communication destination in calculations as an I/O device, given the large number of communication destinations.\\n\\nBandwidth isolation can be completed in two steps: tag traffic and isolate the tagged traffic based on pre-defined requirements. The first step can be performed only at the application layer, and the second step can be performed either at the application layer or the kernel layer. Since Linux Traffic Control (TC) provides a variety of throttling and priority strategies, OceanBase Database tags traffic at the application layer and throttles the tagged traffic at the kernel layer. This solution reuses capabilities of the kernel that are supported by a widely accepted ecosystem. Users do not bother to learn new throttling mechanisms.\\n\\n**What is achieved by resource isolation in OceanBase Database?**\\n\\nAt present, OceanBase Database supports the isolation of memory, disk, CPU, and IOPS, and will support bandwidth isolation in the future. The following test takes CPU isolation as an example to show the performance of resource isolation in OceanBase Database.\\n\\nWhen talking about the method of defining resource groups, we mentioned that a dedicated user can be created for OLAP. In this test, we created two test users named AP@ORACLE and TP@ORACLE, and bound OLAP tasks to AP_GROUP and OLTP tasks to TP_GROUP, assuming that the test business involves heavy OLTP workloads during daytime and most OLAP workloads are handled at night. Therefore, we set two resource plans for daytime and night. The daytime plan schedules 80% of the resources for OLTP and 20% for OLAP, and the night plan schedules 50% of the resources for OLTP and 50% for OLAP.\\n\\nSwitch from the daytime plan to the night plan\\n\\nThe result shows that the OLAP QPS increases significantly while the OLTP QPS decreases after the plan switchover due to a larger portion of CPU resources allocated to OLAP in the night plan. In the figure below, you can see the turning points of OLAP and OLTP throughput curves caused by the plan switchover.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/df4eeb08-6997-4447-9ba2-a934dd3cad25/image/2022-09-29/673ae0a8-e777-4612-8326-0dd0a3d81e09.png)\\n\\nIt seems that the change in the OLTP throughput is not as noticeable in comparison to that of OLAP. This is actually a result as expected. The percentage of resources for OLAP is increased from 20% to 50%, an increase of 150%, and that for OLTP is reduced from 80% to 50%, a decrease of 37.5%. Given that the actual OLTP throughput drops from 19,000 to 14,300 QPS, a 24.7% decrease, the gap does not make much difference.\\n\\nThe performance of CPU isolation relies largely on the type of workload. If the network becomes a bottleneck, bandwidth isolation is also necessary. The test is not intended to bang the drum for CPU isolation as a cure-all, but it does show that simple CPU isolation works well for CPU-bound workloads, even without the CPU cache isolation. Keep in mind that isolation capabilities are getting better over time. CPU isolation alone takes effect on OLTP-simple OLAP isolation or OLTP-OLTP isolation. If we combine CPU isolation with IOPS isolation and network bandwidth isolation, the application scope will be even wider.\\n\\n# [](#k31aH)**Wrap-up**\\n\\nThis article introduces thoughts of the OceanBase team on resource isolation technology and its implementation solution. Highly efficient and effective resource isolation is required to ensure sharing of hardware resources among different tenants and among OLTP and OLAP services within the same tenant in an HTAP database. We believe that a resource isolation solution that integrates complementary physical and logical isolation mechanisms is more suitable for an HTAP database. OceanBase will keep optimizing the resource isolation technology to better meet the needs of users."},{"id":"flink-and-ob","metadata":{"permalink":"/blog/flink-and-ob","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/misc-flink-and-ob.md","source":"@site/blog/misc-flink-and-ob.md","title":"Flink CDC + OceanBase integration solution for full and incremental synchronization","description":"Introduction","date":"2024-06-03T13:23:07.000Z","tags":[],"readingTime":5.79,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"flink-and-ob","title":"Flink CDC + OceanBase integration solution for full and incremental synchronization"},"unlisted":false,"prevItem":{"title":"Why is resource isolation important for HTAP?","permalink":"/blog/resource-isolation"},"nextItem":{"title":"Hello OceanBase! The first lesson for becoming an OceanBase contributor","permalink":"/blog/hello-oceanbase"}},"content":"## Introduction\\n\\nChange Data Capture (CDC) is a widely applied technology that captures database changes. In this post, we will introduce you to the Flink CDC + OceanBase Database data integration solution. This solution combines CDC with extraordinary pipeline capabilities and diversified ecosystem tools of Flink, to synchronize processed CDC data to the downstream, formulating a solution for integrated full and incremental synchronization based on OceanBase Database Community Edition.\\n\\nThe solution brings two benefits. First, it synchronizes data by using one component and one link. Second, Flink SQL supports aggregation and extract-transform-load (ETL) of database and table shards, making it much easier for users to analyze, process, and synchronize CDC data by executing a Flink SQL job.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Keypoints\\n\\nThis post is based on the content shared by Wang He, an open source tool expert with OceanBase.\\n\\nIt contains the following five parts:\\n\\n1. Introduction to the CDC technology\\n\\n2. Introduction to OceanBase CDC components\\n\\n3. Introduction to Flink CDC\\n\\n4. Use Flink CDC OceanBase Connector\\n\\n5. Conclusion\\n\\n## I. CDC technology\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/eaa84253-a48e-438c-a458-29cc39c6d194/image/2022-05-07/7f3f97dc-eb5c-43f5-a8c8-44f5064bee08.png)\\n\\nThe CDC technology monitors and captures changes in a database, such as the INSERT, UPDATE, and DELETE operations on the data or data tables, and then writes the changes to message-oriented middleware, so that other services can subscribe to and consume the changes.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/7b3f7a72-e230-4ada-8f67-d1feb68d1168/image/2022-05-07/ac855f6e-3a00-44f3-b7e6-39517751fec0.png)\\n\\nAlibaba Canal is a popular open source CDC tool, which is mainly used in Alibaba Cloud open-source components for incremental MySQL data subscription and consumption. The latest version of Alibaba Canal supports OceanBase Database Community Edition data sources, incremental DDL and DML operations, and filtering of databases, tables, and columns. You can use it with ZooKeeper for the deployment of high-availability clusters. The client adapter of Alibaba Canal supports multiple types of containers as the destination. You can use it with Alibaba Otter to achieve active geo-redundancy.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/17c2e5c5-9b31-4290-8cbd-983815f1e469/image/2022-05-07/300d3459-5fa8-416c-a5d5-84e03fdcc488.png)\\n\\nAnother popular open source CDC framework is Debezium.\\n\\nIt supports the synchronization of DDL and DML operation logs, uses the primary key or unique key as the key of the message body, and also supports the snapshot mode and full synchronization.\\n\\nDebezium also supports a variety of data sources. You can integrate Debezium Server into a program as an embedded engine to directly write data to a message system without using Kafka.\\n\\n# II. OceanBase CDC components\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/b38cbaf6-fc19-4eef-b172-27a5016cc451/image/2022-05-07/15fd5ef4-7679-403b-aad1-a39d8700b0bc.png)\\n\\nOceanBase Database Community Edition provides four CDC components:\\n\\nobcdc (formerly liboblog): pulls incremental logs in sequence.\\n\\noblogmsg: parses the format of incremental logs.\\n\\noblogproxy: pulls the incremental logs.\\n\\noblogclient: connects to oblogproxy to obtain the incremental logs.\\n\\nOceanBase Migration Service (OMS) Community Edition is provided. It is an all-in-one data migration tool for incremental data migration, full data migration, and full data verification.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/0fa227bf-343a-4b54-9eb8-ec0adcb7b018/image/2022-05-07/8eceea9f-1f1e-445c-9a9f-24c7cd495cd6.png)\\n\\nThe preceding figure shows the CDC logic of OceanBase Database Community Edition. Data is pulled by oblogproxy and OMS Community Edition. Canal and Flink CDC are integrated with oblogclient to obtain incremental logs from oblogproxy.\\n\\n# III. Flink CDC\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/fa9a24c3-5684-40c4-b584-35992433bff5/image/2022-05-07/ba8f10ce-eacf-48ef-b6b6-402368806f09.png)\\n\\nFlink CDC supports multiple data sources, such as MySQL, PostgreSQL, and Oracle. Flink CDC reads the full and incremental data from a variety of databases, and then automatically transfers data to the Flink SQL engine for processing.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/fc9856de-09ed-4eaa-b110-dff6d75f5b32/image/2022-05-07/4d648854-a0e5-4404-83ea-18d1ce2b94cd.png)\\n\\nFlink is a hybrid engine that supports both batch and streaming processing. Flink CDC converts streaming data into a dynamic table. In the preceding figure, the lower left part shows the mapping between streaming data and a dynamic table. The lower right part shows the results of multiple executions of continuous queries.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/091b66ed-1259-46f9-a385-3353c6c77a2f/image/2022-05-07/3217492e-54fc-4dac-831f-814dfd2b855c.png)\\n\\nThe preceding figure shows the working principle of Flink CDC. It implements the SourceFunction API based on Debezium and supports MySQL, Oracle, MongoDB, PostgreSQL, and SQLServer.\\n\\nThe latest version of Flink CDC supports data reads from a MySQL data source by using the Source API, which provides enhanced concurrent reading compared to the SourceFunction API.\\n\\nThe OceanBaseRichSourceFunction API is implemented for full and incremental data reads respectively based on JDBC and oblogclient.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/ef1db322-542c-460c-a4cc-7052711bdbb8/image/2022-05-07/7144e0a0-de64-4123-966a-4de6cf427d0b.png)\\n\\n=============\\n\\n# IV. Use Flink CDC OceanBase Connector\\n\\nConfigure the `docker-compose.yml` file and start the container. Go to the directory where the `docker-compose.yml` file is stored, and run the `docker-compose up-d` command to start the required components.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/9b5d74d0-01c9-42b7-88e8-329996981c57/image/2022-05-07/b87e2f73-36ad-47c2-b991-1bdff8fb5aba.png)\\n\\nRun the `docker-compose exec observer obclient-h127.0.0.1-P2881-uroot-ppsw` command to log on by using newly created username and password. Download the required dependency packages and execute Flink DDL statements on the CLI of Flink SQL to create a table.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/67ccf926-fd34-4175-b7f7-6219ce53ec66/image/2022-05-07/022ef971-e3b8-4861-8590-3734401c1020.png)\\n\\nSet the checkpointing interval to 3 seconds and the local time zone to Asia/Shanghai. Then, create an order table, a commodity table, and the associated order data table. Perform data reads and writes.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/77f314e9-0062-455a-a0aa-e34bc0f3de1a/image/2022-05-07/1a5ea095-35cf-4106-b629-7064a8a75792.png)\\n\\nView the data in Kibana by visiting the following address:\\n\\n[http://localhost:5601/app/kibana#/management/kibana/index_pattern](http://localhost:5601/app/kibana#/management/kibana/index_pattern)\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/f79cc96e-182c-4180-808b-a7b8b091f4a7/image/2022-05-07/50904e7a-a129-46d1-b841-f9b19c64edee.png)\\n\\nCreate an index pattern named `enriched_orders`, and then view the written data by visiting [http://localhost:5601/app/kibana#/discover](http://localhost:5601/app/kibana#/discover%E7%9C%8B%E5%88%B0%E5%86%99%E5%85%A5%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BA%86%E3%80%82).\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/69bd86f3-8601-4561-a9e0-30831203e859/image/2022-05-07/01c329c8-b97b-4dfc-8e6c-117d00e24f69.png)\\n\\nModify the data of the monitored table and view the incremental data changes. Perform the following modification operations in OceanBase Database in sequence, and refresh Kibana once after each step. We can see that the order data displayed in Kibana is updated in real time.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/62c8b1a5-50d1-4903-a221-ce21c8f1d6c4/image/2022-05-07/991c53d2-425e-4d01-a20d-3851bff54609.png)\\n\\nClean up the environment. Go to the directory where the `docker-compose.yml` file is located, and run the `docker-compose down` command to stop all containers. Go to the Flink deployment directory and run the `./bin/stop-cluster.sh` command to stop the Flink cluster.\\n\\n# V. Conclusion\\n\\nFlink CDC supports full and incremental data migration between many types of data sources and works with Flink SQL to perform ETL operations on streaming data. As of the release of Flink CDC 2.2, the project has 44 contributors, 4 maintainers, and more than 4,000 community members.\\n\\nOceanBase Connector can be integrated with Flink CDC 2.2 or later to read full data and incremental DML operations from multiple databases and tables in AT_LEAST_ONCE mode. Flink CDC OceanBase Connector will gradually support concurrent reads, incremental DDL operations, and the EXACTLY_ONCE mode in later versions.\\n\\nNow, let\'s briefly compare the existing CDC solutions. OMS Community Edition is a proven online data migration tool with a GUI-based console. It provides full data migration, incremental data migration, data verification, and O&M services. DataX + Canal/Otter is a fully open source solution. Canal supports many types of destinations and incremental DDL operations, and Otter supports active-active disaster recovery.\\n\\n# Afterword\\n\\nFlink CDC is a fully open source solution and is supported by an active community. It supports full and incremental data synchronization between many types of data sources and destinations. It is worth mentioning that Flink CDC is easy to use, and supports aggregation and ETL of database and table shards. Compared with some existing CDC solutions that involve complex data cleaning, analysis, and aggregation operations, Flink SQL allows users to easily process data for various business needs by using methods such as stream-stream join and dimension table join.\\n\\n## Contact us\\n\\n**Feel free to contact us at any time.**\\n\\n[Visit the official forum of OceanBase Database Community Edition](https://open.oceanbase.com/answer)\\n\\n[Report an issue of OceanBase Database Community Edition](https://github.com/oceanbase/oceanbase/issues)\\n\\n**DingTalk Group ID: 33254054**\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/8223c1be-2a25-4658-9d9f-5fd4594e9900/image/2022-05-07/77e7a1ce-01b1-4b45-9aaa-ff184f43f822.png)"},{"id":"hello-oceanbase","metadata":{"permalink":"/blog/hello-oceanbase","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/misc-hello-oceanbase.md","source":"@site/blog/misc-hello-oceanbase.md","title":"Hello OceanBase! The first lesson for becoming an OceanBase contributor","description":"About the author: Xia Ke, a contributor of OceanBase Community, has engaged in the design and development of financial core systems for years. He is now working on the investigation of China-made databases in a subsidiary of a stock exchange, and has recently obtained OceanBase Certified Associate (OBCA) and PingCAP Certified TiDB Associate (PCTA) certificates.","date":"2024-06-03T13:23:07.000Z","tags":[],"readingTime":7.525,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"hello-oceanbase","title":"Hello OceanBase! The first lesson for becoming an OceanBase contributor"},"unlisted":false,"prevItem":{"title":"Flink CDC + OceanBase integration solution for full and incremental synchronization","permalink":"/blog/flink-and-ob"},"nextItem":{"title":"How OB Cloud Achieves Cost Reduction and Efficiency Improvement in Replacing MySQL Scenarios \uff1f","permalink":"/blog/obcloud"}},"content":"> About the author: Xia Ke, a contributor of OceanBase Community, has engaged in the design and development of financial core systems for years. He is now working on the investigation of China-made databases in a subsidiary of a stock exchange, and has recently obtained OceanBase Certified Associate (OBCA) and PingCAP Certified TiDB Associate (PCTA) certificates.\\n\\n# Introduction\\n\\nThe other day I read this post [Decoding OceanBase (11): Expressions and Functions](https://open.oceanbase.com/blog/8600156?currentPage=undefined) by Zhuweng, a Peking University alumnus, director of OceanBase kernel R&D. At the end, he mentioned that creating a built-in function is the first test for new recruits joining the SQL team of OceanBase. Though having no intention of looking for a new job, I was so intrigued by this **first test**. I am not a database administrator, so I have not spent much time working with databases until lately when I started learning about top-notch home-grown database products for job reasons. As far as my job description is concerned, I have \\"walked out of my circle\\" to do the test. After reading posts from popular IT communities and participating in open online courses, I was deeply impressed by the vigorous momentum of the database ecosystem in China. I want to say thank you to database developers and engineers who have shared their experience, which enables laymen like me to quickly get on track. Benefiting from their good deeds, I would like to take this opportunity to make my contributions to the OceanBase community, hoping that you might find it useful.\\n\\n\x3c!-- truncate --\x3e\\n\\n# Overview\\n\\nSeeing `Hello OceanBase` in the title, you may think of \\"Hello World\\". Yes, that is exactly the vibe I wanted to strike here. This post is a \\"Hello World\\" demo that shows you how to create or modify a built-in function in OceanBase Database, or in other words, how to do secondary development based on OceanBase Database Community Edition. In addition to being motivated by `Zhuweng`, I want to take this test also because of the requirement for extending database capabilities by using external functions. Oracle allows users to call external C or JAVA functions, just like calling built-in functions. In a sense, this feature makes a database more capable. Users can call external C or JAVA functions to, for example, implement complex mathematical algorithms, which may otherwise cause troubles by using SQL statements or Oracle built-in functions. Of course, you can always implement those algorithms at the business layer, but maybe we can talk about that in another post. Based on my research on a bunch of home-grown databases, I have come up with similar procedures for using external functions. You can take a look at them in my posts [Implement Oracle external functions in Dameng DM8 Database](https://blog.csdn.net/xk_xx/article/details/123091480?spm=1001.2014.3001.5501) and [Implement PostgreSQL UDFs by using the contrib module](https://blog.csdn.net/xk_xx/article/details/123011397?spm=1001.2014.3001.5501). Calling external functions in DM8 is basically the same as in Oracle, except for some slight implementation differences. Some databases, such as openGauss, MogDB, TDSQL PostgreSQL, and KingbaseES, come with an PostgreSQL kernel, and they inherently support the extension mechanisms of PostgreSQL. To the best of my knowledge, however, OceanBase Database does not support external functions. So, I wondered if I could not find a way out, a way in might do the trick. Let\'s start with \\"Hello OceanBase\\".\\n\\n# Preparations\\n\\n## An OceanBase cluster\\n\\nYou can find tons of posts in the community about how to deploy OceanBase Database in all kinds of supported modes. Here are some of my posts in this regard: [Use Docker to deploy OceanBase Database](https://blog.csdn.net/xk_xx/article/details/122757336), [Manually deploy OceanBase Database in standalone mode](https://blog.csdn.net/xk_xx/article/details/122763419), and [Use OBD to locally deploy OceanBase Database in standalone mode](https://blog.csdn.net/xk_xx/article/details/123166584). To implement the demo, I recommend that you pick an easy one and use OceanBase Deployer (OBD) to locally deploy a standalone OceanBase database in a development environment.\\n\\n## OceanBase source code\\n\\nYou can get the latest source code by running the following command: `git clone https://github.com/oceanbase/oceanbase`\\n\\n# Code structure\\n\\nYou can take a look at the `Decoding OceanBase` serial posts of the community for details.\\n\\nHere, let me briefly describe the code related to the `sql/resolver/expr` directory.\\n\\n## Register a built-in function\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/ec1d519b-9b37-4d6e-9b86-68074ff85e3b/image/2022-03-04/78c15e68-ea1b-40f0-b4e7-07b71454a82b.png)\\n\\n```C++\\n    #define REG_OP(OpClass)                                                \\\\do {                                                                 \\\\\\n        OpClass op(alloc);                                                 \\\\if (OB_UNLIKELY(i >= EXPR_OP_NUM)) {                               \\\\\\n          LOG_ERROR(\\"out of the max expr\\");                                \\\\\\n        } else {                                                           \\\\\\n          NAME_TYPES[i].name_ = op.get_name();                             \\\\\\n          NAME_TYPES[i].type_ = op.get_type();                             \\\\\\n          OP_ALLOC[op.get_type()] = ObExprOperatorFactory::alloc<OpClass>; \\\\\\n          i++;                                                             \\\\\\n        }                                                                  \\\\\\n      } while (0)\\n```\\n\\n---\\n\\n## Diagram of the expr class\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/69375afc-a8a4-4cdf-b339-5cd3ebfe84be/image/2022-03-04/726f726f-7ed3-4060-b909-6595697154b3.png)\\n\\nBuilt-in functions mainly implement the ObExprOperator interface class, which contains many functions.\\n\\nThe `calc_result_type0` and `calc_result0` functions specify the memory allocation and type definition for function registration. The `cg_expr` function registers the function pointer to the `eval_func_` function. The built-in function `rt_expr.eval_func_ = ObExprHello::eval_hello;` is called by using the function pointer. `eval_hello` is the function that actually do the job.\\n\\n## Develop Hello OceanBase\\n\\nIn this project, you need to modify the files shown in the following figure.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/b33bf6b7-47c4-4d2e-b3f8-31e1a2349567/image/2022-03-04/38ff080c-902e-4976-9b12-6113ba751b0d.png)\\n\\n### 1. Create the ObExprHello class\\n\\nMany implementation examples are provided in the `sql/resolver/expr` directory. You can select reference objects as needed.\\n\\n```C++\\n    #ifndef _OB_EXPR_HELLO_H_\\n    #define  _OB_EXPR_HELLO_H_\\n\\n    #include  \\"sql/engine/expr/ob_expr_operator.h\\"\\n\\n    namespace  oceanbase {\\n    namespace  sql {\\n    class  ObExprHello : public  ObStringExprOperator {\\n    public:\\n      explicit  ObExprHello(common::ObIAllocator&  alloc);\\n      virtual  ~ObExprHello();\\n      virtual  int  calc_result_type0(ObExprResType&  type, common::ObExprTypeCtx&  type_ctx) const;\\n      virtual  int  calc_result0(common::ObObj&  result, common::ObExprCtx&  expr_ctx) const;\\n\\n      static  int  eval_hello(const  ObExpr&  expr, ObEvalCtx&  ctx, ObDatum&  expr_datum);\\n      virtual  int  cg_expr(ObExprCGCtx&  op_cg_ctx, const  ObRawExpr&  raw_expr, ObExpr&  rt_expr) const  override;\\n\\n    private:\\n      DISALLOW_COPY_AND_ASSIGN(ObExprHello);\\n    };\\n\\n    } /* namespace sql */\\n    } /* namespace oceanbase */\\n\\n    #endif\\n```\\n\\nThe content of the new file `ob_expr_hello.cpp` is as follows:\\n\\n```C++\\n    #define  USING_LOG_PREFIX SQL_ENG\\n    #include  \\"sql/engine/expr/ob_expr_hello.h\\"\\n    static  const  char* SAY_HELLO = \\"Hello OceanBase!\\";\\n\\n    namespace  oceanbase {\\n    using  namespace  common;\\n    namespace  sql {\\n\\n    ObExprHello::ObExprHello(ObIAllocator& alloc) : ObStringExprOperator(alloc, T_FUN_SYS_HELLO, N_HELLO, 0)\\n    {}\\n\\n    ObExprHello::~ObExprHello()\\n    {}\\n\\n    int  ObExprHello::calc_result_type0(ObExprResType&  type, ObExprTypeCtx&  type_ctx) const\\n    {\\n      UNUSED(type_ctx);\\n      type.set_varchar();\\n      type.set_length(static_cast<common::ObLength>(strlen(SAY_HELLO)));\\n      type.set_default_collation_type();\\n      type.set_collation_level(CS_LEVEL_SYSCONST);\\n      return  OB_SUCCESS;\\n    }\\n\\n    int  ObExprHello::calc_result0(ObObj&  result, ObExprCtx&  expr_ctx) const\\n    {\\n      UNUSED(expr_ctx);\\n\\n      result.set_varchar(common::ObString(SAY_HELLO));\\n      result.set_collation(result_type_);\\n      return  OB_SUCCESS;\\n    }\\n\\n    int  ObExprHello::eval_hello(const  ObExpr&  expr, ObEvalCtx&  ctx, ObDatum&  expr_datum)\\n    {\\n      UNUSED(expr);\\n      UNUSED(ctx);\\n      expr_datum.set_string(common::ObString(SAY_HELLO));\\n      return  OB_SUCCESS;\\n    }\\n\\n    int  ObExprHello::cg_expr(ObExprCGCtx&  op_cg_ctx, const  ObRawExpr&  raw_expr, ObExpr&  rt_expr) const\\n    {\\n      UNUSED(raw_expr);\\n      UNUSED(op_cg_ctx);\\n      rt_expr.eval_func_ = ObExprHello::eval_hello;\\n      return  OB_SUCCESS;\\n    }\\n    }  // namespace sql\\n    }  // namespace oceanbase\\n```\\n\\n### 2. Modify or add the function name definition\\n\\n- ob_name_def.h\\n\\nThe function name is registered here and can be used for syntax parsing.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/889fe5d5-bcc4-49ee-90c2-65c8d5a8e0d7/image/2022-03-04/0736c95b-c1be-4e3d-984b-2422fd3dee7f.png)![](https://gw.alipayobjects.com/zos/oceanbase/d03c52a3-a866-4f3d-bac7-ac5201428edc/image/2022-03-04/606b6b77-f22a-4b38-9515-3c9cf03a3ffb.png)\\n\\n### 3. Modify the factory class\\n\\nob_expr_operator_factory.cpp\\n\\nThe function pointer is registered at this step, and will be used for calling the specific built-in function at runtime.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/f07b83a3-4d9e-4169-bd6c-620e9a1b02b8/image/2022-03-04/d67d1e07-8e82-4169-b400-19f30d813d19.png)\\n\\n- Register a built-in function\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/e544598d-cb7d-4b8d-ae8a-85fa04f802c8/image/2022-03-04/bb53c1e7-27f7-4370-a7cc-619241c945b2.png)\\n\\n### 4. Add IDs\\n\\n- ob_item_type.h\\n\\nYou can take an ID as a key that points to the function pointer.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/597a87f1-1998-41f0-8c5d-29a818026d92/image/2022-03-04/8223570b-4af3-438d-a533-e20eec745e17.png)\\n\\n### 5. Modify project files\\n\\n- CMakeLists.txt\\n\\nAdd the new ObExprHello function to the project for compilation.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/7128397f-fcd9-4320-bcb7-982fe9b80a12/image/2022-03-04/458ceecc-d9ba-4fdd-a7ad-c00b89f39935.png)\\n\\n### 6\\\\. ob_expr_hello.cpp\\n\\n### ![](https://gw.alipayobjects.com/zos/oceanbase/98290631-db52-4f32-bf1f-4b185907c9ac/image/2022-03-04/2ac97743-4494-4d81-ba5f-25dec9aabe2c.png)\\n\\n###\\n\\n### 7.ob_expr_eval_functions.cpp\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/b3d7d96d-7c46-445f-883d-473ecb068cd4/image/2022-03-04/d587de86-0571-42bc-8f1e-3081b47b6899.png)\\n\\n## Compile the function\\n\\nI\'ll skip this part. Please read other posts about how to compile the OceanBase source code for details.\\n\\nAnd by the way, I have found some compilation errors in the latest code and created a pull request on GitHub, which has been accepted but not yet been merged.\\n\\n---\\n\\n## Verify the function\\n\\n### 1. Replace the observer process\\n\\nCreate a soft connection to points the observer process in the `/root/observer/bin` directory to the observer process in the `/root/.obd/repository/oceanbase-ce/3.1.2/7fafba0fac1e90cbd1b5b7ae5fa129b64dc63aed/bin` directory.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/83dba035-7b05-49c2-828e-0da3bd8c0f8c/image/2022-03-04/d8015f5f-b55c-4bee-bfe1-e16bad69b2ec.png)\\n\\n### 2. Start the observer process\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/9d017a42-5261-4e8a-84e8-e578707c3c41/image/2022-03-04/8de93958-9c77-4932-ac53-55f2389d153f.png)\\n\\nYou may notice that the version is 3.1.3, which is not released yet. We got that result because the latest code was used.\\n\\n## Test the function\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/57787d55-ce12-4960-8e87-f39478f97f68/image/2022-03-04/60705b08-75e1-477b-bf21-5d07bd7902ef.png)\\n\\n### Suggestions\\n\\nIn the past few months, I have been researching home-grown databases, such as OceanBase Database, TiDB, openGauss, MogDB, and Dameng. They are capable of online transaction processing (OLTP), online analytical processing (OLAP), or hybrid transaction/analytical processing (HTAP). I am not bold enough to compare them as a layman, but as a user, I would like to bring up a few points based on my experience with OceanBase Database.\\n\\nIt took me some time to build the environment, which is acceptable because, after all, it is a distributed system. However, it would be great if users are provided with a tool to quickly build a demo cluster, like the playground of TiDB.\\n\\nThe system is resource-consuming. Users with small-specification devices may suffer deployment failures due to resource insufficiency. They will be grateful if small-specification deployment is supported.\\n\\nMaybe OceanBase Database can consider supporting user-defined extension interfaces? Some may think that is not a necessary feature, but it is quite useful in some enterprise-level applications, and wins OceanBase a point or two when comparing it to Oracle.\\n\\nOceanBase Database Enterprise Edition can support more driver APIs for Oracle tenants. For more information, see \\"Use JDBC to connect to OceanBase Database through JayDeBeApi in Python\\".\\n\\n---\\n\\n## Afterword\\n\\nMost of posts in the community are intended for database administrators, focusing on deployment, migration, application, performance, and O&M. This one may not attract a large audience. However, I hope it can encourage more better content on the secondary development of open source databases."},{"id":"obcloud","metadata":{"permalink":"/blog/obcloud","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/misc-obcloud.md","source":"@site/blog/misc-obcloud.md","title":"How OB Cloud Achieves Cost Reduction and Efficiency Improvement in Replacing MySQL Scenarios \uff1f","description":"Cost reduction and efficiency improvement seem an eternal topic for any enterprises that are seeking sustainable development and profit. However, hasty rigid cost reduction measures tend to cause uncontrollable impact, leading to inefficient operation and slow business growth. By promoting cost reduction, we should aim at improving productivity while cutting operation costs.","date":"2024-06-03T13:23:07.000Z","tags":[],"readingTime":16.635,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"obcloud","title":"How OB Cloud Achieves Cost Reduction and Efficiency Improvement in Replacing MySQL Scenarios \uff1f"},"unlisted":false,"prevItem":{"title":"Hello OceanBase! The first lesson for becoming an OceanBase contributor","permalink":"/blog/hello-oceanbase"},"nextItem":{"title":"Adaptive Techniques in the OceanBase SQL Execution Engine","permalink":"/blog/adaptive-sql-execution-engine"}},"content":"Cost reduction and efficiency improvement seem an eternal topic for any enterprises that are seeking sustainable development and profit. However, hasty rigid cost reduction measures tend to cause uncontrollable impact, leading to inefficient operation and slow business growth. By promoting cost reduction, we should aim at improving productivity while cutting operation costs.\\n\\nIn the years when the conventional centralized databases dominated the industry, reducing costs without hurting database efficiency was really a headache for IT departments of many enterprises. Based on the content shared by OceanBase solution architect Gao Jiwei in OceanBase Cloud Open Class, this post compares OceanBase Cloud with MySQL, analyzes features that help enterprises increase efficiency with reduced costs, and discusses how to make cost reduction plans for small and medium-sized enterprises (SMEs) and large corporations by using two examples.\\n\\n![1691647401](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647401464.png)\\n\\n\x3c!-- truncate --\x3e\\n\\nIn recent years, enterprises have been increasing their efforts to reduce costs and increase efficiency, but mostly by downgrading resource specifications, which could pose unbearable risk, especially for databases, the cornerstone of a software architecture.\\n\\nTherefore, it is crucial to find a way that reduces costs while ensuring a very high throughput without compromising performance stability. This is what OceanBase is doing right now. We are designing cost reduction solutions leveraging our technical know-how. Let\'s first look at the following figure, which shows a general comparison between MySQL and OceanBase Cloud.\\n\\n![1691647424](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647424030.png)\\n\\n- **MySQL downside No. 1: numerous complex instances**. In a MySQL cluster, the resource usage of some instances is higher than others. This resource usage imbalance makes O&M more difficult. OceanBase Cloud features a natively distributed architecture, which allows you to consolidate MySQL instances into tenants of one cluster. This simplifies O&M and improves the overall utilization of cluster resources.\\n- **MySQL downside No. 2: uncompressed data**. MySQL adopts a B+ tree-based storage structure, which has been in use for years. An inherent flaw of this structure is that unused space exists on each page. OceanBase Cloud comes with a fully self-developed log-structured merge-tree (LSM-tree)-based storage structure. By using unique compression algorithms, OceanBase Cloud compresses a data set to 1/4 to 1/5 or even less of its original size in MySQL. Such a high compression ratio greatly reduces the storage costs.\\n- **MySQL downside No. 3: poor scalability**. MySQL Database runs in primary/secondary mode. Server replacement is inevitable if you want to scale its capacity. To replace a server, a primary/secondary switchover is performed, which causes considerable application interruptions. For that reason, it is often necessary to configure a MySQL database against the maximum possible traffic to avoid a switchover. For example, if eight CPU cores are enough most of the time, but 16 CPU cores are required to handle occasional peak traffic, a MySQL database must be configured with 16 CPU cores. OceanBase Cloud is capable of fast and flexible scaling. You can scale specifications up and down as needed, saving costs significantly.\\n- **MySQL downside No. 4: weak analysis capabilities**. A MySQL database handles only online transaction processing (OLTP) tasks. To cope with the business involving analytical reports, users must migrate data to a separately built analytical instance, leading to doubled costs. OceanBase Cloud provides hybrid transaction/analytical processing (HTAP) capabilities, which allow users to complete most less complicated tasks all in the same database without creating an analytical instance.\\n\\nTo reduce the overall cost, OceanBase provides targeted solutions to address inherent drawbacks of the MySQL architecture, such as low resource utilization and high resource redundancy. In fact, OceanBase Cloud reduces the total cost of ownership (TCO) by 30%. For more information, see [How can OceanBase Cloud help users achieve sustainable cost reduction and efficiency improvement? ](https://mp.weixin.qq.com/s?__biz=MzU0ODg0OTIyNw==&mid=2247500776&idx=1&sn=eb7f931f2d1c59fdac8ca07d55f15c5a&chksm=fbba50cccccdd9daf6b0bfb49d68196450e64bbabe5fbb0d9a6e43c8724ba01fd73453a73d83&scene=21#wechat_redirect)\\n\\n![1691647534](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647534704.png)\\n\\nBefore we delve into the features, let me walk you through the overall system architecture of OceanBase Cloud, so that you can get familiar with the following terms: OBProxy, OBServer node, partition, zone, and Paxos group.\\n\\n![1691647553](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647552979.png)\\n\\n- **OBProxy:** Applications connect to OceanBase Cloud through a component called OBProxy. Unlike other middleware, OBProxy is a lightweight node. It only forwards SQL queries without evaluating them. In OceanBase Cloud, OBProxy is not charged.\\n\\n- **Zone:** A zone of OceanBase Cloud corresponds to an IDC. By default, an OceanBase cluster is deployed across three zones. Each zone contains at least one OBServer node, and hosts many partitions. Therefore, OceanBase Cloud provides IDC-level high availability by default.\\n- **OBServer node:** An OBServer node is where OceanBase Cloud execute SQL queries or stores data. OceanBase Cloud supports powerful horizontal scaling, which allows users to add more OBServer nodes to each zone.\\n- **Partition:** A partition is the elementary unit of load balancing in OceanBase Cloud. If a table is divided into several parts, each part is a partition. The whole of a non-partitioned table can be considered as a partition.\\n- **Paxos group:** In the preceding figure, for example, the three replicas of partition P7 distributed in three zones form a Paxos group. Unlike the primary/secondary mode of MySQL, OceanBase Cloud synchronizes data among replicas based on the Paxos consensus algorithm, which supports automatic elections without external intervention. In other words, the three replicas of P7 can automatically elect the leader. As shown in the figure, the leader is in zone1 at the moment.\\n\\n  In a MySQL database, the data to be synchronized is converted into logical binary logs (binlogs), which are then synchronized from the primary node to the secondary nodes and then converted back into physical logs.\\n\\nNot involving such complex conversion, the Paxos consensus algorithm works with less latency and higher reliability. At any given moment, Paxos requires the agreement of two of the three replicas to elect the leader and commit logs. This way, OceanBase Cloud can maintain data integrity even in the case of an IDC-level failure. In fact, OceanBase Cloud can recover its service from failures of any level within 30 seconds. In the latest version, the recovery time is reduced to 8 seconds, showing great availability improvement compared with MySQL.\\n\\n## **I. Consolidate multiple instances into tenants of one cluster to optimize resource utilization and make O&M easier**\\n\\nThe left part of the figure shows a common deployment mode, where one database instance is created for each application. However, this deployment mode often leads to resource utilization imbalance among multiple database instances. For example:\\n\\n- Instance 1 hosts an internal HR system with very low daily traffic, and the resource utilization ranges from 5% to 10%.\\n- Instance 2 hosts an e-commerce system, which handles flash sales from time to time, leading to fluctuating transaction volumes. As a result, the resource utilization also severely fluctuates from 3% to 80%\\n- Instance 3 hosts a busy system, whose resource utilization remains high all the time.\\n\\nWhen a DBA or O&M engineer maintains those systems, they have to check multiple MySQL instances in the console, which means high O&M complexity and risks.\\n\\n![1691647655](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647654901.png)\\n\\nIn an OceanBase cluster, you can deploy a resource pool of many OBServer nodes and resources. Then, you can allocate resources exclusive to each tenant, and host each original RDS or MySQL instance in a tenant of the OceanBase cluster. This way, you need to maintain only one OceanBase cluster, significantly reducing the O&M complexity.\\n\\nThe benefits are that you can dynamically adjust tenant specifications anytime without affecting your business, and flexibly schedule resources to optimize the overall resource utilization, leading to a significant cost cut.\\n\\n## **II. Reduce storage costs by using an advanced LSM-tree-based compression engine**\\n\\nOceanBase Cloud uses an advanced LSM-tree-based storage engine developed in house. Unlike B+ tree, the OceanBase storage engine first stores written data in memory, and when the amount of data in memory reaches a specified threshold, dumps the data to disk. The storage engine compresses the data dumps and merges them with baseline SSTables (ROS) on a daily basis during off-peak hours, starting from 2:00 a.m. by default. This process is named as major compaction. The LSM-tree-based storage stores baseline data in a compact manner. Therefore, compared with B+ tree-based MySQL, OceanBase Cloud inherently provides a higher compression ratio.\\n\\n![1691647698](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647698306.png)\\n\\nIn addition, on top of the effect of the LSM-tree architecture, OceanBase Cloud compresses a data set twice.\\n\\nOceanBase Cloud uses hybrid row-column storage architecture based on microblocks. Each microblock corresponds to a layer of a MySQL page. This way, row-based storage is converted to columnar storage. One of the benefits of columnar storage is that, OceanBase Cloud can compress the columnar storage for the first time by encoding.\\n\\n![1691647731](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647731261.png)\\n\\nWe often deal with highly repeated columns in development, such as the `RATE_ID` column that contains four values that occur repeatedly in the preceding figure. In this case, we can map the values to a dictionary. For example, the value `1901321` in the column maps to `0` in the dictionary. As long as the dictionary is maintained, values of each column can be simply stored as, for example, `0`, `1`, and `2`, thus greatly compressing the storage. In practice, OceanBase will adaptively design a encoding method suitable for each application.\\n\\nAfter the first compression, a data set of 100 GB may be compressed to 30 GB. Then, OceanBase Cloud will compress the 30 GB of data further by using a general compression algorithm, such as LZ4.\\n\\nAfter the second compression, the data set may be compressed to 15 GB. In other words, 100 GB of data in MySQL may occupy only 15 GB of disk space in OceanBase Cloud. In practice, we have compressed data to less than 15% its original size for many of our customers, significantly reducing storage costs.\\n\\nSome may worry that such a high compression ratio will affect the real-time read/write performance. Well, relax. The LSM-tree-based storage engine compresses data during the daily major compaction, which often takes place after 2:00 a.m. Besides, you can specify when to start the major compaction based on your actual off-peak hours. This way, your business will not suffer from performance loss during the day, and the high compression ratio allows you to cut the storage costs.\\n\\n## **III. Handle peak traffic with rapid flexible scaling**\\n\\nWith the multi-level auto scaling capability of OceanBase Cloud, you can adjust resources of an OceanBase cluster at any time as your business grows. This way, you can flexibly control your resource bills and O&M tasks. OceanBase Cloud supports triple-level scalability. Specifically, you can scale your database by adjusting tenant specifications, OBServer node specifications, and the number of OBServer nodes.\\n\\n**\u25fc Level 1 scaling: adjustment of tenant specifications**\\n\\nWith a distributed architecture, OceanBase Cloud contains multiple OBServer nodes in a resource pool, which is divided into isolated resource groups. Each resource group is called a tenant. To increase the capacity of your database, you can first scale up tenant specifications. The adjustment of tenant specifications is completed within the OceanBase Database kernel, and does not involve changes of physical resources. In addition, the adjustment takes effect within seconds and has no impact on applications.\\n\\n![1691647784](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647784065.png)\\n\\nThe O&M team can adjust the number of CPU cores and memory size of a tenant at any time to smoothly improve the maximum transactions that can be processed per second (TPS) by the tenant. For example, they can adjust the resources during normal business hours.\\n\\n![1691647808](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647808099.png)\\n\\n**\u25fc Level 2 scaling: adjustment of OBServer node specifications**\\n\\nIf the Level 1 scaling cannot handle the soaring business traffic, you can increase specifications of the OBServer nodes, which is also known as vertical scaling. For example, you can scale the number of CPU cores of a cluster from 30 to 62. Unlike a MySQL database, whose capacity expansion involves a primary/secondary switchover, and causes business interruptions, OceanBase Cloud synchronizes data among nodes over the Paxos protocol, which features automatic leader election and automatic decision of whether to commit a log.\\n\\nThis brings two advantages:\\n\\n- OceanBase Cloud synchronizes data by a smaller data unit, delivering higher performance and flexibility. Compared with node-level log synchronization of MySQL, partitions of a Paxos group in OceanBase Cloud are smaller in size. This avoids the need of compromising performance for ensuring the global order. In addition, OceanBase Cloud supports distributed transactions and allows leaders to be stored on different nodes. As shown in the figure depicting the overall system architecture of OceanBase Cloud, the leaders (indicated in dark blue) are distributed on different nodes to support multi-point writing. This makes full use of resources of multiple servers and allows you to add more nodes.\\n- OceanBase Cloud synchronizes data by using more lightweight physical transaction logs (clogs) at lower costs over the Paxos protocol. When a MySQL database synchronizes data, the primary node generates logical binlogs, which are synchronized to the secondary nodes and then converted into relay logs for execution. With lighter and more efficient clogs, plus the partition-level synchronization granularity of the Paxos protocol, OceanBase Cloud is not affected by the troublesome primary/secondary synchronization latency like MySQL.\\n\\nWhen you adjust node specifications during scaling, OceanBase Cloud first mounts a node and synchronizes data to it from the leader. When the leader commits its last clog, OceanBase Cloud initiates a Paxos-based leader election, where the current leader gives up its leader role and votes for another node. Then, the leader role is smoothly switched to another node. This scale-up process of OceanBase Cloud, compared with MySQL, is nearly transparent to applications.\\n\\n![1691647876](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647875877.png)\\n\\n**\u25fc Level 3 scaling: adjustment of the number of OBServer nodes**\\n\\nThis is something that cannot be done by using a MySQL database in primary/secondary architecture. With its native distributed architecture, OceanBase Cloud supports distributed transactions and horizontal scale-out with zero business interruptions. Specifically, once an OBServer node is added to an OceanBase cluster, business traffic is automatically channeled to this new node. During this process, applications use the cluster like a standalone MySQL database without being affected. This benefit has been proven to be a better solution than database and table sharding in many engineering practices.\\n\\n![1691647918](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647918807.png)\\n\\n## **IV. Handle OLTP and OLAP in one HTAP-capable system**\\n\\nMySQL is a typical OLTP database system. Therefore, its optimizer shows unsatisfactory performance in joining large tables and handling queries with large result sets. In a MySQL database, it sometimes takes forever to execute an SQL query, and it does not support flexible resource isolation. Transaction processing (TP) tasks are often affected by large SQL statements. That is why everybody is avoiding using MySQL for analytical processing (AP) tasks.\\n\\nWhat if AP tasks are unavoidable? A common solution is to build a dedicated analytical instance and execute AP tasks through the asynchronous extract-transform-load (ETL) process. This solution inevitably incurs costs of the transmission service and the analytical instance.\\n\\nOceanBase Cloud is an HTAP-capable system that handles both OLTP and online analytical processing (OLAP) requests in the same cluster. So why does OceanBase Cloud outperform MySQL in this regard? We have strengthened its AP capabilities from the following four aspects:\\n\\n- OceanBase Cloud provides an enterprise-level optimizer that is comparable to the Oracle optimizer. Despite the query complexity, for example, joining more than 10 tables, or the way an SQL statement is written, the OceanBase optimizer generates optimal execution plans to guarantee the lowest cost of SQL executions.\\n- As mentioned earlier, OceanBase Cloud stores data in microblocks by column. AP tasks that are executed by columns can be faster based on columnar storage.\\n- OceanBase Cloud supports parallel execution. It divides a large SQL execution plan into multiple smaller tasks and launch multiple threads to process these small tasks in parallel. At present, SQL queries and DML and DDL operations can be executed in parallel to accelerate AP tasks.\\n- OceanBase Cloud provides a vectorized execution engine. Compared with a volcano model that reads data by row, the vectorized engine reads data in batches, which is more friendly to large analytical SQL queries.\\n\\n![1691647973](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647973201.png)\\n\\nThe HTAP capabilities of OceanBase Cloud allow you to handle AP and TP requests in one system by using the same data set, without incurring extra costs or the need of creating a dedicated analytical node.\\n\\nYou may be worried that TP tasks are impacted by AP tasks. OceanBase Cloud solves that issue by providing multiple resource isolation methods:\\n\\n- Physical isolation between tenants: A dedicated tenant is created to handle AP business.\\n- Physical isolation between zones in the same tenant: A dedicated zone that contains only read-only replicas is created to handle AP business.\\n- Isolation between resource groups in the same tenant: Dedicated resources of a node are allocated to handle AP business through resource isolation.\\n\\n![1691648013](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691648013527.png)\\n\\nAs a DBA or O&M engineer, how can you make use of OceanBase Cloud capabilities to achieve significant cost reduction for your enterprise?\\n\\nGenerally, you can deploy an OceanBase cluster whose CPU and memory resources are 80% to 95% of those of your original MySQL database and storage capacity is 15% to 20% thereof. Here are two examples to help you develop a cost reduction plan based on OceanBase Cloud and estimate the result.\\n\\n### Example 1: Cost reduction plan for an SME\\n\\nAs shown in the following figure, the enterprise has one RDS instance with 16 CPU cores, two RDS instances with 16 CPU cores each, and four RDS instances with 2 CPU cores each, which is a typical product array.\\n\\n- The largest 16C instance hosts core external business of the enterprise, and runs with high resource utilization.\\n- The two 4C instances handle secondary business, such as the business of an inventory or order management system. Their resource utilization is lower than the core instance.\\n- The four 2C instances handle internal business.\\n\\nThe resource utilization of these instances will fluctuate with the business traffic, causing troubles in O&M. Given the aforesaid specification estimation rule, the enterprise can migrate all its business systems to an OceanBase cluster with 30 CPU cores and 1.5 TB of storage. The OceanBase cluster not only takes care of all its business modules, but also optimizes its imbalanced resource utilization to a healthy level.\\n\\n![1691648052](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691648052884.png)\\n\\nIn this example, OceanBase Cloud is deployed across multiple zones and is compared against a MySQL cluster. The MySQL cluster also requires multi-zone deployment where each zone is configured with exclusive resources. They are tested by using Sysbench with 1,000 concurrent read/write processes. OceanBase Cloud reduces the cost by about 30%, and the result comparison is shown in the following figure.\\n\\n![1691648078](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691648078609.png)\\n\\n## Example 2: Cost reduction plan for a corporation\\n\\nThis corporation runs a 32C RDS instance to support its core business, a 16C RDS instance to support its secondary business, and five 4C RDS instances to support its internal low-traffic business. The overall resource utilization of the RDS instances is roughly the same with that in Example 1. The corporation can host all its RDS instances in an OceanBase cluster with 62 CPU cores and 4 TB of storage.\\n\\n![1691648109](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691648109839.png)\\n\\nRegarding how to plan costs, here is a brief introduction to the difference between the two. In the Sysbench test scenario with 1,000 concurrent read/write processes, if the primary node of a MySQL database uses 68 CPU cores, another 136 CPU cores must be used for the secondary nodes, resulting in serious resource waste. OceanBase Cloud, on the contrary, allows both its leader and follower replicas to share the 62 CPU cores of all OBServer nodes, reducing the total cost by about 40%. The result comparison is shown in the following figure.\\n\\n![1691648148](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691648148198.png)\\n\\nIn general, we should combine cost reduction with efficiency improvement. OceanBase Cloud relies on technical means to reduce costs, in the expectation that the overall value of a database can be improved without sacrificing data throughput, high availability, online DDL capability, or O&M friendliness for cost reduction.\\n\\nAs OceanBase Cloud becomes better, we hope that it can bring more benefits to enterprises and help enterprises boost efficiency at reduced costs. In this way, related personnel such as DBAs and O&M engineers can be freed from burdensome maintenance chores and have more time to create greater value for the enterprise."},{"id":"adaptive-sql-execution-engine","metadata":{"permalink":"/blog/adaptive-sql-execution-engine","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/tech-adaptive-engine.md","source":"@site/blog/tech-adaptive-engine.md","title":"Adaptive Techniques in the OceanBase SQL Execution Engine","description":"I have been studying the book \\"An Interpretation of OceanBase Database Source Code\\" and noticed that it contains very little content about the SQL executor. The book focuses on parallel execution in the executor. This blog post introduces some common adaptive techniques in the executor of OceanBase Database. You can take it as a supplement to the executor part in this book.","date":"2024-06-03T13:23:07.000Z","tags":[],"readingTime":11.445,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"adaptive-sql-execution-engine","title":"Adaptive Techniques in the OceanBase SQL Execution Engine"},"unlisted":false,"prevItem":{"title":"How OB Cloud Achieves Cost Reduction and Efficiency Improvement in Replacing MySQL Scenarios \uff1f","permalink":"/blog/obcloud"},"nextItem":{"title":"OceanBase Distributed Pushdown Technology","permalink":"/blog/distributed-push-down"}},"content":"> I have been studying the book \\"An Interpretation of OceanBase Database Source Code\\" and noticed that it contains very little content about the SQL executor. The book focuses on parallel execution in the executor. This blog post introduces some common adaptive techniques in the executor of OceanBase Database. You can take it as a supplement to the executor part in this book.\\n\\n## Challenges facing AP performance improvement\\n\\nIf you want to improve the AP performance of a database, you will face three challenges:\\n\\n- First, the optimizer cannot ensure that its estimates are always absolutely accurate. The reasons are complex. For example, the statistics are inaccurate in some scenarios, or the cost model is inconsistent with the actual model. These reasons will contribute to a non-optimal execution plan.\\n- Second, data skew often occurs in production and business scenarios, which will significantly affect the execution efficiency, especially the parallel execution efficiency.\\n- Third, the semantics of NULL are special. Characteristics of widespread NULL values are different from those of normal values in operations such as joins, but this is easily ignored. The executor must perform special processing on NULL values. Otherwise, various bad cases can occur.\\n\\nAdaptive techniques enable the execution engine to dynamically adjust the execution strategy based on the actual situation, thereby improving the execution performance. In a word, adaptive techniques are introduced to address the preceding challenges. Next, let me introduce some typical adaptive techniques in the executor of OceanBase Database.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Adaptive join filter\\n\\nLet me take a hash join shown in the following figure as an example to introduce the background of join filters. The hash join involves two tables and uses hash repartitioning as the data shuffle mode. In other words, each row in the left-side and right-side tables will be repartitioned based on the hash value.\\n\\n![1705636564](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636564738.png)\\n\\nGenerally, the right-side table of a hash join is very large in size, which will lead to a high cost in data shuffle. When the left-side table is read to build the hash table, a join filter can extract the data characteristics of the left-side table and send them to the right-side table. This can filter out some of the data in the right-side table before a shuffle. If the join filter has high filtering performance, this step can significantly reduce the network overhead.\\n\\n![1705636577](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636577135.png)\\n\\nOceanBase Database has implemented join filters as early as in V3.x and has been undergoing join filter optimization ever since. The following figure shows the impact of join filters on the overall performance during the TPC-H benchmark run in 2021, in which OceanBase Database won the first place in the world.\\n\\n![1705636587](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636587368.png)\\n\\nJoin filters help significantly improve the performance for joins on large tables, such as Q9 shown in the preceding figure.\\n\\nHowever, join filters cannot always bring positive benefits in all scenarios, such as Q18 shown in the preceding figure. The overhead of a join filter is used for three tasks:\\n\\n- Create the join filter. This is to extract the data characteristics of the left-side table in a hash join when the left-side table is read to build the hash table.\\n- Send the join filter. This is to send the data characteristics of the left-side table to the right-side table.\\n- Apply the join filter. This is to apply the data characteristics of the left-side table on the right-side table for row filtering.\\n\\nIf the selectivity of the join filter is low, the reduced network overhead cannot make up the preceding overhead, and the overall performance will deteriorate.\\n\\nThe optimizer determines whether to allocate a join filter based on the cost. The optimizer can roughly estimate the selectivity of a join filter based on statistics such as the number of distinct values (NDV) and MIN/MAX values. However, the optimizer cannot provide accurate estimates of intermediate calculation results in the executor.\\n\\nTo resolve this issue, OceanBase Database V4.1 implements sliding window-based adaptive join filters. This algorithm aims to make up the performance loss when an incorrect join filter is applied.\\n\\nThis algorithm splits data into multiple sliding windows and collects statistics on the apply process of each window. If the algorithm detects that the filtering effects of a window are not as expected, it will not apply the join filter on the next window and pass this window. If the filtering effects of continuous windows are not as expected, the number of passed windows will also increase linearly to reduce the apply cost.\\n\\n![1705636596](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636596184.png)\\n\\nThe following figure shows a bad case of join filter. Different strategies are used for performance tests. In the performance test where an adaptive join filter is used, the performance loss is made up by half. However, the performance after compensation is still lower than that achieved when the join filter is not allocated.\\n\\n![1705636604](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636604838.png)\\n\\nAlthough this solution makes up the performance loss caused by applying a join filter on the right-side table, the cost in creating and sending the join filter is not made up. OceanBase Database will enhance the capability for adaptive join filter creation in later versions.\\n\\n## Adaptive HASH GROUP BY\\n\\nThis section introduces the adaptive algorithm for HASH GROUP BY in OceanBase Database.\\n\\nThe following figures show the execution plans for HASH GROUP BY in a parallel scenario.\\n\\nHere is the execution plan for two-phase HASH GROUP BY.\\n\\n![1705636615](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636615372.png)\\n\\n![1705636625](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636625338.png)\\n\\nHere is the execution plan for one-phase HASH GROUP BY.\\n\\n![1705636636](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636635870.png)\\n\\nThe difference is that two-phase HASH GROUP BY performs a partial GROUP BY operation on data before a shuffle. Like join filters, one-phase and two-phase HASH GROUP BY have their own advantages and disadvantages.\\n\\n- Two-phase HASH GROUP BY applies to scenarios with a high data aggregation rate, where the amount of data to be shuffled can be decreased through pre-aggregation.\\n\\n![1705636648](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636648890.png)\\n\\n- One-phase HASH GROUP BY applies to scenarios with a low data aggregation rate.\\n\\n![1705636705](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636705335.png)\\n\\nIf the data aggregation rate is low, the network overhead will still be high because a two-phase plan will consume extra CPU resources to probe the hash table, leading to poorer performance than a one-phase plan.\\n\\nThe following figure compares the performance of two-phase and one-phase plans for queries in ClickBench. It can be observed that some queries are suitable for two-phase execution while others are suitable for one-phase execution. Generally, the optimizer tends to select a two-phase plan to avoid serious performance deterioration.\\n\\n![1705636714](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636714319.png)\\n\\nIn versions earlier than OceanBase Database V4.x, the optimizer will determine whether to select a two-phase plan or one-phase plan based on the NDV value in the statistics. You can also use the session variable \\\\_GROUPBY_NOPUSHDOWN_CUT_RATIO to set the plan preference. If the ratio of the input data amount to the data amount after aggregation is greater than the specified value, a two-phase plan is generated. Otherwise, a one-phase plan is generated. In practice, it is difficult to use this variable. The input and output data amounts of GROUP BY are estimated by the optimizer based on statistics. Generally, it is challenging for O&M personnel to set this variable to an appropriate value, ensuring that the optimizer selects a better plan for GROUP BY.\\n\\n![1705636724](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636723981.png)\\n\\nIn OceanBase Database V4.x, the \\\\_GROUPBY_NOPUSHDOWN_CUT_RATIO variable is deprecated and the optimizer is forced to select a two-phase plan. In a two-phase plan in V4.x, the first phase must be adaptive GROUP BY.\\n\\n![1705636736](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636736767.png)\\n\\nThe core idea of the adaptive GROUP BY technique is to determine whether to perform deduplication or directly send data based on an NDV value collected in real time. The technique splits data into multiple rounds and measures the aggregation rate of each round. If the deduplication rate of a round is not as expected, the technique will clear the hash table, flush all the data obtained in the first phase to the network, and aggregate the final data in the second phase.\\n\\nData is split into rounds based on the size of three-level CPU caches. This is because if the hash table can be accommodated in the L2 cache, the performance can be improved by more than 30% compared with that of a large hash table. A cache-aware mechanism is provided to increase the size of data in each round from the L2 cache size to the L3 cache size when the deduplication rate becomes low so that data will be accommodated in the L3 cache.\\n\\nIf the hash deduplication effects of multiple consecutive rounds are poor, the bypass strategy is used. Specifically, rows are directly delivered to the upper-layer operator without hash deduplication, which looks like a one-phase plan.\\n\\n![1705636744](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636744584.png)\\n\\nThis strategy greatly improves the performance but also has bad cases where a large amount of data is involved while the overall deduplication rate is favorable. If the overall deduplication rate is estimated based on only a small part of the data, the estimate is probably inaccurate. In OceanBase Database V4.2, the NDV values of multiple data rounds are merged to improve the estimate accuracy.\\n\\nThe following figure compares the performance of one-phase, adaptive, and two-phase GROUP BY for queries in ClickBench.\\n\\n![1705636754](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636754240.png)\\n\\nThe result shows that adaptive GROUP BY can select an appropriate execution strategy almost in all scenarios. With adaptive GROUP BY, one plan applies to different data models. This is the goal we aim to achieve. OceanBase Database V4.3 will support a global NDV estimate strategy to make adaptive decision-making more accurate.\\n\\n## Adaptive hybrid hash shuffling\\n\\nNext, let me introduce some adaptive techniques we have developed based on data skew. The following figures show two common plans for a simple distributed hash join.\\n\\nOne is a broadcast plan, which broadcasts the left-side table to each thread of the right-side table. The threads will use data in the right-side table to probe data in the left-side table.\\n\\n![1705636765](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636765811.png)\\n\\n![1705636773](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636772964.png)\\n\\nThe other is a hash repartitioning plan, which distributes the data in the left-side and right-side tables to different threads based on the hash value. Each thread performs a join separately.\\n\\n![1705636781](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636780959.png)\\n\\n![1705636790](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636790880.png)\\n\\nThe two plans have their own advantages and disadvantages. Generally, the broadcast plan applies to a scenario where a large table is joined with a small table. In this scenario, the small table is broadcast to limit the broadcast cost. In a scenario where two large tables are joined and repartitioning based on the join key is not supported, hash-hash is almost the only choice. However, the hash-hash strategy also has bad cases in data skew scenarios. For example, the following figure shows a high-frequency value. Since data is distributed to different threads based on the hash value for hash repartitioning, all instances of the high-frequency value are distributed to the same hash join, leading to a long-tail situation of the hash join.\\n\\n![1705636798](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636798440.png)\\n\\nThe following figure shows a similar business scenario as observed by the SQL plan monitor tool.\\n\\n![1705636805](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636805216.png)\\n\\nTo resolve this issue, OceanBase Database V4.x implements the hybrid hash shuffling algorithm. The following figure shows an execution plan that uses this algorithm. It looks like a plan that uses the hash repartitioning algorithm. The only difference is that a HYBRID keyword is contained in the EXCHANGE operator.\\n\\n![1705636812](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636812110.png)\\n\\nThe hybrid hash shuffling algorithm will obtain related information about high-frequency values from the optimizer. For regular values, normal hash shuffling is used for hash repartitioning. For a high-frequency value, if it exists on the left side (hash join build side), the value will be broadcast to all threads to build the hash table. If it exists on the right side (hash join probe side), the instances of this value are randomly distributed to ensure the evenness. This algorithm can effectively resolve performance issues caused by hash repartitioning in data skew scenarios.\\n\\n![1705636821](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636821493.png)\\n\\n## Adaptive NULL-aware hash join\\n\\nFinally, let me briefly introduce some optimization techniques we have applied to handle NULL values. For a join, the return result of a NULL value is always NULL in an equal condition. However, the semantics of NULL vary based on the join method.\\n\\nIn inner joins and semi-joins, values whose join key is NULL can be ignored.\\n\\n![1705636829](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636829687.png)\\n\\nIn left outer joins, NULL values on the left side also need to be output.\\n\\n![1705636836](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636835954.png)\\n\\nIf NULL values are processed as normal values, correct results can be obtained. However, data skew of NULL values or a useless network shuffle of massive NULL values may occur. The following special measures are taken inside hash joins and during a hash join shuffle:\\n\\n- Skip NULL values in join keys. This measure usually applies to inner joins and semi-joins, in which values whose join key is NULL will not be output.\\n- Randomly distribute NULL values to avoid data skew. Generally, for an outer join, specifically, the left side of a left outer join, right side of a right outer join, or both sides of a full outer join, if the NULL values of one side or both sides need to be output, these NULL values will not successfully match any value. In this case, random hash values are assigned to these NULL values and the NULL values are randomly distributed to different threads. A NULL value that does not need to be output will still be skipped.\\n- Use the NULL-aware anti-join algorithm, which will not be described in this post, to process anti-joins that contain NOT IN. The semantics of such anti-joins are special.\\n\\n![1705636844](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636844393.png)\\n\\n## Preview of the next post\\n\\nThis post introduces some representative adaptive techniques in the executor of OceanBase Database, based on the assumption that you have a basic understanding of the two-phase pushdown technique for HASH GROUP BY. If you are unfamiliar with the multi-phase pushdown technique of the executor, please look forward to the next post [Distributed Pushdown Techniques of OceanBase Database](https://open.oceanbase.com/blog/5382203648)."},{"id":"distributed-push-down","metadata":{"permalink":"/blog/distributed-push-down","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/tech-distributed-push-down.md","source":"@site/blog/tech-distributed-push-down.md","title":"OceanBase Distributed Pushdown Technology","description":"I have been studying the book \\"An Interpretation of OceanBase Database Source Code\\" and noticed that it contains very little content about the SQL executor. Therefore, I want to write some blog posts about the SQL executor as a supplement to this book. In my last post Adaptive Techniques in the OceanBase Database Execution Engine, I introduced some representative adaptive techniques in the executor, based on the assumption that you have a basic understanding of the two-phase pushdown technique for HASH GROUP BY. If you are unfamiliar with the multi-phase pushdown technique of the executor, you are welcome to read this post to learn about common adaptive distributed pushdown techniques in OceanBase Database.","date":"2024-06-03T13:23:07.000Z","tags":[],"readingTime":13.545,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"distributed-push-down","title":"OceanBase Distributed Pushdown Technology"},"unlisted":false,"prevItem":{"title":"Adaptive Techniques in the OceanBase SQL Execution Engine","permalink":"/blog/adaptive-sql-execution-engine"},"nextItem":{"title":"OceanBase Technical Insights for High-Concurrency Scenarios","permalink":"/blog/high-concurrency"}},"content":"> I have been studying the book \\"An Interpretation of OceanBase Database Source Code\\" and noticed that it contains very little content about the SQL executor. Therefore, I want to write some blog posts about the SQL executor as a supplement to this book. In my last post [Adaptive Techniques in the OceanBase Database Execution Engine](https://open.oceanbase.com/blog/5250647552), I introduced some representative adaptive techniques in the executor, based on the assumption that you have a basic understanding of the two-phase pushdown technique for HASH GROUP BY. If you are unfamiliar with the multi-phase pushdown technique of the executor, you are welcome to read this post to learn about common adaptive distributed pushdown techniques in OceanBase Database.\\n\\n## What is distributed pushdown?\\n\\nTo better utilize parallel execution capabilities and reduce CPU and network overheads during distributed execution, the optimizer often pushes down some operators to lower-layer compute nodes when it generates execution plans. This is to make full use of the computing resources of the cluster to improve the execution efficiency. Next, I\'m going to introduce the most common distributed pushdown techniques in OceanBase Database.\\n\\n\x3c!-- truncate --\x3e\\n\\n## LIMIT pushdown\\n\\nLet me first talk about the pushdown of the LIMIT operator. The following are two SQL statements for creating a table named `order` and reading 100 rows from the `orders` table, respectively.\\n\\n```SQL\\nCREATE TABLE `orders` (\\n    `o_orderkey` bigint(20) NOT NULL,\\n    `o_custkey` bigint(20) NOT NULL,\\n    `o_orderdate` date NOT NULL,\\n    PRIMARY KEY (`o_orderkey`, `o_orderdate`, `o_custkey`),\\n    KEY `o_orderkey` (`o_orderkey`) LOCAL  BLOCK_SIZE 16384\\n)  partition by range columns(o_orderdate)\\n    subpartition by hash(o_custkey) subpartitions 64\\n(partition ord1 values less than (\'1992-01-01\'),\\npartition ord2 values less than (\'1992-02-01\'),\\npartition ord3 values less than (\'1992-03-01\'),\\npartition ord77 values less than (\'1998-05-01\'),\\npartition ord78 values less than (\'1998-06-01\'),\\npartition ord79 values less than (\'1998-07-01\'),\\npartition ord80 values less than (\'1998-08-01\'),\\npartition ord81 values less than (MAXVALUE));\\n\\nselect * from orders limit 100;\\n```\\n\\nThe following plan shows a very common scenario of distributed pushdown.\\n\\n```SQL\\nexplain select * from orders limit 100;\\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n| Query Plan                                                                                                                                                      |\\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n| =================================================================                                                                                               |\\n| |ID|OPERATOR                     |NAME    |EST.ROWS|EST.TIME(us)|                                                                                               |\\n| -----------------------------------------------------------------                                                                                               |\\n| |0 |LIMIT                        |        |1       |2794        |                                                                                               |\\n| |1 |\u2514\u2500PX COORDINATOR             |        |1       |2794        |                                                                                               |\\n| |2 |  \u2514\u2500EXCHANGE OUT DISTR       |:EX10000|1       |2793        |                                                                                               |\\n| |3 |    \u2514\u2500LIMIT                  |        |1       |2792        |                                                                                               |\\n| |4 |      \u2514\u2500PX PARTITION ITERATOR|        |1       |2792        |                                                                                               |\\n| |5 |        \u2514\u2500TABLE FULL SCAN    |orders  |1       |2792        |                                                                                               |\\n| =================================================================                                                                                               |\\n| Outputs & filters:                                                                                                                                              |\\n| -------------------------------------                                                                                                                           |\\n|   0 - output([orders.o_orderkey], [orders.o_custkey], [orders.o_orderdate]), filter(nil)                                                                        |\\n|       limit(100), offset(nil)                                                                                                                                   |\\n|   1 - output([orders.o_orderkey], [orders.o_custkey], [orders.o_orderdate]), filter(nil)                                                                        |\\n|   2 - output([orders.o_orderkey], [orders.o_custkey], [orders.o_orderdate]), filter(nil)                                                                        |\\n|       dop=1                                                                                                                                                     |\\n|   3 - output([orders.o_orderkey], [orders.o_custkey], [orders.o_orderdate]), filter(nil)                                                                        |\\n|       limit(100), offset(nil)                                                                                                                                   |\\n|   4 - output([orders.o_orderkey], [orders.o_orderdate], [orders.o_custkey]), filter(nil)                                                                        |\\n|       force partition granule                                                                                                                                   |\\n|   5 - output([orders.o_orderkey], [orders.o_orderdate], [orders.o_custkey]), filter(nil)                                                                        |\\n|       access([orders.o_orderkey], [orders.o_orderdate], [orders.o_custkey]), partitions(p0sp[0-63], p1sp[0-63], p2sp[0-63], p3sp[0-63], p4sp[0-63], p5sp[0-63], |\\n|        p6sp[0-63], p7sp[0-63])                                                                                                                                  |\\n|       limit(100), offset(nil), is_index_back=false, is_global_index=false,                                                                                      |\\n|       range_key([orders.o_orderkey], [orders.o_orderdate], [orders.o_custkey]), range(MIN,MIN,MIN ; MAX,MAX,MAX)always true                                     |\\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n```\\n\\nYou can see that Operators 0 and 3 in the plan are both LIMIT. Operator 0 is pushed down to generate Operator 3 to reduce the number of rows scanned by Operator 5, a TABLE SCAN operator, from each partition of the `orders` table. Each thread of the TABLE SCAN operator scans at most 100 rows. This reduces the overhead in data scan by the TABLE SCAN operator and the network overhead in sending data to Operator 1 for aggregation. At present in OceanBase Database, the EXCHANGE operator will send a packet after it receives 64 KB data from a lower-layer operator. If the LIMIT operator is not pushed down, massive data may be scanned, leading to a high network overhead.\\n\\nIn actual business scenarios, a LIMIT operator is usually used in combination with the ORDER BY keyword. If the ORDER BY keyword is used in the preceding example, a TOP-N SORT operator with much higher performance than a SORT operator will be generated in the plan.\\n\\n```SQL\\nexplain select * from orders order by o_orderdate limit 100;\\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n| Query Plan                                                                                                                                                      |\\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n| =================================================================                                                                                               |\\n| |ID|OPERATOR                     |NAME    |EST.ROWS|EST.TIME(us)|                                                                                               |\\n| -----------------------------------------------------------------                                                                                               |\\n| |0 |LIMIT                        |        |1       |2794        |                                                                                               |\\n| |1 |\u2514\u2500PX COORDINATOR MERGE SORT  |        |1       |2794        |                                                                                               |\\n| |2 |  \u2514\u2500EXCHANGE OUT DISTR       |:EX10000|1       |2793        |                                                                                               |\\n| |3 |    \u2514\u2500TOP-N SORT             |        |1       |2792        |                                                                                               |\\n| |4 |      \u2514\u2500PX PARTITION ITERATOR|        |1       |2792        |                                                                                               |\\n| |5 |        \u2514\u2500TABLE FULL SCAN    |orders  |1       |2792        |                                                                                               |\\n| =================================================================                                                                                               |\\n| Outputs & filters:                                                                                                                                              |\\n| -------------------------------------                                                                                                                           |\\n|   0 - output([orders.o_orderkey], [orders.o_custkey], [orders.o_orderdate]), filter(nil)                                                                        |\\n|       limit(100), offset(nil)                                                                                                                                   |\\n|   1 - output([orders.o_orderkey], [orders.o_custkey], [orders.o_orderdate]), filter(nil)                                                                        |\\n|       sort_keys([orders.o_orderdate, ASC])                                                                                                                      |\\n|   2 - output([orders.o_orderkey], [orders.o_custkey], [orders.o_orderdate]), filter(nil)                                                                        |\\n|       dop=1                                                                                                                                                     |\\n|   3 - output([orders.o_orderkey], [orders.o_custkey], [orders.o_orderdate]), filter(nil)                                                                        |\\n|       sort_keys([orders.o_orderdate, ASC]), topn(100)                                                                                                           |\\n|   4 - output([orders.o_orderkey], [orders.o_orderdate], [orders.o_custkey]), filter(nil)                                                                        |\\n|       force partition granule                                                                                                                                   |\\n|   5 - output([orders.o_orderkey], [orders.o_orderdate], [orders.o_custkey]), filter(nil)                                                                        |\\n|       access([orders.o_orderkey], [orders.o_orderdate], [orders.o_custkey]), partitions(p0sp[0-63], p1sp[0-63], p2sp[0-63], p3sp[0-63], p4sp[0-63], p5sp[0-63], |\\n|        p6sp[0-63], p7sp[0-63])                                                                                                                                  |\\n|       is_index_back=false, is_global_index=false,                                                                                                               |\\n|       range_key([orders.o_orderkey], [orders.o_orderdate], [orders.o_custkey]), range(MIN,MIN,MIN ; MAX,MAX,MAX)always true                                     |\\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\\n```\\n\\nIf the LIMIT operator is not pushed down, Operator 3 will be a SORT operator. In this case, each thread needs to sort and send all the scanned data to the upper-layer data flow object (DFO). A DFO is a sub-plan. Adjacent DFOs are separated with an EXCHANGE operator. For more information, see [https://www.oceanbase.com/docs/common-oceanbase-database-1000000000034037](https://www.oceanbase.com/docs/common-oceanbase-database-1000000000034037).\\n\\nThe purpose of pushing down the LIMIT operator is to end execution in advance to reduce calculation and network overheads.\\n\\n## AGGREGATION pushdown\\n\\nLet me take the following statement that contains GROUP BY as an example to describe distributed pushdown in aggregation.\\n\\n```SQL\\nselect count(o_totalprice), sum(o_totalprice) from orders group by o_orderdate;\\n```\\n\\nThis SQL statement queries the daily order count and sales amount. If you want to execute the statement in parallel, the most straightforward approach would be to distribute data in the table based on the hash values of the GROUP BY column (o_orderdate). This way, all rows with the same o_orderdate value are sent to the same thread. The threads can aggregate received data in parallel.\\n\\nHowever, this plan requires to perform a shuffle on all data in the table, which may lead to a very high network overhead. Moreover, if data skew occurs in the table, for example, a large number of orders were placed on a specific day, the workload of the thread responsible for processing orders of this day will be much higher than that of other threads. This long-tail task may directly lead to a long execution time for the query.\\n\\nTo address these issues, the GROUP BY operator is pushed down to generate the following plan:\\n\\n```SQL\\nexplain select count(o_totalprice), sum(o_totalprice) from orders group by o_orderdate;\\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\\n| Query Plan                                                                                                                                                |\\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\\n| =====================================================================                                                                                     |\\n| |ID|OPERATOR                         |NAME    |EST.ROWS|EST.TIME(us)|                                                                                     |\\n| ---------------------------------------------------------------------                                                                                     |\\n| |0 |PX COORDINATOR                   |        |1       |2796        |                                                                                     |\\n| |1 |\u2514\u2500EXCHANGE OUT DISTR             |:EX10001|1       |2795        |                                                                                     |\\n| |2 |  \u2514\u2500HASH GROUP BY                |        |1       |2795        |                                                                                     |\\n| |3 |    \u2514\u2500EXCHANGE IN DISTR          |        |1       |2794        |                                                                                     |\\n| |4 |      \u2514\u2500EXCHANGE OUT DISTR (HASH)|:EX10000|1       |2794        |                                                                                     |\\n| |5 |        \u2514\u2500HASH GROUP BY          |        |1       |2793        |                                                                                     |\\n| |6 |          \u2514\u2500PX PARTITION ITERATOR|        |1       |2792        |                                                                                     |\\n| |7 |            \u2514\u2500TABLE FULL SCAN    |orders  |1       |2792        |                                                                                     |\\n| =====================================================================                                                                                     |\\n| Outputs & filters:                                                                                                                                        |\\n| -------------------------------------                                                                                                                     |\\n|   0 - output([INTERNAL_FUNCTION(T_FUN_COUNT_SUM(T_FUN_COUNT(orders.o_totalprice)), T_FUN_SUM(T_FUN_SUM(orders.o_totalprice)))]), filter(nil)              |\\n|   1 - output([INTERNAL_FUNCTION(T_FUN_COUNT_SUM(T_FUN_COUNT(orders.o_totalprice)), T_FUN_SUM(T_FUN_SUM(orders.o_totalprice)))]), filter(nil)              |\\n|       dop=1                                                                                                                                               |\\n|   2 - output([T_FUN_COUNT_SUM(T_FUN_COUNT(orders.o_totalprice))], [T_FUN_SUM(T_FUN_SUM(orders.o_totalprice))]), filter(nil)                               |\\n|       group([orders.o_orderdate]), agg_func([T_FUN_COUNT_SUM(T_FUN_COUNT(orders.o_totalprice))], [T_FUN_SUM(T_FUN_SUM(orders.o_totalprice))])             |\\n|   3 - output([orders.o_orderdate], [T_FUN_COUNT(orders.o_totalprice)], [T_FUN_SUM(orders.o_totalprice)]), filter(nil)                                     |\\n|   4 - output([orders.o_orderdate], [T_FUN_COUNT(orders.o_totalprice)], [T_FUN_SUM(orders.o_totalprice)]), filter(nil)                                     |\\n|       (#keys=1, [orders.o_orderdate]), dop=1                                                                                                              |\\n|   5 - output([orders.o_orderdate], [T_FUN_COUNT(orders.o_totalprice)], [T_FUN_SUM(orders.o_totalprice)]), filter(nil)                                     |\\n|       group([orders.o_orderdate]), agg_func([T_FUN_COUNT(orders.o_totalprice)], [T_FUN_SUM(orders.o_totalprice)])                                         |\\n|   6 - output([orders.o_orderdate], [orders.o_totalprice]), filter(nil)                                                                                    |\\n|       force partition granule                                                                                                                             |\\n|   7 - output([orders.o_orderdate], [orders.o_totalprice]), filter(nil)                                                                                    |\\n|       access([orders.o_orderdate], [orders.o_totalprice]), partitions(p0sp[0-63], p1sp[0-63], p2sp[0-63], p3sp[0-63], p4sp[0-63], p5sp[0-63], p6sp[0-63], |\\n|        p7sp[0-63])                                                                                                                                        |\\n|       is_index_back=false, is_global_index=false,                                                                                                         |\\n|       range_key([orders.o_orderkey], [orders.o_orderdate], [orders.o_custkey]), range(MIN,MIN,MIN ; MAX,MAX,MAX)always true                               |\\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\\n```\\n\\nIn this plan, each thread will pre-aggregate the data it reads before distributing the data. The pre-aggregate job is done by Operator 5, a GROUP BY operator. Then, Operator 5 will send the aggregation results to its upper-layer operator. Operator 2, another GROUP BY operator, will aggregate the received data again. After Operator 5 pre-aggregates the data, the data amount will remarkably decrease. This can decrease the network overhead caused by data shuffle and reduce the impact of data skew on the execution time.\\n\\nThen, let me demonstrate the execution process of the preceding SQL statement.\\n\\n```SQL\\nselect count(o_totalprice), sum(o_totalprice) from orders group by o_orderdate;\\n```\\n\\nThe original data comprises seven rows. The amount of each order is CNY 10. The orders were placed on June 1, June 2, and June 3.\\n\\n![1705644329](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705644329067.png)\\n\\nThe following figure shows the execution process, where the DOP is set to 2.\\n\\n![1705644337](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705644337050.png)\\n\\nThe first thread in the upper-left corner scans three rows, and the second thread in the lower-left corner scans four rows. Data with the same date, namely, data in the same group, is marked with the same color.\\n\\nThe first thread aggregates the three rows it scans, which are distributed in two groups. The dates of two rows are June 1. Therefore, for June 1, the order count is 2 and the sales amount is 20. The date of one row is June 3. Therefore, for June 3, the order count is 1 and the sales amount is 10. The four rows scanned by the second thread are also distributed in two groups. Two rows are generated after aggregation. The job of this part is completed by Operator 5 in the plan.\\n\\nThen, the two threads distribute the data based on the hash values of the o_orderdate column. Data with the same date is sent to the same thread. The job of this part is completed by Operators 3 and 4 in the plan.\\n\\nEach thread on the right side will aggregate the received data again. The two rows of June 3 scanned by the two threads on the left side, which are marked red, are sent to the thread in the lower-right corner. The two rows are aggregated again by the operator on the right side. After aggregation, the order count is 2 and the sales amount is 20. The two rows are finally aggregated into one row. The job of this part is completed by Operator 2 in the plan.\\n\\nThen, all data is sent to the coordinator, which will summarize the data and send the final calculation results to the client.\\n\\n## JOIN FILTER pushdown\\n\\nIn a JOIN operator, the join filters of the left-side table will be pushed down to the right-side table to perform pre-filtering and partition pruning for data in the right-side table.\\n\\n### Pre-filtering\\n\\nWhen a hash join is executed, the data in the left-side table is always read first to build a hash table. Then, the data in the right-side table is used to probe the hash table, and if successful, the data will be sent to the upper-layer operator. If a reshuffle is performed on the data in the right-side table of the hash join, the network overhead may be high, which is subject to the data amount of the right-side table. In this case, join filters can be used to reduce the network overhead caused by data shuffle.\\n\\nTake the plan shown in the following figure as an example.\\n\\n![1705644346](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705644346491.png)\\n\\nIn the plan, Operator 2, a HASH JOIN operator, reads data from the left-side table. During the read, it will use the t1.c1 join key to create a join filter, which is done by Operator 3, a JOIN FILTER CREATE operator. The most common form of join filter is Bloom filter. After the join filter is created, it is sent to the right-side DFO, which contains Operator 6 and other lower-layer operators.\\n\\nOperator 10, a TABLE SCAN operator, has a filter sys_op_bloom_filter(t2.c1), which specifies to use values of t2.c1 in the right-side table to quickly probe the hash table based on the Bloom filter. If a value of t2.c1 does not match any value of t1.c1, the row where the t2.c1 value is located in the t2 table can be pre-filtered and does not need to be sent to the HASH JOIN operator.\\n\\n### Partition pruning\\n\\nJoin filters can be used not only for row filtering but also for partition pruning (or filtering). Assume that t1 is a partitioned table and the join key is also its partitioning key. A plan shown in the following figure can be generated.\\n\\n![1705644359](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705644359718.png)\\n\\nIn this plan, Operator 3 is a PARTITION JOIN FILTER CREATE operator. It will detect the partitioning method of the right-side t1 table of the hash join. When it obtains a row in the left-side table from the lower-layer operator, it will use the c1 value to calculate the partition to which this row belongs in the right-side t1 table, and record the partition ID in the join filter. The join filter that contains the partition ID will be used on Operator 8 for partition pruning for the right-side table of the hash join. When the table scan operator scans each partition in the right-side table, it will verify whether the partition ID exists in the join filter. If no, it can skip the entire partition.\\n\\nA join filter can be used for data pre-filtering and partition pruning, thereby reducing the overheads in data scan, network transmission, and hash table probe. At present, OceanBase Database supports only Bloom filters in versions earlier than V4.2. OceanBase Database supports two new types of join filters since V4.2: In filter and Range filter. The new join filters can help significantly improve the performance in some scenarios, especially when the left-side table contains a few distinct values or contains continuous values.\\n\\n## Other distributed pushdown techniques\\n\\nApart from the preceding common distributed pushdown techniques that are easy to understand, OceanBase Database also supports more adaptive distributed pushdown techniques, such as adaptive two-phase pushdown for window functions and three-phase pushdown for aggregate functions.\\n\\nThis post will not provide a detailed introduction to the more complex distributed pushdown techniques of OceanBase Database. Below are sample execution plans of the two distributed pushdown techniques mentioned earlier for those who are interested in conducting further research.\\n\\nAdaptive two-phase pushdown for window functions:\\n\\n```SQL\\nselect /*+parallel(3) */\\n        c1, sum(c2) over (partition by c1) from t1 order by c1;\\nQuery Plan\\n===================================================\\n|ID|OPERATOR                             |NAME    |\\n---------------------------------------------------\\n|0 |PX COORDINATOR MERGE SORT            |        |\\n|1 | EXCHANGE OUT DISTR                  |:EX10001|\\n|2 |  MATERIAL                           |        |\\n|3 |   WINDOW FUNCTION CONSOLIDATOR      |        |\\n|4 |    EXCHANGE IN MERGE SORT DISTR     |        |\\n|5 |     EXCHANGE OUT DISTR (HASH HYBRID)|:EX10000|\\n|6 |      WINDOW FUNCTION                |        |\\n|7 |       SORT                          |        |\\n|8 |        PX BLOCK ITERATOR            |        |\\n|9 |         TABLE SCAN                  |t1      |\\n===================================================\\n```\\n\\nThree-phase pushdown for aggregate functions:\\n\\n```SQL\\nselect /*+ parallel(2) */\\n    c1, sum(distinct c2),count(distinct c3), sum(c4) from t group by c1;\\nQuery Plan\\n===========================================================================\\n|ID|OPERATOR                               |NAME    |EST.ROWS|EST.TIME(us)|\\n---------------------------------------------------------------------------\\n|0 |PX COORDINATOR                         |        |1       |8           |\\n|1 |\u2514\u2500EXCHANGE OUT DISTR                   |:EX10002|1       |7           |\\n|2 |  \u2514\u2500HASH GROUP BY                      |        |1       |6           |\\n|3 |    \u2514\u2500EXCHANGE IN DISTR                |        |2       |6           |\\n|4 |      \u2514\u2500EXCHANGE OUT DISTR (HASH)      |:EX10001|2       |6           |\\n|5 |        \u2514\u2500HASH GROUP BY                |        |2       |4           |\\n|6 |          \u2514\u2500EXCHANGE IN DISTR          |        |2       |4           |\\n|7 |            \u2514\u2500EXCHANGE OUT DISTR (HASH)|:EX10000|2       |3           |\\n|8 |              \u2514\u2500HASH GROUP BY          |        |2       |2           |\\n|9 |                \u2514\u2500PX BLOCK ITERATOR    |        |1       |1           |\\n|10|                  \u2514\u2500TABLE FULL SCAN    |t       |1       |1           |\\n===========================================================================\\n```\\n\\n## Preview of the next post\\n\\nThis post introduces several typical distributed pushdown techniques in the executor of OceanBase Database, based on the assumption that you have a basic understanding of distributed execution of the database. If you are unfamiliar with parallel execution techniques of the executor, please look forward to the next post [Parallel Execution Techniques of OceanBase Database](https://open.oceanbase.com/blog/5558373888)."},{"id":"high-concurrency","metadata":{"permalink":"/blog/high-concurrency","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/tech-high-concurrency.md","source":"@site/blog/tech-high-concurrency.md","title":"OceanBase Technical Insights for High-Concurrency Scenarios","description":"As large-scale promotions become the norm, it is a severe challenge for enterprises to ensure a smooth shopping experience for users during peak times, apart from annual shopping festivals such as \\"Double 11\\" and \\"618\\", by designing their application architecture and database architecture effectively to handle traffic surges. Based on its 10 years of experience in supporting \\"Double 11\\" as well as its features of online scaling, high concurrency, and low latency, OceanBase Database can quickly respond to business load changes without affecting the business, thereby improving system throughput during flash sales.","date":"2024-06-03T13:23:07.000Z","tags":[],"readingTime":14.415,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"high-concurrency","title":"OceanBase Technical Insights for High-Concurrency Scenarios"},"unlisted":false,"prevItem":{"title":"OceanBase Distributed Pushdown Technology","permalink":"/blog/distributed-push-down"},"nextItem":{"title":"How We Approach Improving Distributed Query Performance","permalink":"/blog/refine-performance"}},"content":"As large-scale promotions become the norm, it is a severe challenge for enterprises to ensure a smooth shopping experience for users during peak times, apart from annual shopping festivals such as \\"Double 11\\" and \\"618\\", by designing their application architecture and database architecture effectively to handle traffic surges. Based on its 10 years of experience in supporting \\"Double 11\\" as well as its features of online scaling, high concurrency, and low latency, OceanBase Database can quickly respond to business load changes without affecting the business, thereby improving system throughput during flash sales.\\n\\n[**Click to see more about scaling solutions for large-scale promotions>>**](https://www.oceanbase.com/solution/ecommerce)\\n\\n\x3c!-- truncate --\x3e\\n\\n## **Cases**\\n\\n[**Haidilao: stable support for holiday traffic peaks**](https://www.oceanbase.com/customer/haidilao)\\n\\n[**POP MART: handle traffic spikes of a hundredfold during flash sales**](https://www.oceanbase.com/customer/popmart)\\n\\n## **Key high-concurrency techniques of OceanBase Database**\\n\\nA database system must ensure both the correctness and high concurrency of the database. **The key to ensure database correctness in high-concurrency scenarios is to ensure the ACID properties of transactions.** Isolation (I) in ACID means that parallel execution of concurrent transactions produces the same effect as serial execution of those transactions. This kind of isolation is a serializable isolation. A serializable isolation is often accompanied by a large amount of conflict wait time or a large number of conflict failures. Therefore, the costs are relatively high.\\n\\nTo provide better performance of concurrent execution, the database has to relax the validation on schedules, allowing more non-serializable schedules to be run. The result of multiple concurrent transactions may no longer be equivalent to the result of any kind of serializable execution. The following promises must be made to regulate the database use: what kinds of errors will occur and what will not. These promises reflect the isolation levels of the database.\\n\\nBased on three phenomena that can lead to data errors in the concurrent execution of transactions, the SQL-92 standard defines a set of isolation levels. It defines four isolation levels based on whether they allow each of the phenomena. **A critique of ANSI SQL Isolation Levels**, a paper published in 1995, identified a number of problems with the definitions of isolation levels in the SQL-92 standard.\\n\\n1. The definition of phenomena in SQL-92 is too narrow. It is impossible to achieve serializable isolation even if the three phenomena are excluded.\\n2. Several new phenomena are added: dirty write, lost update, read skew, and write skew.\\n\\n### **Consistency of distributed transactions**\\n\\nFor concurrent transactions, distributed databases face more challenges than standalone databases.\\n\\n**The first challenge is read/write concurrency.**\\n\\nIn distributed database systems, a two-phase commit protocol or rollback compensation mechanism is usually used to ensure atomicity of distributed transactions. Regardless of which mechanism is used, the read/write concurrency problem exists. Take two-phase commit as an example, it involves two phases: prepare phase and commit phase. After all participants are prepared to commit, the coordinator will initiate a commit request to them. It is impossible to guarantee that all participants will commit at the same time in the protocol.\\n\\nSuppose T1 is a transaction that transfers $50 from account A to account B. T1 is in the process of committing. The modification to account A has been committed, and the modification to account B is being committed. What are the balance values of accounts A and B read by the concurrent transaction T2?\\n\\n![](https://intranetproxy.alipay.com/skylark/lark/0/2020/png/23425/1604221222816-a931bbcf-a0ef-4bca-8572-3ec63042a62b.png)\\n\\n**The other challenge is external consistency that distributed database systems often face.**\\n\\nSuppose a user places an order on Taobao and the payment is successful.\\n\\nThe order transaction T1 and the payment transaction T2 are located on different servers. T1 and T2 are committed separately. The version number of T1 is 1000, and that of T2 is 800. Assume that the system also has a read transaction T3, whose snapshot version number is 900. T3 can read the successful payment information but cannot read the order information. It violates the business semantics.\\n\\n![](https://intranetproxy.alipay.com/skylark/lark/0/2020/png/23425/1604221277781-781e9486-aa16-4a67-b0e7-f37a4e6a1172.png)\\n\\n### **Scheduling algorithms of concurrent transactions**\\n\\nWe have described the challenges brought by concurrent transactions. The behavior of concurrent transactions depends on the scheduling. General concurrent transaction scheduling methods are as follows:\\n\\n- The first method is two-phase locking. Two-phase locking is a pessimistic concurrency control method that guarantees serializable schedules of concurrent transactions. Different levels of isolation can be implemented by adjusting the strategy of reading and acquiring locks.\\n- The second method is about concurrency control in an optimistic way, which includes timestamp ordering concurrency control and optimistic concurrency control (OCC).\\n- The third method is the multi-version concurrency control mechanism that is widely used by popular databases.\\n\\nThis post focuses on two-phase locking and multi-version concurrency control.\\n\\n### **Two-phase locking**\\n\\nAs the name implies, the two-phase locking protocol involves two phases: the locking phase and the unlocking phase. In the locking phase, each transaction requests the locks that it needs from the lock manager but it cannot release any locks. Each transaction can request for a read or write lock. When the lock manager denies the lock requests, the transactions need to wait. When the transactions enter the unlocking phase, no more lock requests can be made. In database practices, a strict two-phase locking technique is employed. That means transactions can release locks only after they are committed.\\n\\n![](https://intranetproxy.alipay.com/skylark/lark/0/2020/png/23425/1604221384181-94e3c4f5-fa76-4eee-b9b9-75251890d6d8.png)\\n\\nTake a look at an example of a strict two-phase locking scheduling case. Assume T1 is a transaction that transfers USD 10 from account A to account B. During the transfer, a mutually exclusive lock is placed on accounts A and B respectively. Transaction T2 requires a read lock to read the balances of accounts A and B. However, due to the mutually exclusive locks, the read request from T2 has to wait. T2 can only release the exclusive locks after transaction T1 is committed. At last, T2 reads the values of accounts A (90) and B (110) and the total value satisfies the constraint that the sum of the two accounts is equal to 200.\\n\\n![](https://intranetproxy.alipay.com/skylark/lark/0/2020/png/23425/1604221459615-669f85f8-ae26-4d14-a862-179ce623eace.png)\\n\\nThe two-phase locking protocol is relatively simple to implement. But when a transaction finds a lock conflict, the transaction needs to wait, which may reduce the concurrent processing capacity of the database. In addition, multiple concurrent transactions are likely to cause deadlocks because they tend to contend for locks.\\n\\n### **Multi-version concurrency control**\\n\\nTo solve the problem of read/write conflict, many databases use the mechanism of multi-version concurrency control. The biggest benefit of this mechanism is that readers do not block writers and writers do not block readers, which greatly improves the concurrency capacity of the system.\\n\\nTake the implementation of multi-version concurrency control in MySQL InnoDB as an example. InnoDB data blocks record the latest version of data. Multiple old versions of data are recorded in undo logs. InnoDB adds two hidden fields to each row: transaction ID field, which records the identifier for the last transaction that modified the row, and a rollback pointer, which points to old versions of data. The initial content of the row can be traced back through the rollback pointer of the current record. Before a transaction modifies a row in InnoDB, it first locks the row with a mutually exclusive lock to prevent other transactions from modifying the row simultaneously. Then, it writes an undo log and a redo log. Finally, it updates the row data, changes the transaction ID of the row to its ID, and points the rollback pointer to the undo log it just wrote. This is a brief process of how a transaction modifies the data in InnoDB.\\n\\n![](https://intranetproxy.alipay.com/skylark/lark/0/2020/png/23425/1604221536100-26733a35-ce30-4043-a16c-b4db0d47ee1a.png)\\n\\nTake a look at the read process of a transaction. Before a transaction reads the data, it is assigned a read view, which represents the visible scope of the current transaction. The read view contains several pieces of information. First, the ID of this transaction. Second, the list of active transactions and the upper and lower limits of these transactions. After the read view is assigned, the snapshot of the transaction is established. Then, the transaction reads the row records and checks the row records based on the read view. If the current row is invisible, the rollback pointer is used to find the old versions of data. In the read process, the transaction first checks the current transaction ID of the row. If the ID is in the list of active transactions or larger than the maximum transaction ID of the read view, the row data is invisible to the transaction and the transaction needs to find old versions of data. Otherwise, the row data is visible.\\n\\n![](https://intranetproxy.alipay.com/skylark/lark/0/2020/png/180072/1604304172889-f4478653-ab85-47e0-976c-957f18321415.png?x-oss-process=image%2Fresize%2Cw_1500)\\n\\n### **Multi-version management**\\n\\nOceanBase Database implements multi-version concurrency control based on mutual exclusive locks. The storage engine of OceanBase Database uses an LSM-tree architecture to split data into static data and dynamic data. Dynamic data is stored in MemTables and periodically dumped to the disk. MemTables use B+Tree and dual hash index structures to keep data stored. The B+Tree structure is used for range query while hash indexes are used for single row query.\\n\\nThe leaf nodes of the B+Tree structure store the meta information of the row data. The meta information contains many fields. Only three fields are introduced here: primary key information, lock information, and linked-list pointer.\\n\\n- Lock information indicates whether a transaction holds a row lock. Before a transaction modifies data, it needs to acquire a row lock.\\n- A linked-list points to multiple versions of the data. Each version only keeps incremental information. For example, if only one field is modified at a time, only modifications to that field are recorded.\\n\\nAs you can see from the following figure, the row with a primary key equal to 1 has been modified three times. The first modification is an insert operation by a transaction with a version number of 1000. The second modification to the field b is made by a transaction with a version number of 1005. The third modification to the field b is made by a transaction with an infinite version number, indicating that the current transaction is not committed.\\n\\n![](https://intranetproxy.alipay.com/skylark/lark/0/2020/png/180072/1604307652308-90813d94-5b83-4545-9f6c-e76eb85d2456.png?x-oss-process=image%2Fresize%2Cw_1500)\\n\\n### **Mechanism and implementation of multi-version concurrency control**\\n\\nThe OceanBase distributed database system schedules transactions to ensure that concurrent transactions do not cause consistency issues. Assume that the system has three concurrent transactions, which are read/write transaction T1, read/only transaction T2, and read/write transaction T3. T1 does not commit and holds a lock on row 1.\\n\\nT3 needs to acquire the row lock of row 1 before it modifies row 1. The lock of row 1 is held by T1, T3 needs to wait until T1 is committed and the row lock is released. T2 is a read-only transaction with a snapshot version number of 1008. Before T2 reads data, it first finds the metadata of the row by the index structure. Then, it finds the committed data with a maximum version number smaller than the snapshot version number based on the snapshot version number. The following figure shows that T2 is able to read the committed data with version number 1005. The preceding is the internal read/write concurrency control mechanism of OceanBase Database. This mechanism solves the write/write conflicts using the mutually exclusive locks on the rows. A multi-version mechanism is used to ensure that readers do not block writers and writers do not block readers.\\n\\nThe multi-version concurrency control mechanism in OceanBase Database is very simple to implement. The snapshot version is a timestamp. The transaction visibility of row records can be determined by comparing the sizes of timestamps without the need to maintain active transactions. In some distributed database systems, a global transaction manager is maintained, which is used to determine transaction snapshots. If the system has multiple concurrent transactions, the global transaction manager can become the bottleneck of the cluster. OceanBase Database does not need to maintain a global transaction manager. Another point is that the lock information is stored on the row metadata in OceanBase Database, eliminating the need for an additional lock manager.\\n\\n![](https://intranetproxy.alipay.com/skylark/lark/0/2020/png/180072/1604307986350-c0c98d47-c314-46d0-bda5-9f067e0bc1d8.png?x-oss-process=image%2Fresize%2Cw_1500)\\n\\nBack to the read/write concurrency problem discussed in the beginning. When T1 is committing, the modification to account A is committed and the modification to account B is being committed. To solve the read/write concurrency problem, a two-phase locking schema is used in some distributed systems. That means T2 needs to acquire a read lock before it can read the balances of accounts A and B after T1 is committed.\\n\\nIn the OceanBase distributed database system, the read request will first find data of the corresponding version based on the snapshot version number. Assume that the version number of the committed transaction T1 is smaller than the snapshot version number of T2. T2 can read modifications made by T1. If T2 finds that row B is in the committing state, T2 needs to wait until row B is committed. This ensures that T2 can read data on both row A and row B. The time window that T2 needs to wait in this scenario starts from the prepare phase and ends at the commit phase. This is much shorter than the time spent to acquire a lock in the two-phase locking process.\\n\\n![](https://intranetproxy.alipay.com/skylark/lark/0/2020/png/23425/1604221770870-f198b243-22c0-479a-b2eb-2260da52abdc.png)\\n\\nAnother issue that troubles distributed database systems is external consistency.\\n\\nFor example, to purchase a phone on Taobao, you need to place an order first, and then pay for that order. Order placement and payment are two transactions. Assume that they are T1 and T2 respectively and are handled on two servers that generate independent commit versions. The commit version of T1 is 1000 and that of T2 is 800. When you start a query transaction T3, the version number of which is 900, T3 can read the payment information, but not the order information. This does not agree with the actual order of the transactions, and is therefore an external consistency problem.\\n\\n### **Global timestamp service (GTS)**\\n\\n**To address external consistency, OceanBase Database introduces the GTS, which assigns snapshot versions and commit versions based on a global timestamp.**\\n\\nIn the following figure, you can see that transactions T1 and T2 each requests a timestamp from GTS upon commit as its commit version. Transaction T3 also requests a timestamp as its snapshot version. GTS ensures that TS1 < TS2 < TS3. In other words, if T3 is able to read data modified by T2, it must also be able to read data modified by T1. External consistency is therefore ensured.\\n\\n![](https://intranetproxy.alipay.com/skylark/lark/0/2020/png/23425/1604221874136-d344c528-d27d-4ae0-90c8-807e06904eba.png)\\n\\n### **Early lock release (ELR)**\\n\\nA nuisance in single-server database systems, hotspot row updates present a bigger challenge for distributed databases. The longer the row locks are held, the more the performance of the update is affected.\\n\\nTransactions in a distributed database have longer delays than that in a single-server database. Therefore, row locks are held longer in a distributed database. This problem is more serious in a deployment model with five IDCs across three regions, where log synchronization may cause a delay of up to dozens of milliseconds. OceanBase addresses this problem using the ELR feature.\\n\\nIn a conventional database, row locks are added when a transaction is running and released after the transaction is persisted. In OceanBase Database, row locks are also added when a transaction is running. The difference is that locks can be released as soon as the system receives the request to commit the transaction, without waiting for the persistence of the transaction. In the following figure, you can see that transactions take place one after another without ELR. When ELR is enabled, the next transaction is able to acquire locks before the previous transaction is persisted. The duration in which the locks are held is significantly reduced.\\n\\n![](https://intranetproxy.alipay.com/skylark/lark/0/2020/png/23425/1604221996559-1b3cf217-088a-4046-8935-a2811d47e43b.png)\\n\\nWe measured the time required by each phase of running an OLTP update transaction in a test. It took 60 \u03bcs to generate the execution plan, 50 \u03bcs to perform the DML operation, 33 \u03bcs to write the clog, and an additional 170 \u03bcs to synchronize that clog. The transaction held its locks for about 270 \u03bcs during the execution process. When ELR is enabled, this duration is reduced by 65%, which means the performance of hotspot row updates can be tripled.\\n\\n![](https://intranetproxy.alipay.com/skylark/lark/0/2020/png/180072/1604312990338-3c7dfd2d-9224-43a1-ad1f-0b9afd93a30c.png?x-oss-process=image%2Fresize%2Cw_1500)\\n\\nWhen you were learning about the ELR feature, some of you may have wondered: What if a preceding transaction fails to persist? How should subsequent transactions be handled?\\n\\nWhen one transaction fails, all subsequent transactions must be rolled back. This is what is referred to as a cascading rollback and must be avoided, as pointed out by many papers related to database management. However, in actual application scenarios and tests, the possibility of a transaction failure and cascading rollback is very low.\\n\\nTo support cascading rollback, you must maintain the dependencies among transactions on each hotspot row. You can create a linked list that contain all transactions that modify the same row. This ensures that when a transaction fails, all subsequent transactions can be rolled back. For example, transactions T1, T2, and T3 modify data of the same hotspot row. Before T1 is persisted, it releases the row lock so that T2 may acquire the lock. T2 also releases the lock early for T3 to acquire lock. As a result, T3 depends on T2, and T2 depends on T1. A linked list is created based on these dependencies so that, if T1 is not persisted, both T2 and T3 are rolled back to ensure the data is correct."},{"id":"refine-performance","metadata":{"permalink":"/blog/refine-performance","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/tech-refine-performance.md","source":"@site/blog/tech-refine-performance.md","title":"How We Approach Improving Distributed Query Performance","description":"Wang Guoping | Senior Technical Expert of OceanBase","date":"2024-06-03T13:23:07.000Z","tags":[],"readingTime":24.495,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"refine-performance","title":"How We Approach Improving Distributed Query Performance"},"unlisted":false,"prevItem":{"title":"OceanBase Technical Insights for High-Concurrency Scenarios","permalink":"/blog/high-concurrency"},"nextItem":{"title":"From OLTP to OLAP: Exploration and practice of the OceanBase SQL engine","permalink":"/blog/tp-to-ap"}},"content":"> **Wang Guoping | Senior Technical Expert of OceanBase**\\n\\n> Wang is the technical director of the OceanBase Database SQL engine. He joined OceanBase in 2016 and is responsible for the R&D of the SQL engine. He graduated from Harbin Institute of Technology in 2008 and received his PhD from National University of Singapore in 2014. His main research direction during his PhD was multi-query optimization and processing in the database field. Before joining OceanBase, he was responsible for database R&D in Huawei.\\n\\nPerformance is one of the important metrics for measuring a database system and also a major concern in the database system field. OceanBase Database V3.x provides a relatively sound optimizer engine, standalone execution engine, parallel execution engine, and vectorized execution engine. In May 2021, OceanBase Database V3.x ran the TPC-H benchmark and ranked first in the 30,000 GB Results list. It achieved a result of 15.26 million QphH@30,000 GB, which showcases its core performance. OceanBase Database has proved its distributed query performance and linear scalability by running this benchmark.\\n\\n\x3c!-- truncate --\x3e\\n\\nDuring massive application of OceanBase Database V3.x, performance issues still occur in some business scenarios. For example, non-optimal execution plans are generated in specific distributed scenarios, the execution engine has no tolerance for non-optimal execution plans, and parallel execution threads cannot be fully used to speed up queries in specific scenarios. To address these issues, when we started to design OceanBase Database V4.0, we thought about how to optimize the SQL engine to improve the distributed query performance. **The distributed query optimization and distributed execution engine** fundamentally determine the distributed query performance of the SQL engine. Let\'s talk about our thoughts from these two aspects.\\n\\n**How does OceanBase Database V4.0 perform distributed query optimization?**\\n\\nAs we all know, query optimization is the focus and difficulty in database kernel development, and also the key point that determines the database query performance. Query optimization aims to select the optimal execution plan for each SQL statement. Generally, an SQL statement has many equivalent execution plans whose performance may vary by orders of magnitude. Therefore, query optimization fundamentally determines the query performance. OceanBase Database is a distributed relational database system, which means it inherently needs to perform distributed query optimization. In a relational database system, query optimization is always difficult in development. Distributed query optimization raises the level of the difficulty. Next, let\'s talk about the challenges in distributed query optimization compared with standalone query optimization.\\n\\n**\u258b Challenges in distributed query optimization**\\n\\n**Significantly expanded plan enumeration space**\\n\\nIn query optimization, the optimizer needs to select an implementation method for each operator in an execution plan. In a standalone scenario, the optimizer only needs to consider the implementation of the operator on a single server. In a distributed scenario, the optimizer also needs to consider the distributed implementation of the operator. For example, in a standalone scenario, the implementation methods for a join operator include hash join, merge join, and nested loop join. In a distributed scenario, the implementation methods include partition-wise join, partial partition-wise join, hash-hash distribution join, and broadcast distribution join. When these distributed implementation methods are combined with standalone implementation methods, the plan enumeration space for distributed query optimization will be significantly expanded, posing challenges for the optimization.\\n\\n**More physical attributes to be maintained**\\n\\nIn standalone query optimization, operator order is a very important physical attribute. The operator order may be used to speed up subsequent operators. The operator order determines whether tuples in the database are output based on a specific order after the operator is executed. For example, tuples are output in the order of (a,b,c) after the index (a,b,c) is scanned, because OceanBase Database preserves the order during index scan. The operator order is related to the implementation of specific operators. It may even affect the cost of subsequent operators. Therefore, after each operator is executed, query optimization will maintain the physical attribute \\"order\\", and execution plans with a useful order will be retained during plan pruning.\\n\\nIn distributed query optimization, another physical attribute is partition information. Partition information mainly includes the data partitioning method and the physical location of each partition. Partition information fundamentally determines the distributed algorithm selected for an operator. For example, whether a join can be implemented as a partition-wise join depends on the join key and the table partition information. As partition information can also affect the cost of subsequent operators, the physical attribute \\"partition information\\" also needs to be maintained during distributed query optimization. Partition information maintenance will finally affect plan pruning and selection and increase the complexity in distributed query optimization.\\n\\n**More accurate distributed cost model**\\n\\nIn query optimization, cost is the standard to evaluate an execution plan. Generally, cost represents the execution time of an execution plan or the amount of database system resources, such as CPU, I/O, and network resources, occupied by the execution plan. In a standalone scenario, the cost model needs to consider only the CPU and I/O costs. In a distributed scenario, apart from CPU and I/O costs, the cost model also needs to consider the network transmission cost, degree of parallelism (DOP) for queries, and cost in specific distributed optimization scenarios such as cost calculation for a Bloom filter. These factors increase the complexity in the design and fitting of a distributed cost model, as well as the complexity in distributed query optimization to some extent.\\n\\n**\u258b Two-phase distributed query optimization in OceanBase Database V3.x**\\n\\nTo decrease the complexity caused by distributed query optimization, OceanBase Database V3.x adopts two-phase distributed query optimization, which is a common solution in the industry.\\n\\nIn the first phase, based on the assumption that all tables are stored on the local server, the optimizer selects a local optimal execution plan by using the existing standalone query optimization capabilities.\\n\\nIn the second phase, based on the fixed join order and local algorithms, the optimizer selects a distributed algorithm for each operator by using a simple distributed cost model.\\n\\nThe following figure shows an example of two-phase distributed query optimization for query Q1. In the first phase, the optimizer selects a local optimal execution plan shown by the chart on the left. MJ represents merge join, HJ represents hash join, and HGBY represents hash group by, which are local algorithms. In the second phase, based on the fixed join order and local algorithms, the optimizer selects a distributed algorithm for each operator by using a simple distributed cost model. In this example, the partition-wise join algorithm is selected for the MJ node, and the hash-hash distribution join algorithm is selected for the HJ node.\\n\\n```SQL\\ncreate table R1(a int primary key, b int, c int, d int) partition by hash(a) partitions 4;\\ncreate table R2(a int primary key, b int, c int, d int) partition by hash(a) partitions 4;\\ncreate table R3(a int primary key, b int, c int, d int) partition by hash(b) partitions 5;\\nselect R2.c, sum(R3.d) from R1, R2, R3 where R1.a = R2.a and R2.C = R3.C group by R2.C;\\n```\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/195f8d40-74bd-4522-9c2b-531c29387a8a/image/2022-11-30/330988c9-f643-4c4b-af39-50584f9b99e0.png)\\n\\nTwo-phase distributed query optimization significantly decreases the optimization complexity. However, during massive commercial use of OceanBase Database V3.x, the optimization effects of two-phase distributed query optimization are sometimes not as expected due to the following reasons:\\n\\n**A non-optimal local algorithm is selected when partition information is ignored**\\n\\nDuring two-phase distributed query optimization, if partition information is ignored in the first phase, a non-optimal local algorithm will generally be selected. The following figure shows a query Q2 and its execution plan in the first phase. During local optimization in the first phase, if the selectivity of predicate `R1.c = 100` is low, a few rows in the R1 table meet this condition. In this case, the optimizer will select a nested loop join for this query. Specifically, for each row in the R1 table that meets the condition, data that meets the condition is quickly obtained from the R2 table based on the index `idx`. In actual execution however, the execution time of the nested loop join is much longer than that estimated by the optimizer. This is because R2 is a partitioned table with 100 partitions and the operation performed for each row in the R1 table must be performed in each partition in the R2 table during the nested loop join, which increases the execution time by 100 times. In this case, the optimal plan may be a hash join rather than a nested loop join. In this scenario, partition information is not considered during the optimization in the first phase. As a result, the standalone costs of operators are incorrectly estimated in the first phase, and a non-optimal local algorithm is selected for the query.\\n\\n```SQL\\ncreate table R1(a int primary key, b int, c int);\\ncreate table R2(a int primary key, b int, c int, index idx(b)) partition by hash(a) partitions 100;\\nQ2: select * from R1, R2 where R2.b = R1.b and R1.c = 100;\\n/* Execution plan for the first phase*/\\n| =============================================\\n|ID|OPERATOR        |NAME   |EST. ROWS|COST |\\n---------------------------------------------\\n|0 |NESTED-LOOP JOIN|       |970299   |85622|\\n|1 | TABLE SCAN     |r1     |990      |40790|\\n|2 | TABLE SCAN     |r2(idx)|1        |44   |\\n=============================================\\n\\nOutputs & filters:\\n-------------------------------------\\n    0 - output([r1.a], [r1.b], [r1.c], [r2.a], [r2.b], [r2.c]), filter(nil),\\n        conds(nil), nl_params_([r1.b])\\n    1 - output([r1.b], [r1.c], [r1.a]), filter([r1.c = 100]),\\n        access([r1.b], [r1.c], [r1.a]), partitions(p0)\\n    2 - output([r2.b], [r2.a], [r2.c]), filter(nil),\\n        access([r2.b], [r2.a], [r2.c]), partitions(p0)\\n```\\n\\n**A non-optimal join order is selected when partition information is ignored**\\n\\nDuring two-phase distributed query optimization, if partition information is ignored in the first phase, a non-optimal join order will generally be selected. The following figure shows a query Q3 and two groups of local plans and distributed plans generated for it. In the first group, the join order is ((R2, R3), R1). In the second group, the join order is ((R1, R2), R3). If partition information is not considered, the optimizer may select the ((R2, R3), R1) join order in the first phase. However, this join order may incur more network transmission costs in the second phase. As shown in the following figure, the tables R1, R2, and R3, as well as the join results of R2 and R3, all need to be transmitted over the network. ((R1,R2), R3) may be a better join order. This is because in the second phase, only R3 and the join results of R1 and R2 need to be transmitted. Since a partition-wise join can be performed on R1 and R2, the two tables do not need to be transmitted over the network. In business scenarios, it is common that an inappropriate join order is selected due to the ignorance of partition information.\\n\\n```SQL\\ncreate table R1(a int primary key, b int, c int, d int) partition by hash(a) partitions 4;create table R2(a int primary key, b int, c int, d int) partition by hash(a) partitions 4;create table R3(a int primary key, b int, c int, d int) partition by hash(b) partitions 5;Q3: select R2.c, sum(R3.d) from R1, R2, R3 where R1.a = R2.a and R2.b = R3.b;\\n```\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/3fd34d2d-e5c4-4a01-a393-3b0e3808221d/image/2022-11-30/2c02e366-d102-488a-b179-42ea1f8c3779.png)\\n\\nIn the foregoing two scenarios, a non-optimal join order and a non-optimal local algorithm are selected because partition information is not considered during optimization in the first phase. Through the two scenarios, we can see that the drawbacks of two-phase distributed query optimization are obvious. Next, let\'s talk about how OceanBase Database V4.0 performs distributed query optimization to resolve these issues.\\n\\n**\u258b Distributed query optimization in OceanBase Database V4.0**\\n\\nOceanBase Database V4.0 uses the one-phase optimization method for distributed queries. In this method, the optimizer enumerates both local and distributed algorithms in the same phase and estimates the costs by using a distributed cost model. OceanBase Database V4.0 restructures the entire distributed query optimization method from two-phase optimization to one-phase optimization.\\n\\nTo facilitate understanding of the one-phase distributed query optimization method, I first want to introduce the bottom-up dynamic programming method in System-R. Given an SQL statement, System-R uses the bottom-up dynamic programming method to enumerate joins and select a join algorithm. For a join that involves N tables, this method will enumerate execution plans for each subset by size. For each enumeration subset, the method will select an optimal plan as follows:\\n\\n- Enumerate all standalone join algorithms, maintain the physical attribute \\"order\\", and calculate the costs based on a standalone cost model.\\n- Retain the plan with the lowest cost and those with a useful order. The order in a plan is useful when and only when this order is useful for the allocation of subsequent operators.\\n\\nThe following figure shows an example of plan enumeration for a join that involves four tables. The method will first enumerate plans for all size 1 base tables. For each base table, the method will enumerate all indexes and retain plans with the lowest cost and a useful order. Then, the method will enumerate plans for each size 2 subset. For example, to enumerate all execution plans for the join of `{R1,R2}`, the method will consider all standalone join algorithms and combine them with all plans retained for R1 and R2. The method will continue enumeration until execution plans are enumerated for the size 4 subset.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/672dc990-54a7-4231-bdab-bb05ab03beff/image/2022-11-30/87ee8003-c9b5-46bb-83c2-52abd3a3eb8c.png)\\n\\nBased on the standalone query optimization method of System-R, OceanBase Database V4.0 implements distributed query optimization as follows:\\n\\n1\\\\. For each enumeration subset, OceanBase Database will enumerate the distributed algorithms of all operators, use a distributed cost model to calculate the cost of each distributed algorithm, and maintain two physical attributes: order and partition information.\\n\\n2\\\\. For each enumeration subset, OceanBase Database will retain the plan with the lowest cost, plans with a useful order, and plans with useful partition information. Partition information is useful when and only when it is useful for subsequent operators. In the scenario shown in the following figure, plan P1 uses a hash-hash distribution join, and plan P2 uses a broadcast distribution join for the R2 table. Though P2 has a higher cost than P1, P2 inherits the partition information of the R1 table, which will be useful for the subsequent group by operator. Therefore, P2 will also be retained.\\n\\n```SQL\\ncreate table R1(a int primary key, b int, c int, d int) partition by hash(a) partitions 4;\\ncreate table R2(a int primary key, b int, c int, d int) partition by hash(a) partitions 4;\\nselect R1.a, SUM(R2.c) from R1, R2 where R1.b = R2.b group by R1.a;\\n```\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/ceb3befd-0018-4a05-8f26-cc0bdb4ce341/image/2022-11-30/0a41f314-f6a5-41f1-913d-c6b230dd0f25.png)\\n\\nOceanBase Database V4.0 uses the one-phase distributed query optimization method, which involves a much larger plan space than standalone query optimization. Facing the issue of a large plan space, OceanBase Database V4.0 provides a variety of methods for quick plan pruning. It also provides new join enumeration algorithms to support distributed plan enumeration for ultra-large tables. Thanks to these techniques, OceanBase Database V4.0 effectively reduces the distributed plan space and improves the distributed query optimization performance. Our experimental results also show that OceanBase Database V4.0 can enumerate distributed plans for 50 tables within seconds.\\n\\n**How does OceanBase Database V4.0 improve the performance of the distributed execution engine?**\\n\\nCompared with OceanBase Database V3.x, OceanBase Database V4.0 has made many improvements in the execution engine. It has implemented new distributed and standalone algorithms, such as null-aware hash anti-join, shared broadcast hash join, hash-based window function, and partition bloom filter. It has also improved the implementation of the entire vectorized engine, developed ultimate parallel pushdown techniques, and initiated the development of adaptive techniques. These efforts have greatly improved the performance of both distributed and standalone queries. Here I want to introduce the adaptive techniques and parallel pushdown techniques of OceanBase Database V4.0.\\n\\n**\u258b Development towards an adaptive execution engine**\\n\\nIn business scenarios of OceanBase Database, we found that the execution engine has no tolerance for non-optimal execution plans generated by the optimizer. When the optimizer generates non-optimal execution plans, the execution engine cannot adjust the plans to improve the execution performance. Although the optimizer is designed to choose the optimal execution plans for database queries, the optimizer itself is not perfect. For example, it cannot accurately estimate the total number of rows. So, the optimizer may pick a less optimal execution plan, or even a lousy one.\\n\\nTo resolve this issue, OceanBase Database V4.0 starts to develop an adaptive execution engine. An adaptive execution engine identifies some non-optimal execution plans based on the real-time execution status and adjusts them accordingly to improve the execution performance. We believe that once an execution engine reaches a certain stage of development, it must use adaptive techniques to address the issue of non-optimal execution plans generated by the optimizer. However, we also do not believe that adaptive techniques can handle all scenarios of non-optimal plans.\\n\\nOceanBase Database V4.0 implements adaptive GROUP BY/DISTINCT parallel pushdown, which can prevent performance downgrade caused by non-optimal plans in GROUP BY/DISTINCT parallel pushdown scenarios. Before we dive into adaptive techniques, let me briefly introduce the GROUP BY/DISTINCT parallel pushdown technique. As a general technique in distributed execution, GROUP BY/DISTINCT parallel pushdown is often used to push down the GROUP BY operator in advance to pre-aggregate some data. This reduces the workload of network transmission, thus improving the performance. The following figure shows an example where the execution plan pushes down the GROUP BY operator to Operator 5 for data pre-aggregation, so that the network transmission workload of Operator 4 is reduced to achieve higher performance. However, note that GROUP BY parallel pushdown does not necessarily improve the performance. It sometimes backfires, mainly because it consumes extra computing resources. GROUP BY parallel pushdown brings benefits only when the performance improvement in network transmission surpasses the extra computing cost.\\n\\n```SQL\\ncreate table R1(a int primary key, b int, c int) partition by hash(a) partitions 4;\\nexplain select b, sum(c) from R1 group by b;\\n| ==========================================================\\n|ID|OPERATOR                     |NAME    |EST. ROWS|COST|\\n----------------------------------------------------------\\n|0 |PX COORDINATOR               |        |1        |10  |\\n|1 | EXCHANGE OUT DISTR          |:EX10001|1        |10  |\\n|2 |  HASH GROUP BY              |        |1        |9   |\\n|3 |   EXCHANGE IN DISTR         |        |1        |9   |\\n|4 |    EXCHANGE OUT DISTR (HASH)|:EX10000|1        |8   |\\n|5 |     HASH GROUP BY           |        |1        |8   |\\n|6 |      PX PARTITION ITERATOR  |        |1        |7   |\\n|7 |       TABLE SCAN            |r1      |1        |7   |\\n==========================================================\\n\\nOutputs & filters:\\n-------------------------------------\\n    0 - output([INTERNAL_FUNCTION(r1.b, T_FUN_SUM(T_FUN_SUM(r1.c)))]), filter(nil), rowset=256\\n    1 - output([INTERNAL_FUNCTION(r1.b, T_FUN_SUM(T_FUN_SUM(r1.c)))]), filter(nil), rowset=256, dop=1\\n    2 - output([r1.b], [T_FUN_SUM(T_FUN_SUM(r1.c))]), filter(nil), rowset=256,\\n        group([r1.b]), agg_func([T_FUN_SUM(T_FUN_SUM(r1.c))])\\n    3 - output([r1.b], [T_FUN_SUM(r1.c)]), filter(nil), rowset=256\\n    4 - (#keys=1, [r1.b]), output([r1.b], [T_FUN_SUM(r1.c)]), filter(nil), rowset=256, dop=1\\n    5 - output([r1.b], [T_FUN_SUM(r1.c)]), filter(nil), rowset=256,\\n        group([r1.b]), agg_func([T_FUN_SUM(r1.c)])\\n    6 - output([r1.b], [r1.c]), filter(nil), rowset=256\\n    7 - output([r1.b], [r1.c]), filter(nil), rowset=256,\\n        access([r1.b], [r1.c]), partitions(p[0-3])\\n```\\n\\nIn OceanBase Database of earlier versions, the optimizer determines whether to push down the GROUP BY operator based on cost estimation. However, the optimizer may sometimes incorrectly estimate the number of rows. As a result, the GROUP BY operator is not pushed down or is incorrectly pushed down, compromising the execution performance. To resolve this issue, OceanBase Database V4.0 introduces adaptive GROUP BY/DISTINCT parallel pushdown. The optimizer will always push down the GROUP BY/DISTINCT operator and determine whether to skip the pushed down GROUP BY/DISTINCT operator by sampling part of the data of the operator during execution. The challenge of this technique lies in how to make the pushed operator achieve satisfactory pre-aggregation performance. The OceanBase Database solution is to control the performance of the hash table of the pushed operator by limiting the table within the L3 cache and perform multiple rounds of sampling to prevent misjudgment due to continuous non-aggregation of data. The key points of the solution are described as follows:\\n\\n- The execution engine limits the hash table within L2 cache (1 MB) and, in the case of unsatisfactory pre-aggregation performance, marks the hash table as discarded. If the pre-aggregation performance is good, the execution engine expands the hash table to L3 cache (10 MB) and, if more memory is needed during the execution, marks the hash table as discarded.\\n- If the hash table is discarded, the execution engine returns and releases all rows of the table, and then rebuilds the hash table to start the next round of sampling.\\n- If pre-aggregation fails to achieve satisfactory performance in five consecutive rounds of sampling, the execution engine skips the pushed GROUP BY operator.\\n\\nCompared with the execution without operator pushdown, adaptive GROUP BY/DISTINCT parallel pushdown involves extra overhead for sampling and computing, which are required to determine whether to skip the pushed down operator during the execution. However, our tests based on various data distribution modes indicate that the extra overhead can be kept within 10%, which is much lower than the performance gain.\\n\\nWe are also working on more adaptive techniques, such as the adaptive creation and detection of Bloom filters, adaptive tuning of nested loop join and hash join, and adaptive tuning of broadcast distribution join and hash-hash distribution join. We believe that these adaptive techniques can elevate the capabilities of the execution engine to a new level, making the execution engine more robust. This way, when the optimizer generates a non-optimal or lousy execution plan, the execution engine can adjust the plan to improve the query performance.\\n\\n**\u258b Development towards ultimate parallel pushdown**\\n\\nParallel pushdown in the execution of distributed queries is a technique where the computing of some operators is pushed down to improve performance. Generally, this technique improves the performance of distributed queries by performing executions at the maximum DOP or reducing network transmission. It significantly improves the performance of distributed queries by orders of magnitude in many cases. The GROUP BY/DISTINCT parallel pushdown technique described in the previous section is a typical example of parallel pushdown techniques. Compared with OceanBase Database V3.x, OceanBase Database V4.0 provides well-developed parallel pushdown techniques, which work on almost all operators in AP scenarios, such as GROUP BY, ROLLUP, and DISTINCT, and window functions.\\n\\nThe following table compares OceanBase Database V3.x and OceanBase Database V4.0 in parallel pushdown.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/dbcb3231-5893-43b1-8456-7c969894bf31/image/2022-11-30/8ae0d7fe-c66c-491d-ae63-c70899cad4b3.png)\\n\\n**Pushdown scenario** **Example V3.x V4.0**GROUP BY, without a DISTINCT aggregate function select a, sum(d) from t group by a; Supported Supported GROUP BY, with a DISTINCT aggregate function select a, sum(distinct c),count(distinct d) from t group by a;Not supported Supported ROLLUP select a, sum(d) from t group by a rollup(b);Not supported Supported DISTINCT select distinct a from t; Supported Supported Window Function select a, b, sum(d) over (partition by c) from t; Not supported Supported\\n\\nIn OceanBase Database V4.0, the implementation of parallel pushdown varies based on operators. Due to the complexity in parallel execution, each implementation is confronted with different challenges. Here we won\'t introduce each implementation of parallel pushdown. Let\'s talk about the three-phase parallel pushdown technique for DISTINCT aggregate functions, to illustrate the advantages of parallel pushdown. The following figure shows a query Q1 that contains two DISTINCT aggregate functions. In OceanBase Database V3.x, parallel pushdown cannot be performed for Q1. The execution plan of Q1 shows that all deduplication logic and aggregate logic are calculated by Operator 0, which does not support parallel execution, leading to poor overall execution performance.\\n\\n```SQL\\ncreate table R1(a int, b int, c int, d int, primary key(a,b)) partition by hash(b) partitions 4;\\nQ1: select sum(distinct c), sum(distinct d) from R1 where a = 5;\\n| =====================================================\\n|ID|OPERATOR                |NAME    |EST. ROWS|COST|\\n-----------------------------------------------------\\n|0 |SCALAR GROUP BY         |        |1        |2365|\\n|1 | PX COORDINATOR         |        |3960     |2122|\\n|2 |  EXCHANGE OUT DISTR    |:EX10000|3960     |1532|\\n|3 |   PX PARTITION ITERATOR|        |3960     |1532|\\n|4 |    TABLE SCAN          |r1      |3960     |1532|\\n=====================================================\\n\\nOutputs & filters:\\n-------------------------------------\\n    0 - output([T_FUN_SUM(distinct r1.c)], [T_FUN_SUM(distinct r1.d)]), filter(nil),\\n        group(nil), agg_func([T_FUN_SUM(distinct r1.c)], [T_FUN_SUM(distinct r1.d)])\\n    1 - output([r1.c], [r1.d]), filter(nil)\\n    2 - output([r1.c], [r1.d]), filter(nil), dop=1\\n    3 - output([r1.c], [r1.d]), filter(nil)\\n    4 - output([r1.c], [r1.d]), filter(nil),\\n        access([r1.c], [r1.d]), partitions(p[0-3])\\n```\\n\\nTo improve the distributed execution performance of a query that uses a DISTINCT aggregate function, OceanBase Database V4.0 introduces the three-phase parallel pushdown logic. The following figure shows the three-phase parallel pushdown logic for a query that uses a DISTINCT aggregate function. The details are as follows:\\n\\n**In the first phase**, the DISTINCT logic is pushed down for partial deduplication. In this example, the job in this phase is completed by Operator 6.\\n\\n**In the second phase**, data is repartitioned based on the deduplicated column, and then full deduplication and partial pre-aggregation calculation are performed. In this example, the job in this phase is completed by Operators 3, 4, and 5.\\n\\n**In the third phase**, the results obtained in the second phase are aggregated. In this example, the job in this phase is completed by Operators 0, 1, and 2.\\n\\nCompared to the execution without operator pushdown, the three-phase parallel pushdown technique has two performance benefits. First, it allows data deduplication and pre-aggregation at the maximum DOP. Second, data deduplication by using the DISTINCT pushdown technique reduces the workload of network transmission.\\n\\n```SQL\\ncreate table R1(a int, b int, c int, d int, primary key(a,b)) partition by hash(b) partitions 4;\\nselect sum(distinct c) from R1 where a = 5;\\n| ===========================================================\\n|ID|OPERATOR                      |NAME    |EST. ROWS|COST|\\n-----------------------------------------------------------\\n|0 |SCALAR GROUP BY               |        |1        |1986|\\n|1 | PX COORDINATOR               |        |1        |1835|\\n|2 |  EXCHANGE OUT DISTR          |:EX10001|1        |1835|\\n|3 |   MERGE GROUP BY             |        |1        |1835|\\n|4 |    EXCHANGE IN DISTR         |        |1        |1683|\\n|5 |     EXCHANGE OUT DISTR (HASH)|:EX10000|1        |1683|\\n|6 |      HASH GROUP BY           |        |1        |1683|\\n|7 |       PX PARTITION ITERATOR  |        |3960     |1532|\\n|8 |        TABLE SCAN            |r1      |3960     |1532|\\n===========================================================\\n\\nOutputs & filters:\\n-------------------------------------\\n    0 - output([T_FUN_SUM(T_FUN_SUM(distinct r1.c))]), filter(nil),\\n        group(nil), agg_func([T_FUN_SUM(T_FUN_SUM(distinct r1.c))])\\n    1 - output([T_FUN_SUM(distinct r1.c)]), filter(nil)\\n    2 - output([T_FUN_SUM(distinct r1.c)]), filter(nil), dop=1\\n    3 - output([T_FUN_SUM(distinct r1.c)]), filter(nil),\\n        group(nil), agg_func([T_FUN_SUM(distinct r1.c)])\\n    4 - output([r1.c]), filter(nil)\\n    5 - (#keys=1, [r1.c]), output([r1.c]), filter(nil), dop=1\\n    6 - output([r1.c]), filter(nil),\\n        group([r1.c]), agg_func(nil)\\n    7 - output([r1.c]), filter(nil)\\n    8 - output([r1.c]), filter(nil),\\n        access([r1.c]), partitions(p[0-3]\\n```\\n\\nThe preceding example shows how the three-phase parallel pushdown technique works for a query with only one DISTINCT aggregate function. The question is, is it still effective for a query with multiple DISTINCT aggregate functions? The answer is yes. The trick is that in the first phase, we create a replica of the data set for each DISTINCT aggregate function and tag the replica to indicate its association with this aggregate function. Similar operations are performed in the second and third phases, except for some minor differences in terms of implementation. The following figure shows the three-phase pushdown logic for a query that uses two DISTINCT aggregate functions. AGGR_CODE is used to mark the redundant data generated after each DISTINCT aggregate function is calculated.\\n\\n```SQL\\ncreate table R1(a int, b int, c int, d int, primary key(a,b)) partition by hash(b) partitions 4;select sum(distinct c), sum(distinct d) from R1 where a = 5;| ===========================================================|ID|OPERATOR                      |NAME    |EST. ROWS|COST|-----------------------------------------------------------|0 |SCALAR GROUP BY               |        |1        |13  ||1 | PX COORDINATOR               |        |2        |13  ||2 |  EXCHANGE OUT DISTR          |:EX10001|2        |12  ||3 |   HASH GROUP BY              |        |2        |11  ||4 |    EXCHANGE IN DISTR         |        |2        |10  ||5 |     EXCHANGE OUT DISTR (HASH)|:EX10000|2        |9   ||6 |      HASH GROUP BY           |        |2        |8   ||7 |       PX PARTITION ITERATOR  |        |1        |7   ||8 |        TABLE SCAN            |r1      |1        |7   |===========================================================Outputs & filters:-------------------------------------  0 - output([T_FUN_SUM(T_FUN_SUM(dup(r1.c)))], [T_FUN_SUM(T_FUN_SUM(dup(r1.d)))]), filter(nil), rowset=256,      group(nil), agg_func([T_FUN_SUM(T_FUN_SUM(dup(r1.c)))], [T_FUN_SUM(T_FUN_SUM(dup(r1.d)))])  1 - output([AGGR_CODE], [T_FUN_SUM(dup(r1.c))], [T_FUN_SUM(dup(r1.d))]), filter(nil), rowset=256  2 - output([AGGR_CODE], [T_FUN_SUM(dup(r1.c))], [T_FUN_SUM(dup(r1.d))]), filter(nil), rowset=256, dop=1  3 - output([AGGR_CODE], [T_FUN_SUM(dup(r1.c))], [T_FUN_SUM(dup(r1.d))]), filter(nil), rowset=256,      group([AGGR_CODE]), agg_func([T_FUN_SUM(dup(r1.c))], [T_FUN_SUM(dup(r1.d))])  4 - output([AGGR_CODE], [dup(r1.c)], [dup(r1.d)]), filter(nil), rowset=256  5 - (#keys=3, [AGGR_CODE], [dup(r1.c)], [dup(r1.d)]), output([AGGR_CODE], [dup(r1.c)], [dup(r1.d)]), filter(nil), rowset=256, dop=1  6 - output([AGGR_CODE], [dup(r1.c)], [dup(r1.d)]), filter(nil), rowset=256,      group([AGGR_CODE], [dup(r1.c)], [dup(r1.d)]), agg_func(nil)  7 - output([r1.c], [r1.d]), filter(nil), rowset=256  8 - output([r1.c], [r1.d]), filter(nil), rowset=256,      access([r1.c], [r1.d]), partitions(p[0-3])\\n```\\n\\nParallel pushdown is common in distributed scenarios. In OceanBase Database V3.x, the distributed query performance often deteriorates due to the imperfection of the parallel pushdown feature. OceanBase Database V4.0 can resolve such issues to improve the distributed query performance.\\n\\n**Final words**\\n\\nIn the end, I want to share with you the actual improvements made by OceanBase Database V4.0 in distributed query performance. Compared with OceanBase Database V3.x, OceanBase Database V4.0 implements a new distributed cost model, a distributed query optimization framework, a set of well-developed parallel pushdown techniques, and adaptive techniques. The development of these techniques is driven by our understanding of customer requirements and distributed systems.\\n\\nWe tested the techniques by running the TPC-DS benchmark with a scale factor of 100 GB. The test results show that the new techniques significantly improve the distributed query performance. The total execution duration of 99 queries decreases from 918s to 270s. The following figure compares the performance of queries in OceanBase Database V3.x and OceanBase Database V4.0 in the TPC-DS benchmark with a scale factor of 100 GB.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/733ccee4-16e2-4058-8444-9ef9003df85c/image/2022-11-30/bb5d7618-9c57-45d6-9547-3fea3c97e651.png)\\n\\nPerformance comparison for the TPC-DS benchmark with a scale factor of 100 GB (OceanBase Database V3.x and V4.0)\\n\\nThese are our thoughts on the value and technical evolution of distributed query optimization of OceanBase Database V4.0. Databases are foundational software in essence. For software users, we hope that later OceanBase Database V4.x versions can bring better user experience and higher query performance based on distributed query optimization and technical innovations in the execution engine.\\n\\nWelcome to follow [OceanBase Community](https://open.oceanbase.com/blog), where we will keep providing valuable technical content and grow together with millions of techies.\\n\\nSearch for \ud83d\udd0d DingTalk group 33254054 or scan the following QR code to join the OceanBase technical consultation group, where you can get a solution to any technical issue.\\n\\n![](https://gw.alipayobjects.com/zos/oceanbase/f4d95b17-3494-4004-8295-09ab4e649b68/image/2022-08-29/00ff7894-c260-446d-939d-f98aa6648760.png)"},{"id":"tp-to-ap","metadata":{"permalink":"/blog/tp-to-ap","editUrl":"https://github.com/oceanbase/oceanbase.github.io/tree/main/blog/blog/tech-tp-to-ap.md","source":"@site/blog/tech-tp-to-ap.md","title":"From OLTP to OLAP: Exploration and practice of the OceanBase SQL engine","description":"On March 25, 2023, we held the first \\"OceanBase Developer Conference\\" to explore cutting-edge database trends such as the standalone distributed architecture, cloud native architecture, and hybrid transaction/analytical processing (HTAP), share new product roadmaps, and communicate about scenario exploration and best practices with developers. In the \\"Product Technology Session\\", Senior Technical Expert Zhu Tao from OceanBase gave a presentation on the topic \\"From TP to AP: Exploration and Practices of the OceanBase Database SQL Engine\\". This blog post is compiled based on the presentation.","date":"2024-06-03T13:23:07.000Z","tags":[],"readingTime":20.995,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"tp-to-ap","title":"From OLTP to OLAP: Exploration and practice of the OceanBase SQL engine"},"unlisted":false,"prevItem":{"title":"How We Approach Improving Distributed Query Performance","permalink":"/blog/refine-performance"}},"content":"> On March 25, 2023, we held the first **\\"OceanBase Developer Conference\\"** to explore cutting-edge database trends such as the standalone distributed architecture, cloud native architecture, and hybrid transaction/analytical processing (HTAP), share new product roadmaps, and communicate about scenario exploration and best practices with developers. In the **\\"Product Technology Session\\"**, Senior Technical Expert Zhu Tao from OceanBase gave a presentation on the topic **\\"From TP to AP: Exploration and Practices of the OceanBase Database SQL Engine\\"**. This blog post is compiled based on the presentation.\\n\\nFirst, what is the OceanBase Database SQL engine and what does it do? Second, OceanBase Database was first designed to serve internal businesses of Ant Group. What have we done to make SQL execution faster in Internet TP scenarios for the database to better serve such scenarios?\\n\\n![1684722861](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1684722859762.png)\\n\\n\x3c!-- truncate --\x3e\\n\\nAfter that, OceanBase Database was commercialized to gradually serve some external customers in fields such as finance, securities, telecom operators, and insurance. At that time, we noticed that external customers used SQL queries in a quite different way from that inside Ant Group. This posed a new challenge in SQL execution and optimization in our product. Confronting new business scenarios and SQL scenarios, what have we done?\\n\\nFinally, as the TP capabilities of OceanBase Database were constantly enhanced and optimized and its execution capabilities were accumulated, the database became AP-capable. What efforts and optimizations have we made for AP scenarios?\\n\\n## **01\\\\. Overview of the OceanBase Database SQL engine**\\n\\nThe following figure shows the overall framework of the OceanBase Database SQL engine. After a query is initiated in OceanBase Database, it first enters into the parser, a module where the kernel figures out the purpose and requirement of the query.\\n\\nThen, the query enters into the optimizer, which will select the best among a variety of implementation methods for the executor.\\n\\nThe executor contains all details for executing the method. It will faithfully execute the query based on the plan recommended by the optimizer and return the result to the user.\\n\\n![1684722877](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1684722875796.png)\\n\\nHere is an example for you to understand the process:\\n\\nAssume that my requirement is to go on a business trip from Shanghai to Beijing to attend OceanBase Developer Conference the next day. What will the optimizer do?\\n\\nThere are two choices to travel from Shanghai to Beijing, by high-speed rail or airplane. After arriving at Beijing, I can take subway or taxi from the airport hub to the hotel. Similarly, I can take subway or taxi from my home to the airport hub in Shanghai. A number of execution plans will be generated for my requirement. The optimizer will evaluate the execution plans to find out the optimal one based on a cost estimation system.\\n\\nAs for my business trip, time consumption is the main factor to consider. Specifically, when I leave for the airport hub in Shanghai, taxi is a more comfortable choice but the time taken is uncontrollable due to the traffic jam during busy hours. Therefore, the optimizer chooses subway for this part. Generally, flying to Beijing is faster than taking high-speed rail, so the optimizer chooses the former. I may arrive at Beijing late at night. Theoretically, I can take subway from the airport hub to the hotel, but I may have missed the last train by then. Therefore, the optimizer chooses taxi for this part. For the entire trip, the optimizer generates an optimal plan as follows: subway from home to the airport hub in Shanghai, airplane from Shanghai to Beijing, and taxi from the airport hub in Beijing to the hotel.\\n\\nTwo key capabilities of the optimizer are required during this process:\\n\\nFirst, the capability in plan enumeration. The optimizer needs to enumerate a variety of different plans, which are the \\"plan space\\" for optimization. The more plans, the higher probability to find a quick execution strategy.\\n\\nSecond, the capability in accurately selecting the optimal plan. The optimizer needs to single out the optimal plan based on an accurate cost model.\\n\\nWhat is also important is that the executor will faithfully execute the selected execution plan. In this example, the executor has to consider all the details, including buying subway tickets, waiting for the subway, taking the subway, checking in for the flight, taking the flight, and landing at the destination. What are the key points in execution?\\n\\nFirst, good implementation. Each part of the trip must be efficient. As the time taken in each part is shortened, the entire trip will be faster.\\n\\nSecond, design and production of various transport tools. For example, subways, airplanes, or more efficient and convenient transport tools need to be designed and produced for the executor to choose, which enriches the execution modes.\\n\\nSo, what are our challenges in actual business scenarios?\\n\\n## **02\\\\. Speed up TP SQL queries in Internet scenarios**\\n\\nFirst, I want to share with you an Internet scenario (a TP scenario inside Ant Group) to illustrate the challenges facing us.\\n\\nI have been serving OceanBase and observing the business model of Ant Group for years. I notice that most Internet SQL queries on databases are single-table short queries. For a single-table short query, the only key point is to accurately select an index.\\n\\nFor example, for a short query that needs to read 100 rows, an appropriate index can obtain the query result by reading over 100 rows. However, an inappropriate index may need to read 10,000 rows to obtain the query result, which is unacceptable. Assume that the system can read a maximum of 2 million rows per second. If the system can obtain the query result by reading 100 rows, the theoretical queries per second (QPS) is 2,000,000/100. If the system needs to read 10,000 rows to obtain the query result, the theoretical QPS is 2,000,000/10,000, which is much lower. The optimization for single-table short queries seems simple but is actually difficult.\\n\\n![1684722894](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1684722892590.png)\\n\\nThe preceding figure shows two cases. The first case is an actual scenario inside Ant Group. The table in this case contains more than 30 indexes. In this case, when I query data from this table, there may be more than 30 execution plans. It is difficult to find an appropriate index.\\n\\nThe second case shows two single-table queries whose predicate or SQL syntax is more complex. The first query does not use an AND condition but an OR predicate on two columns. Both of the columns may have an index. However, if no optimization is made for this query, no index can be used.\\n\\nThe second query is more complex. After filtering, the filter conditions and filtering performance may be poor and a large number of rows may be retrieved. However, to ensure that a large amount of data is retrieved, the data will be paginated and sorted by the `gmt_create` field, and the first 20 rows will be displayed. To optimize this query, the best choice is to push down the WHERE, ORDER BY, and LIMIT operators to an index, to scan 20 rows from a specific row and then return the result.\\n\\nWhat can we do for the two scenarios from the O&M perspective? You can bind each SQL query with an index. If the optimizer cannot select an appropriate index, select it manually.\\n\\nFor the second case, it is not easy to use hints. To accelerate the execution of these queries, you may need to split the first query into two queries combined by the UNION operator and execute the two queries by using different indexes. For the second query, you also need to split the STATUS IN condition into two queries, obtain the first 20 rows sorted by the `gmt_create` field respectively, merge the two result sets, and return the first 20 rows in the final result set. SQL query rewrite is not easy, especially from the O&M perspective. Database personnel will receive complaints if they cannot handle the issue and kick it to business personnel.\\n\\nIn OceanBase Database, an appropriate index is automatically selected in most scenarios. For a single query, many indexes are totally irrelevant. For the query in the first case, OceanBase Database will prune irrelevant indexes to decrease the number of indexes from 30 to 5, and then find an appropriate index based on cost estimation.\\n\\nFor queries in the second case, the kernel optimizer of OceanBase Database provides a rewrite module to accurately translate the queries. It splits the OR condition in the first query and ORDER BY ... LIMIT in the second query into conditions in queries combined by the UNION operator.\\n\\nHowever, the optimizer may still make a wrong choice in index selection. The optimizer selects an index based on statistics or historical data. If the data is not real, the selected index may be inappropriate. To address this issue, OceanBase Database provides an automatic eviction strategy. If an execution plan of an SQL query has been used multiple times but the execution performance is not as expected, OceanBase Database will evict this execution plan and generate a new one.\\n\\nInternet SQL queries are relatively simple and can be optimized at the business layer. This is not the focus of my presentation today. Next, let\'s see what SQL queries are like when OceanBase Database serves external customers such as insurance and telecom operator customers.\\n\\n## **03\\\\. Speed up TP SQL queries in conventional business systems**\\n\\n### **Multi-table (outer) joins: the join order is essential**\\n\\nThe first scenario I want to share is a join scenario. In Internet scenarios, SQL queries are usually manually restricted to single-table queries. In most industrial business systems, SQL queries are not restricted. In such a business system, an SQL query may join a large number of tables to get a small result set. I have encountered a complex scenario where an SQL query joins 17 tables. To improve the performance of such a multi-table outer join, selecting an appropriate index is far from enough. An appropriate join order is also a must.\\n\\n![1684722911](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1684722909399.png)\\n\\nWhat is an appropriate join order?\\n\\nIn the example shown in the preceding figure, four tables T1, T2, T3, and T4 are joined. Among them, T2 is a large table and others are small tables. According to the join order specified in the query, T1 is joined with T2 first and then with T3 and T4. Three of the joins are performed between a small table and a large table, resulting in a high overhead. To simplify the join query, allow T1 to join with the small tables first and then with the large table T2 to achieve higher performance.\\n\\nFrom the O&M side, the SQL query needs to be rewritten to manually adjust the write sequence of the outer join. It is difficult to rewrite the query from the business side because you have to determine which are large tables, which joins need to be moved backward, and which joins need to be moved forward. The whole process is complex, let alone that 17 tables are involved.\\n\\nCan hints be used to enable the optimizer to generate such an execution plan with a more appropriate join order? It is uncertain. Many database systems support changing the order of inner joins but do not support such change for outer joins. This is because the behavior of outer joins is more complex, making them unqualified for order change in many scenarios. The optimizers of many databases are incapable of generating an execution plan shown on the right side of the preceding figure, even though hints are used to instruct the optimizers to do so.\\n\\nOceanBase Database provides a complete set of join enumeration algorithms for adjusting the join order of inner joins and outer joins. It can automatically adjust the join order of outer joins, anti-joins, and semi-joins, or convert an outer join into an inner join or anti-join.\\n\\n### **Ubiquitous subqueries**\\n\\nThe second scenario is of subqueries, which are interesting. Business personnel like to write subqueries because their semantics are intuitive.\\n\\n![1684722925](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1684722923374.png)\\n\\nIn external business systems, the usage mode of subqueries is more complex, for example, subqueries appear in unusual positions. Subqueries commonly appear in a FROM or WHERE clause. However, in Case 1 in the preceding figure, a subquery is used as the right operand in an assignment. In other words, the value of the left operand equals the result of the subquery. In Case 2, a subquery is used in a WHERE clause. It is not directly used as the root predicate but nested in an OR condition. If it is a root predicate, the EXISTS subquery will be converted into a semi-join for optimization in many databases. Since it is nested in an OR condition in this case, it cannot be optimized. Case 3 is more complex. Many subqueries are used in projected items. It is simplified but still has six subqueries.\\n\\nWhat can the business side do to address the issue of slow queries?\\n\\nThe answer is also to rewrite the queries. Generally, the semantics of a subquery are similar to those of a join, so a subquery can be rewritten into a join. However, it is difficult for business personnel to correctly rewrite a subquery into a join because the two still have slight differences in semantics. An error may easily occur.\\n\\nThe rewrite module of OceanBase Database provides a wide range of subquery optimization strategies. It can rewrite any subquery that is not nested in a deep structure into a join, which can then be optimized by using different join enumeration algorithms.\\n\\nAnother quite interesting point of Case 3 is that the S1, S2, and S3 subqueries are similar in structure. They read the same table by using the same WHERE condition. The only difference is the function used for calculating the projected item. OceanBase Database can rewrite these subqueries into a join to combine the three subqueries as one to reduce the calculations by 2/3, thereby achieving high performance.\\n\\n## **04\\\\. Speed up AP SQL queries**\\n\\nAs OceanBase Database is constantly optimized to accommodate different TP scenarios, we have accumulated a lot of experience in optimizing and executing TP queries. As for the kernel, AP and TP have no substantial differences. For example, multi-table joins in TP scenarios also appear in AP scenarios, and join enumeration for optimizing multi-table joins in TP scenarios is also applicable to AP scenarios. Based on our accumulated experience, we can now probe and practice the optimization in AP scenarios.\\n\\nHere are the topics we are going to talk about next:\\n\\n- What are SQL queries like in AP scenarios?\\n- What are the challenges?\\n- What targeted optimizations have we made for AP scenarios?\\n\\n### **Data skew and load skew: advocate collaboration and avoid slacking off**\\n\\nI want to share with you a typical AP scenario.\\n\\nWhat are the characteristics of an AP SQL query? An AP SQL query usually involves a large amount of data to read and massive calculations. The most intuitive solution is to use multiple threads for parallel execution. If one thread needs 1 hour to execute the query, can 10 threads take only 6 minutes to execute the query? If each thread does their own job, multi-thread parallel execution can indeed achieve linear efficiency. However, this is impossible in practice. In the case of data skew or load skew, if only one thread is fully devoted and the other nine threads slack off, it will still take 1 hour to execute the query.\\n\\n![1684722942](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1684722941017.png)\\n\\nHere is an actual case of data skew in a telecom operator. This is a large table with dozens of partitions. A large account that owns 90% of the total data is located in a single partition. Here is the result set, in which a row contains more than 23 million records. To measure the number of distinct values in a specific field of each partition, I can use multiple threads, with each thread calculating the results in specific partitions, and then summarize the results. For partitions with small amounts of data, the threads can easily complete the calculations and report the results. The thread responsible for the partition with a large amount of data needs to take a long time in calculations. Other threads have to wait for this thread to complete calculations. In such a scenario, parallel execution cannot gain expected benefits because some threads slack off due to unbalanced load.\\n\\nTo resolve this issue from the O&M perspective, we can adjust the partitioning mode and rewrite the query, which is theoretically feasible. However, the specific implementation method varies based on scenarios, making this strategy infeasible. To effectively improve the query performance, we need to optimize the parallel execution algorithm in the kernel for data skew and load skew scenarios, so as to evenly distribute fine-grained calculations to each thread to leverage the capabilities of each thread.\\n\\nOceanBase Database is also optimized for scenarios such as table scan, joins, window function-based calculations, as well as data skew and load skew after parallel execution of data add, delete, and modify operations. After that, more threads can be added for parallel execution to accelerate such calculation tasks.\\n\\n### **Large table aggregation: a smart execution engine**\\n\\nThe following figure shows a large table aggregation scenario, which is common in business. For example, we usually need to scan a large table and aggregate the data to report to the boss on today\'s performance.\\n\\n![1684722951](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1684722949942.png)\\n\\nGenerally, in a large table aggregation scenario, the table contains a huge amount of data. A common optimization method is pre-aggregation. Specifically, the system splits the data in the large table into multiple pieces and arranges different threads to group and aggregate the data to obtain partial aggregation results. Then, the threads exchange their data and hand over data in the same group to a specific thread. The threads summarize the data to obtain and report the final results. This is a typical optimization method.\\n\\nWhat are the benefits? Assume that a large table contains 1 billion rows and that the data can be divided into 100 groups. Theoretically, the initial grouping and aggregation results of each thread will not exceed 100 rows, and the cost of summarizing the 100 groups will not be high. Therefore, this optimization method is commonly used in many database systems. However, this optimization method still has a drawback. If most data is not duplicate, grouping is not effective. For example, 1 billion rows may only change to 0.9 billion rows after grouping. In this scenario, if grouping is performed for optimization, the performance may be compromised.\\n\\nWhat can we do?\\n\\nOn the O&M side, we can choose whether to instruct the optimizer to perform pushdown.\\n\\nOn the kernel side, the optimizer automatically determines whether to perform optimization based on statistics. This practice is common but not very effective. In a typical data skew scenario, pushdown may be required by large accounts but not by small accounts. But the preceding strategy is global, that is, the optimizer will perform pushdown for all or none of the accounts.\\n\\nTo resolve this issue, OceanBase Database does not allow the optimizer to make a decision on pushdown, but makes the executor smarter. OceanBase Database allows the executor to determine whether to perform pre-aggregation based on the actual situation during calculations in all scenarios such as grouping, deduplication, and window function-based calculations.\\n\\nThe next scenario is about batch statement execution.\\n\\n### **Batch statement execution in an AP task: reads and writes in parallel**\\n\\nIn most scenarios, an AP task contains more than one SQL statement. The first statement operates data and generates a result set, which is a temporary table, for the second statement. The second statement calculates and analyzes the data in the temporary result set and generates another temporary result set for the third statement. The third statement reads data from the second temporary result set, aggregates the data for analysis, and writes the analysis results to the final result set.\\n\\n![1684722979](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1684722977912.png)\\n\\nThe preceding figure shows a simplified case that involves three queries: Q1, Q2, and Q3. The first challenge is that a huge amount of data is operated during the entire process. Large amounts of data is read, calculated, and written.\\n\\nThe second challenge is about statistics. The tables that Q1 depends on have statistics. Therefore, an appropriate execution plan can be selected for this query based on statistics. Q2 and Q3 need to use the temporary table generated by its previous query. A temporary table has no statistics. As a result, an inappropriate execution plan with low performance will be probably generated for Q2 and Q3 due to the absence of statistics on temporary tables TMP2 and TMP3.\\n\\nTo cope with the first challenge, OceanBase Database supports parallel write since V3.x. Multiple threads can write data to the database in parallel.\\n\\nFor the second challenge, a solution from the O&M perspective is to add statements for statistics collection in a batch execution task. However, this is unimaginable for a large table. Statistics collection is time-consuming and may increase the total execution time of the task. OceanBase Database V4.x supports online statistics collection. In this case, statistics collection starts as early as when Q1 inserts data into TMP1. After data is successfully inserted, statistics on TMP1 have been collected. Then, Q2 can be optimized based on the statistics. Similarly, statistics on TMP2 starts when Q2 inserts data into TMP2.\\n\\nThe last scenario is not an actual business scenario but an insight into the complexity of optimization from the kernel perspective. Therefore, it will be a little abstract.\\n\\n### **Complexity in distributed and parallel queries**\\n\\nIn some AP tasks, tables are usually large partitioned tables on multiple servers, and parallel execution is enabled, making the queries distributed and parallel ones. In this case, the optimization is not about optimizing serial plans in a standalone database but parallel plans in a distributed scenario. The difference is brought by data partitioning. Partitioning means an explosive increase in the number of execution plans.\\n\\nFor example, 10 execution plans are generated in a serial scenario. However in a distributed scenario, 100 execution plans may be generated considering parallel execution. It is difficult to select the best one among 100 plans. In many databases or conventional research, a compromise strategy will be used for optimization in such a scenario. Specifically, the system will generate the optimal serial plan based on the assumption that all tables are non-partitioned tables located on the same server, and then convert the serial plan into a distributed and parallel plan.\\n\\nIf we do so, what will happen?\\n\\n![1684722989](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1684722987752.png)\\n\\nLet\'s see an actual case in a banking business scenario. In a two-table join, the referenced tables are both T1 and a subquery is used for grouping and aggregation for T1. If T1 is a non-partitioned table, the query performance is stable, with a response time (RT) of 2s. If T1 is a partitioned table, slow queries with an RT of 15s will occasionally occur. Why is that?\\n\\nAnalytics show that in a serial scenario (the first chart on the right), the two optimal execution plans respectively perform a merge join or nested loop join on the two tables. The performance of the two plans is very close, with an RT of about 2s. The performance of the merge join is slightly higher. After T1 is partitioned, a partition-level iteration will be incorporated into execution plans. Then, the RT of the merge join is still about 2s. For the nested loop join, the number of executions of the right branch is subject to the number of rows in the left-side table. Since the left-side table in the join contains 20,000 rows, data in the right-side table needs to be calculated for 20,000 times. This means the partition-level iteration is amplified by 20,000 times, increasing the RT to 15s.\\n\\nWe find that conventional optimization is not suitable for distributed scenarios. OceanBase Database V4.x provides a new distributed plan enumeration mechanism. The optimizer will not select the optimal serial plan, but generate all possible distributed execution plans and select the best one for the executor.\\n\\n## **05\\\\. Summary**\\n\\nThe development of the SQL engine is accompanied by business development. The optimization and execution capabilities of the SQL engine evolve as the SQL mode of business systems changes.\\n\\n![1684722999](https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1684722998039.png)\\n\\nOceanBase Database V1.x and V2.x are mainly intended for Internet TP scenarios. In such scenarios, the optimizer focuses on index selection or simple join enumeration, and the executor focuses on executing queries instead of speeding up queries.\\n\\nOceanBase Database V2.x and V3.x start to serve more conventional TP scenarios, where the optimizer needs to have strong join enumeration and query rewrite capabilities, and the executor needs to provide higher performance and requires new execution strategies.\\n\\nIn OceanBase Database V3.x and V4.x, the optimizer needs to enhance the optimization capabilities in AP scenarios while selecting a more appropriate execution plan to ensure stable performance in TP scenarios. To do so, OceanBase Database provides features, such as grayscale evolution of execution plans and more powerful hints, to enable business personnel to better control execution plans generated by the optimizer. For the execution engine, OceanBase Database provides features such as vectorized optimization, hardware feature mining, parallel execution pushdown, and adaptation technologies."}]}}')}}]);