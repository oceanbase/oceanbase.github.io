"use strict";(self.webpackChunkmy_docs_website=self.webpackChunkmy_docs_website||[]).push([[4169],{81720:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var s=a(74848),t=a(28453);const i={title:"Use other tools for data migration and synchronization",weight:8},r="4.7 Use other tools for data migration and synchronization",o={id:"user_manual/quick_starts_and_hands_on_practices_in_english/chapter_04_migration_and_synchronization_oceanbase/migration_and_synchronization_through_other_tools",title:"Use other tools for data migration and synchronization",description:"OceanBase Database also supports many third-party data import and export tools, such as Flink CDC, DataX, and Canal. These open source tools provide slightly different features, and therefore are applicable to different scenarios. This topic describes how to use these tools for data migration and synchronization between OceanBase Database and other databases.",source:"@site/docs/user_manual/quick_starts_and_hands_on_practices_in_english/chapter_04_migration_and_synchronization_oceanbase/07_migration_and_synchronization_through_other_tools.md",sourceDirName:"user_manual/quick_starts_and_hands_on_practices_in_english/chapter_04_migration_and_synchronization_oceanbase",slug:"/user_manual/quick_starts_and_hands_on_practices_in_english/chapter_04_migration_and_synchronization_oceanbase/migration_and_synchronization_through_other_tools",permalink:"/docs/user_manual/quick_starts_and_hands_on_practices_in_english/chapter_04_migration_and_synchronization_oceanbase/migration_and_synchronization_through_other_tools",draft:!1,unlisted:!1,editUrl:"https://github.com/oceanbase/oceanbase.github.io/tree/main/docs/user_manual/quick_starts_and_hands_on_practices_in_english/chapter_04_migration_and_synchronization_oceanbase/07_migration_and_synchronization_through_other_tools.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{title:"Use other tools for data migration and synchronization",weight:8},sidebar:"quick_starts_and_hands_on_practices_in_englishSidebar",previous:{title:"Use SQL statements for data migration",permalink:"/docs/user_manual/quick_starts_and_hands_on_practices_in_english/chapter_04_migration_and_synchronization_oceanbase/migration_and_synchronization_through_sql"},next:{title:"Introduction",permalink:"/docs/user_manual/quick_starts_and_hands_on_practices_in_english/chapter_05_operation_and_maintenance/introduction"}},d={},c=[{value:"Use Flink CDC for data migration and synchronization",id:"use-flink-cdc-for-data-migration-and-synchronization",level:2},{value:"Overview",id:"overview",level:3},{value:"Supported connectors",id:"supported-connectors",level:3},{value:"Configure Flink CDC-based data migration and synchronization",id:"configure-flink-cdc-based-data-migration-and-synchronization",level:3},{value:"Use Canal for data migration and synchronization",id:"use-canal-for-data-migration-and-synchronization",level:2},{value:"Overview",id:"overview-1",level:3},{value:"Architecture and components",id:"architecture-and-components",level:3},{value:"Configure Canal-based data migration and synchronization",id:"configure-canal-based-data-migration-and-synchronization",level:3},{value:"Use DataX for data migration",id:"use-datax-for-data-migration",level:2},{value:"Overview",id:"overview-2",level:3},{value:"DataX architecture",id:"datax-architecture",level:3},{value:"DataX 3.0 plug-ins",id:"datax-30-plug-ins",level:3},{value:"DataX core modules",id:"datax-core-modules",level:3},{value:"Configure DataX-based data migration",id:"configure-datax-based-data-migration",level:3},{value:"Use SeaTunnel for data migration and synchronization",id:"use-seatunnel-for-data-migration-and-synchronization",level:2},{value:"Deploy SeaTunnel",id:"deploy-seatunnel",level:3},{value:"Configure the environment",id:"configure-the-environment",level:4},{value:"Download the software",id:"download-the-software",level:4},{value:"Install the connector",id:"install-the-connector",level:4},{value:"Configure a synchronization task",id:"configure-a-synchronization-task",level:3}];function l(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"47-use-other-tools-for-data-migration-and-synchronization",children:"4.7 Use other tools for data migration and synchronization"})}),"\n",(0,s.jsx)(n.p,{children:"OceanBase Database also supports many third-party data import and export tools, such as Flink CDC, DataX, and Canal. These open source tools provide slightly different features, and therefore are applicable to different scenarios. This topic describes how to use these tools for data migration and synchronization between OceanBase Database and other databases."}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Note"})}),"\n",(0,s.jsx)(n.p,{children:"The official documents referenced in this tutorial are of the latest version available at the time of writing. You can switch to another version as needed in the upper-left corner of the document page."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"use-flink-cdc-for-data-migration-and-synchronization",children:"Use Flink CDC for data migration and synchronization"}),"\n",(0,s.jsx)(n.h3,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Change data capture (CDC) helps you monitor and capture changes in databases. You can do many things with data provided by CDC. For example, you can use the data to make historical databases or perform near real-time caching. You can also provide the CDC data to message queues (MQs), so you can use MQs for analysis and auditing."}),"\n",(0,s.jsx)(n.p,{children:"Flink CDC Connectors (Flink CDC for short) is a set of source connectors for Apache Flink. Flink CDC can read historical data and incremental changes from most databases in real time, and synchronize full and incremental data of different databases to MQs and data warehouses. You can also use Flink CDC for real-time data integration, to import database data to a data lake or data warehouse in real time."}),"\n",(0,s.jsx)(n.p,{children:"Flink CDC also supports data processing. You can use the SQL client of Flink CDC to associate, widen, and aggregate database data in real time, and write the results to various stores."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Flink CDC",src:a(25698).A+"",width:"1898",height:"856"})}),"\n",(0,s.jsx)(n.h3,{id:"supported-connectors",children:"Supported connectors"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Connector"}),(0,s.jsx)(n.th,{children:"Database"}),(0,s.jsx)(n.th,{children:"Driver"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"mongodb-cdc"}),(0,s.jsx)(n.td,{children:"MongoDB: 3.6, 4.x, and 5.0"}),(0,s.jsx)(n.td,{children:"MongoDB Driver: 4.3.4"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"mysql-cdc"}),(0,s.jsx)(n.td,{children:(0,s.jsxs)("ul",{children:[(0,s.jsx)("li",{children:"MySQL: 5.6, 5.7, and 8.0.x"}),(0,s.jsx)("li",{children:"RDS for MySQL: 5.6, 5.7, and 8.0.x"}),(0,s.jsx)("li",{children:"PolarDB for MySQL: 5.6, 5.7, and 8.0.x"}),(0,s.jsx)("li",{children:"Aurora for MySQL: 5.6, 5.7, and 8.0.x"}),(0,s.jsx)("li",{children:"MariaDB: 10.x"}),(0,s.jsx)("li",{children:"PolarDB X: 2.0.1"})]})}),(0,s.jsx)(n.td,{children:"JDBC Driver: 8.0.28"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"oceanbase-cdc"}),(0,s.jsx)(n.td,{children:(0,s.jsxs)("ul",{children:[(0,s.jsx)("li",{children:"OceanBase Database Community Edition: V3.1.x and V4.x"}),(0,s.jsx)("li",{children:"OceanBase Database Enterprise Edition: V2.x, V3.x, and V4.x"})]})}),(0,s.jsx)(n.td,{children:"OceanBase Driver: V2.4.x"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"oracle-cdc"}),(0,s.jsx)(n.td,{children:"Oracle: 11, 12, 19, and 21"}),(0,s.jsx)(n.td,{children:"Oracle Driver: 19.3.0.0"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"postgres-cdc"}),(0,s.jsx)(n.td,{children:"PostgreSQL: 9.6, 10, 11, 12, 13, and 14"}),(0,s.jsx)(n.td,{children:"DBC Driver: 42.5.1"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"sqlserver-cdc"}),(0,s.jsx)(n.td,{children:"SQLServer: 2012, 2014, 2016, 2017, and 2019"}),(0,s.jsx)(n.td,{children:"JDBC Driver: 9.4.1.jre8"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"tidb-cdc"}),(0,s.jsx)(n.td,{children:"TiDB: 5.1.x, 5.2.x, 5.3.x, 5.4.x, and 6.0.0"}),(0,s.jsx)(n.td,{children:"JDBC Driver: 8.0.27"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"db2-cdc"}),(0,s.jsx)(n.td,{children:"DB2: 11.5"}),(0,s.jsx)(n.td,{children:"DB2 Driver: 11.5.0.0"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"vitess-cdc"}),(0,s.jsx)(n.td,{children:"Vitess: 8.0.x and 9.0.x"}),(0,s.jsx)(n.td,{children:"MySQL JDBC Driver: 8.0.26"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"configure-flink-cdc-based-data-migration-and-synchronization",children:"Configure Flink CDC-based data migration and synchronization"}),"\n",(0,s.jsx)(n.p,{children:"For more information, see the following topics:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://en.oceanbase.com/docs/common-oceanbase-database-10000000001103629",children:"Use Flink CDC to synchronize data from a MySQL database to OceanBase Database"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://en.oceanbase.com/docs/common-oceanbase-database-10000000001103614",children:"Use Flink CDC to migrate data from OceanBase Database to a MySQL database"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"use-canal-for-data-migration-and-synchronization",children:"Use Canal for data migration and synchronization"}),"\n",(0,s.jsx)(n.h3,{id:"overview-1",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Canal is an open source product of Alibaba. It provides subscription and consumption of incremental data based on the parsing of incremental logs in MySQL databases."}),"\n",(0,s.jsx)(n.p,{children:"The following figure shows how Canal works."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"How Canal works",src:a(81785).A+"",width:"1361",height:"717"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Canal disguises itself as a MySQL slave by simulating the communication protocol of the MySQL slave, and sends a dump request to the MySQL master."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The MySQL master receives the dump request, and pushes binary logs to the slave, which is Canal."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Canal parses the binary logs into a stream of bytes."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"architecture-and-components",children:"Architecture and components"}),"\n",(0,s.jsx)(n.p,{children:"The following figure shows the architecture and components of Canal."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Canal architecture ",src:a(75536).A+"",width:"880",height:"382"})}),"\n",(0,s.jsx)(n.p,{children:"where"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"server: indicates a Canal instance, which corresponds to a Java virtual machine (JVM)."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Instance: indicates a data queue. A server can host one to N instances. An instance module contains the following components:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"eventParser: the parser that accesses the data source, and simulates and parses the communication protocol of the MySQL slave for communication with the MySQL master."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"eventSink: the linker between eventParser and eventStore for data filtering, processing, and dispatch."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"eventStore: the data storage."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"metaManager: the incremental subscription and consumption information manager."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"configure-canal-based-data-migration-and-synchronization",children:"Configure Canal-based data migration and synchronization"}),"\n",(0,s.jsx)(n.p,{children:"For more information, see the following topics:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://en.oceanbase.com/docs/common-oceanbase-database-10000000001103636",children:"Use Canal to synchronize data from a MySQL database to OceanBase Database"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://en.oceanbase.com/docs/common-oceanbase-database-10000000001103616",children:"Use Canal to synchronize data from OceanBase Database to a MySQL database"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"use-datax-for-data-migration",children:"Use DataX for data migration"}),"\n",(0,s.jsx)(n.h3,{id:"overview-2",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"DataX is the open source edition of DataWorks of Alibaba Cloud. It is an offline data synchronization tool/platform widely used in Alibaba Group. DataX efficiently synchronizes data between heterogeneous data sources such as MySQL, Oracle, SQL Server, PostgreSQL, Hadoop Distributed File System (HDFS), Hive, ADS, HBase, Table Store (OTS), MaxCompute (formerly known as ODPS), Distributed Relational Database Service (DRDS), and OceanBase Database."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"DataX topology",src:a(71407).A+"",width:"1037",height:"315"})}),"\n",(0,s.jsx)(n.p,{children:"To address issues of data synchronization between heterogeneous data sources in a complex mesh topology, DataX introduces the star topology and serves as the transmission hub that connects to various data sources. This way, you can connect a new data source to DataX and start synchronizing data between the new data source and the existing data sources right away."}),"\n",(0,s.jsx)(n.p,{children:"DataX has been widely used in Alibaba Group for six years with stable operation. It undertakes all offline big data synchronization services. At present, DataX handles more than 80,000 synchronization jobs and transmits more than 300 TB of data every day."}),"\n",(0,s.jsx)(n.h3,{id:"datax-architecture",children:"DataX architecture"}),"\n",(0,s.jsx)(n.p,{children:"DataX is an offline data synchronization framework that is designed based on the framework + plug-in architecture. Data source reads and writes are abstracted as the reader and writer plug-ins and are integrated into the entire framework."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"DataX architecture",src:a(32390).A+"",width:"829",height:"154"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Reader: The reader plug-in is a data collection module that collects data from a data source and sends the data to the framework."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Writer: The writer plug-in is a data write module that retrieves data from the framework and writes the data to the destination."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Framework: The framework builds a data transmission channel to connect the reader and the writer and processes core technical issues such as caching, throttling, concurrency, and data conversion."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"datax-30-plug-ins",children:"DataX 3.0 plug-ins"}),"\n",(0,s.jsx)(n.p,{children:"Over years of development, DataX has supported a wide range of plug-ins for connection with mainstream RDBMS databases, NoSQL databases, and big data computing systems. The following table describes data sources supported by DataX."}),"\n",(0,s.jsxs)("table",{children:[(0,s.jsx)("thead",{children:(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"Category"}),(0,s.jsx)("th",{children:"Data source"}),(0,s.jsx)("th",{children:"Reader"}),(0,s.jsx)("th",{children:"Writer"}),(0,s.jsx)("th",{children:"File access"})]})}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{rowspan:"8",children:"RDBMS database"}),(0,s.jsx)("td",{children:"MySQL"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Oracle"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"OceanBase Database"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"SQLServer"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"PostgreSQL"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"DRDS"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Dameng"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"General RDBMS databases"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{rowspan:"4",children:"Alibaba Cloud data warehouse"}),(0,s.jsx)("td",{children:"ODPS"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"ADS"}),(0,s.jsx)("td",{}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"OSS"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"OCS"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{rowspan:"5",children:"NoSQL data storage"}),(0,s.jsx)("td",{children:"OTS"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"HBase 0.94"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"HBase 1.1"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"MongoDB"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Hive"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{rowspan:"4",children:"Unstructured data storage"}),(0,s.jsx)("td",{children:"TxtFile"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"FTP"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"HDFS"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Read/Write"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Elasticsearch"}),(0,s.jsx)("td",{}),(0,s.jsx)("td",{children:"\u2705"}),(0,s.jsx)("td",{children:"Write"})]})]}),"\n",(0,s.jsx)(n.h3,{id:"datax-core-modules",children:"DataX core modules"}),"\n",(0,s.jsx)(n.p,{children:"Open source DataX 3.0 supports data synchronization by multiple threads on a single server. The following sequence diagram describes how modules of DataX work with each other in handling a job."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"DataX modules",src:a(29949).A+"",width:"873",height:"391"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Core modules"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"When DataX receives a data synchronization job, it starts a process to handle the job. The Job module of DataX, as the hub that manages the execution of a job, provides features such as data cleaning, job splitting, and task group management."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"When the Job module is started, it splits a job into multiple tasks for concurrent execution based on the specific source splitting strategy. A task is the basic unit of a DataX job. Each task synchronizes part of data."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"After job splitting, the Job module calls the Scheduler module to group the tasks based on the number of concurrent tasks that you specified. Tasks in each group are executed based on the supported number of concurrent tasks, which is 5 for each task group by default."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"When a task is started in a task group, the task is executed by the thread of Reader > Channel > Writer."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The Job module monitors the execution of tasks and then exits when all tasks in all task groups are completed. If the Job module exits unexpectedly, the process returns a non-zero value."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"DataX scheduling process"})}),"\n",(0,s.jsx)(n.p,{children:"Assume that you submit a DataX job to synchronize data from 100 MySQL tables to ODPS and set the number of concurrent tasks to 20. The scheduling logic in DataX is as follows:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The Job module splits the job into 100 tasks."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Since 20 concurrent tasks are configured and each task group supports 5 concurrent tasks by default, DataX allocates the tasks to 4 groups."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"The 25 tasks allocated to each of the 4 task groups are executed with the concurrency of 5 tasks."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"configure-datax-based-data-migration",children:"Configure DataX-based data migration"}),"\n",(0,s.jsx)(n.p,{children:"For more information, see the following topics:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://en.oceanbase.com/docs/common-oceanbase-database-10000000001103634",children:"Use DataX to migrate table data from a MySQL database to OceanBase Database"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://en.oceanbase.com/docs/common-oceanbase-database-10000000001103612",children:"Use DataX to migrate table data from OceanBase Database to a MySQL database"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://en.oceanbase.com/docs/common-oceanbase-database-10000000001103609",children:"Use DataX to migrate table data from an Oracle database to OceanBase Database"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://en.oceanbase.com/docs/common-oceanbase-database-10000000001103640",children:"Use DataX to migrate table data from OceanBase Database to an Oracle database"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://en.oceanbase.com/docs/common-oceanbase-database-10000000001103638",children:"Use DataX to migrate CSV files to OceanBase Database"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"use-seatunnel-for-data-migration-and-synchronization",children:"Use SeaTunnel for data migration and synchronization"}),"\n",(0,s.jsx)(n.p,{children:"This section describes how to deploy and use SeaTunnel. For more information about SeaTunnel, see '4.2 Ecosystem components for data migration and synchronization'."}),"\n",(0,s.jsx)(n.h3,{id:"deploy-seatunnel",children:"Deploy SeaTunnel"}),"\n",(0,s.jsx)(n.h4,{id:"configure-the-environment",children:"Configure the environment"}),"\n",(0,s.jsx)(n.p,{children:"Before installing SeaTunnel, you need to configure a Java 8 or 11 environment. Other versions later than Java 8 can theoretically work as well."}),"\n",(0,s.jsx)(n.h4,{id:"download-the-software",children:"Download the software"}),"\n",(0,s.jsx)(n.p,{children:"If your server can connect to the Internet, you can run the following command to directly download the software by using the terminal:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'export version="2.3.3"\nwget "https://archive.apache.org/dist/seatunnel/${version}/apache-seatunnel-${version}-bin.tar.gz"\ntar -xzvf "apache-seatunnel-${version}-bin.tar.gz"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["If your server cannot connect to the Internet, you can download the installation package from ",(0,s.jsx)(n.a,{href:"https://seatunnel.apache.org/download/",children:"SeaTunnel official website"})," and decompress the package on your server for installation."]}),"\n",(0,s.jsx)(n.h4,{id:"install-the-connector",children:"Install the connector"}),"\n",(0,s.jsx)(n.p,{children:"You can install the connector by using the following two methods:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Online installation"}),"\n",(0,s.jsxs)(n.p,{children:["In SeaTunnel 2.2.0-beta and later, the binary package of SeaTunnel does not provide the connector dependencies by default. If you use SeaTunnel of a version in that range for the first time, you need to execute the following command to install the connector. In this example, the connector of version ",(0,s.jsx)(n.code,{children:"2.3.3"})," is installed."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"sh bin/install-plugin.sh 2.3.3\n"})}),"\n",(0,s.jsxs)(n.p,{children:["In general, you do not need to install all connector plug-ins. You can open the ",(0,s.jsx)(n.code,{children:"plugin_config"})," configuration file in the ",(0,s.jsx)(n.code,{children:"config/"})," directory and specify the required plug-ins. For example, if you need only the ",(0,s.jsx)(n.code,{children:"connector-console"})," plug-in, you can modify ",(0,s.jsx)(n.code,{children:"plugin.properties"})," as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"--seatunnel-connectors--\nconnector-console\n--end--\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Offline installation"}),"\n",(0,s.jsxs)(n.p,{children:["If your server cannot connect to the Internet, you can manually download the connector and upload it to the ",(0,s.jsx)(n.code,{children:"connectors/seatunnel"})," directory."]}),"\n",(0,s.jsx)(n.p,{children:"In this case, take note of the following considerations:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"connectors"})," directory must contain the following subdirectories. Otherwise, you must manually create the subdirectories."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"flink\nflink-sql\nseatunnel\nspark\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["You can download only the required V2 connector plug-ins and store them in the ",(0,s.jsx)(n.code,{children:"seatunnel"})," directory."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"configure-a-synchronization-task",children:"Configure a synchronization task"}),"\n",(0,s.jsx)(n.p,{children:"After you install SeaTunnel, you can edit the configuration file and start a data synchronization task. The following example describes how to synchronize data from a MySQL database to OceanBase Database."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{}),(0,s.jsx)(n.th,{children:"Source"}),(0,s.jsx)(n.th,{children:"Destination"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Database"}),(0,s.jsx)(n.td,{children:"MySQL"}),(0,s.jsx)(n.td,{children:"OceanBase Database in MySQL mode"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Table"}),(0,s.jsx)(n.td,{children:"mysql2ob"}),(0,s.jsx)(n.td,{children:"mysql2ob"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"IP address"}),(0,s.jsx)(n.td,{children:"10.10.10.1"}),(0,s.jsx)(n.td,{children:"10.10.10.2"})]})]})]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["In the source MySQL database named ",(0,s.jsx)(n.code,{children:"test"}),", create a table named ",(0,s.jsx)(n.code,{children:"mysql2ob"})," and write three data records."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"MySQL [test]> create table mysql2ob(id int primary key, name varchar(20));\nQuery OK, 0 rows affected (0.01 sec)\n\nMySQL [test]> insert into mysql2ob values(1,'oceanbase');\nQuery OK, 1 row affected (0.00 sec)\n\nMySQL [test]> insert into mysql2ob values(2,'oracle');\nQuery OK, 1 row affected (0.00 sec)\n\nMySQL [test]> insert into mysql2ob values(3,'mysql');\nQuery OK, 1 row affected (0.00 sec)\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["In the destination database named ",(0,s.jsx)(n.code,{children:"test"}),", which is a MySQL tenant of OceanBase Database, create a table named ",(0,s.jsx)(n.code,{children:"mysql2ob"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"obclient [test]> create table mysql2ob(id int primary key, name varchar(20));\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Create a configuration file named ",(0,s.jsx)(n.code,{children:"mysql_to_oceanbase.conf"})," in the ",(0,s.jsx)(n.code,{children:"config"})," directory."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"[admin@test ~]$ vim config/mysql_to_oceanbase.conf\n"})}),"\n",(0,s.jsx)(n.p,{children:"The file content is as follows:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:'env {\n        job.mode = "STREAMING"\n        execution.parallelism = 1\n        checkpoint.interval = 10000\n}\nsource {\n    MySQL-CDC {\n        result_table_name = "test"\n        parallelism = 1\n        server-id = 5656\n        username = "root"\n        password = "******"\n        table-names = ["test.mysql2ob"]\n        base-url = "jdbc:mysql://10.10.10.1:3306/test"\n    }\n}\n\nsink {\n  jdbc {\n        url = "jdbc:mysql://10.10.10.2:2883/test"\n        driver = "com.mysql.jdbc.Driver"\n        user = "root@obtest#obcluster"\n        password = "******"\n        generate_sink_sql = true\n        database = "test"\n        table = "mysql2ob"\n  }\n}\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Save the file and start the synchronization task."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"[admin@test ~]$ bash ./bin/seatunnel.sh --config ./config/mysql_to_oceanbase.conf -e local\n"})}),"\n",(0,s.jsx)(n.p,{children:"The logs are as follows:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"2024-02-29 18:56:25,664 WARN  org.apache.seatunnel.core.starter.seatunnel.args.ClientCommandArgs$MasterTypeValidator -\n******************************************************************************************\n-e and --deploy-mode deprecated in 2.3.1, please use -m and --master instead of it\n******************************************************************************************\nFeb 29, 2024 6:56:25 PM com.hazelcast.internal.config.AbstractConfigLocator\nINFO: Loading configuration '/opt/seatunnel/apache-seatunnel-2.3.3/config/seatunnel.yaml' from System property 'seatunnel.config'\nFeb 29, 2024 6:56:25 PM com.hazelcast.internal.config.AbstractConfigLocator\nINFO: Using configuration file at /opt/seatunnel/apache-seatunnel-2.3.3/config/seatunnel.yaml\nFeb 29, 2024 6:56:25 PM org.apache.seatunnel.engine.common.config.SeaTunnelConfig\nINFO: seatunnel.home is /opt/seatunnel/apache-seatunnel-2.3.3\nFeb 29, 2024 6:56:25 PM com.hazelcast.internal.config.AbstractConfigLocator\nINFO: Loading configuration '/opt/seatunnel/apache-seatunnel-2.3.3/config/hazelcast.yaml' from System property 'hazelcast.config'\nFeb 29, 2024 6:56:25 PM com.hazelcast.internal.config.AbstractConfigLocator\nINFO: Using configuration file at /opt/seatunnel/apache-seatunnel-2.3.3/config/hazelcast.yaml\n2024-02-29 18:56:26,178 WARN  com.hazelcast.instance.AddressPicker - [LOCAL] [seatunnel-997250] [5.1] You configured your member address as host name. Please be aware of that your dns can be spoofed. Make sure that your dns configurations are correct.\n2024-02-29 18:56:26,178 INFO  com.hazelcast.instance.AddressPicker - [LOCAL] [seatunnel-997250] [5.1] Resolving domain name 'localhost' to address(es): [127.0.0.1]\n2024-02-29 18:56:26,179 INFO  com.hazelcast.instance.AddressPicker - [LOCAL] [seatunnel-997250] [5.1] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [localhost/127.0.0.1]\n2024-02-29 18:56:26,202 INFO  org.apache.seatunnel.engine.server.SeaTunnelServer - SeaTunnel server start...\n2024-02-29 18:56:26,204 INFO  com.hazelcast.system - [localhost]:5801 [seatunnel-997250] [5.1] Based on Hazelcast IMDG version: 5.1.0 (20220228 - 21f20e7)\n2024-02-29 18:56:26,204 INFO  com.hazelcast.system - [localhost]:5801 [seatunnel-997250] [5.1] Cluster name: seatunnel-997250\n2024-02-29 18:56:26,204 INFO  com.hazelcast.system - [localhost]:5801 [seatunnel-997250] [5.1]\n\n _____               _____                             _\n/  ___|             |_   _|                           | |\n\\ `--.   ___   __ _   | |   _   _  _ __   _ __    ___ | |\n `--. \\ / _ \\ / _` |  | |  | | | || '_ \\ | '_ \\  / _ \\| |\n/\\__/ /|  __/| (_| |  | |  | |_| || | | || | | ||  __/| |\n\\____/  \\___| \\__,_|  \\_/   \\__,_||_| |_||_| |_| \\___||_|\n\n......\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"After you write data in the source database, you can find that the data is automatically synchronized to the destination database. That's it. You can try more data synchronization scenarios based on the preceding example."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},25698:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/001-7bca9bf40d8b825dd3fc74c2d0c95e10.png"},81785:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/002-ef24ca402c3466186b9aaf7290c74f47.png"},75536:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/003-de30e614f130a287eea696f4416ec7f7.png"},71407:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/004-6625144f7fd29667c2bb3df07dfc0624.png"},32390:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/005-7b2bdeb35b301842bc15b3b63626469a.png"},29949:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/006-8cb77d2be436911ae42ad7e90b82fca4.png"},28453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>o});var s=a(96540);const t={},i=s.createContext(t);function r(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);