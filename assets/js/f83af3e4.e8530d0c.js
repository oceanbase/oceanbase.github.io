"use strict";(self.webpackChunkmy_docs_website=self.webpackChunkmy_docs_website||[]).push([[2719],{83472:(e,a,s)=>{s.r(a),s.d(a,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var t=s(74848),n=s(28453);const i={slug:"vivo",title:"Migrated from MySQL to OceanBase Database, vivo Built a Robust Data Foundation Without Standalone Performance Bottlenecks",tags:["User Case"]},o=void 0,r={id:"blogs/users/vivo",title:"Migrated from MySQL to OceanBase Database, vivo Built a Robust Data Foundation Without Standalone Performance Bottlenecks",description:"This article, authored by Xu Shaohui from the vivo Internet and Database Team, was originally published by vivo Internet Technology on WeChat Official Accounts Platform. It listed major database challenges vivo faced and described the solution provided by OceanBase, along with its implementation.",source:"@site/docs/blogs/users/vivo.md",sourceDirName:"blogs/users",slug:"/blogs/users/vivo",permalink:"/docs/blogs/users/vivo",draft:!1,unlisted:!1,editUrl:"https://github.com/oceanbase/oceanbase.github.io/tree/main/docs/blogs/users/vivo.md",tags:[{inline:!0,label:"User Case",permalink:"/docs/tags/user-case"}],version:"current",frontMatter:{slug:"vivo",title:"Migrated from MySQL to OceanBase Database, vivo Built a Robust Data Foundation Without Standalone Performance Bottlenecks",tags:["User Case"]},sidebar:"blogsSidebar",previous:{title:"Handling Two Trading Scenarios with One OceanBase Cluster",permalink:"/docs/blogs/users/two-trading-one-cluster"},next:{title:"The architectural evolution of OceanBase Database",permalink:"/docs/blogs/showcases/architectural-evolution"}},c={},l=[{value:"1 Replace the Sharding Solution with OceanBase Database",id:"1-replace-the-sharding-solution-with-oceanbase-database",level:2},{value:"2 Deploy Tools to Prepare for Migration",id:"2-deploy-tools-to-prepare-for-migration",level:2},{value:"3 Smooth Migration to Break Capacity Limits of a Single Server",id:"3-smooth-migration-to-break-capacity-limits-of-a-single-server",level:2},{value:"4 Summary",id:"4-summary",level:2}];function d(e){const a={blockquote:"blockquote",code:"code",em:"em",h2:"h2",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,n.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(a.blockquote,{children:["\n",(0,t.jsx)(a.p,{children:"This article, authored by Xu Shaohui from the vivo Internet and Database Team, was originally published by vivo Internet Technology on WeChat Official Accounts Platform. It listed major database challenges vivo faced and described the solution provided by OceanBase, along with its implementation."}),"\n"]}),"\n",(0,t.jsx)(a.p,{children:"vivo is a technology company providing smart devices and intelligent services to over 500 million users worldwide. As our expanding user base kept generating more data, our database team ran into challenges in O&M of our legacy database system."}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Necessity for sharding"}),": As the growing data volume of MySQL instances exceeded the capacity limits of a single server, we must perform database and table sharding, which incurred high costs and risks, and compelled the need for a MySQL-compatible distributed database to address these issues."]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.strong,{children:"Cost pressure"}),": Our large user base caused significant annual data growth, and we had to keep buying new servers for data storage, leading to mounting cost pressure."]}),"\n"]}),"\n",(0,t.jsx)(a.p,{children:"To tackle those challenges, we chose OceanBase Database after evaluating distributed database products that are compatible with MySQL and provide proven features."}),"\n",(0,t.jsx)(a.h2,{id:"1-replace-the-sharding-solution-with-oceanbase-database",children:"1 Replace the Sharding Solution with OceanBase Database"}),"\n",(0,t.jsx)(a.p,{children:"We chose OceanBase Database in the expectation that its native distributed architecture and table partitioning feature could resolve the issues due to the MySQL sharding solution. We also hoped that its exceptional data compression and tenant-level resource isolation could help cut our storage and O&M costs."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"(1) Native distributed architecture and table partitioning"})}),"\n",(0,t.jsx)(a.p,{children:"The native distributed architecture of OceanBase Database consists of an OBProxy layer for data routing and an OBServer layer that stores data and handles computing tasks. OBServer nodes are managed in zones to ensure the proper functioning of automatic disaster recovery mechanisms and optimization strategies within an OceanBase cluster. Depending on the business scenarios, we can deploy OceanBase Database in different high-availability architectures, such as three IDCs in the same region and five IDCs across three regions. By adding or removing OBServer nodes, we can horizontally scale out or in an OceanBase cluster to quickly increase or decrease resources, thus eliminating capacity limits of a single server."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"1733888919",src:s(43442).A+"",width:"1080",height:"788"})}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.em,{children:"Figure: Distributed architecture of OceanBase Database"})}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"(2) Data compression and tenant-level resource isolation"})}),"\n",(0,t.jsx)(a.p,{children:"OceanBase Database supports table partitioning. Partitions are evenly distributed across different OBServer nodes. Each physical partition has a storage layer object, called a tablet, for storing data records. A tablet has multiple replicas distributed across different OBServer nodes. OceanBase Database uses log streams for data persistence and inter-replica synchronization. Under normal conditions, leader replicas are used to provide services. When a leader replica fails, the system automatically uses a follower replica instead to ensure data safety and service availability."}),"\n",(0,t.jsx)(a.p,{children:"In an OceanBase cluster, you can create multiple isolated database instances. Each of such instances is called a tenant. In other words, a single cluster can serve multiple business lines with the data of one tenant isolated from that of others. This feature reduces deployment and O&M costs."}),"\n",(0,t.jsx)(a.p,{children:"Moreover, OceanBase Database provides a storage engine based on the log-structured merge-tree (LSM-tree) architecture, and thus boasts exceptional data compression capabilities. According to official documentation and case studies, it can slash storage costs by over 70%."}),"\n",(0,t.jsx)(a.p,{children:"In a nutshell, OceanBase Database's native table partitioning feature effectively addresses the issues due to a sharding solution. Table partitioning is transparent to upper-layer applications. It not only greatly cuts the costs and time wasted on code modifications, but also lowers system risks and improves business availability. Additionally, OceanBase Database provides data compression algorithms that substantially shrink the storage space required, while its performance, availability, security, and community support meet our expectations and business needs."}),"\n",(0,t.jsx)(a.h2,{id:"2-deploy-tools-to-prepare-for-migration",children:"2 Deploy Tools to Prepare for Migration"}),"\n",(0,t.jsx)(a.p,{children:"To ensure a successful migration to OceanBase Database and smooth database O&M in the new architecture, we deployed OceanBase Cloud Platform (OCP), OceanBase LogProxy (oblogproxy), and OceanBase Migration Service (OMS) before migration. These tools could help us manage cluster deployment, handle monitoring alerts, perform backup and restore, collect logs, and migrate data. Combined with our internal database management platform, our database administrators were able to manage metadata, and query and modify data, making the system ready for production."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"(1) OCP deployment"})}),"\n",(0,t.jsx)(a.p,{children:"OCP is an enterprise-level database management platform tailored for OceanBase clusters. It provides full-lifecycle management of components such as OceanBase clusters and tenants, and manages OceanBase resources such as hosts, networks, and software packages. It enables us to manage OceanBase clusters more efficiently and reduces our IT O&M costs."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"1733889058",src:s(84112).A+"",width:"1080",height:"527"})}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.em,{children:"Figure: Architecture of OCP"})}),"\n",(0,t.jsx)(a.p,{children:"OCP consists of six modules working in coordination: Management Agent, Management Service, Metadata Repository, Monitor Repository, Management Console, and OBProxy. It can be deployed in high availability mode, where one primary and multiple standby OCP clusters are maintained to avoid single points of failure (SPOFs)."}),"\n",(0,t.jsx)(a.p,{children:"We deployed OCP on three nodes in different IDCs. In addition, since we already had an alerting platform, we created custom alerting channels to integrate it with OCP, making it more compatible with the OCP alerting service."}),"\n",(0,t.jsx)(a.p,{children:"Another crucial feature of OCP is backup and restore. Physical backups stored in OCP consist of baseline data and archived log data, and follower replicas are often used for backup tasks. When a user initiates a backup request, it is first forwarded to the node running RootService. RootService generates a data backup task based on the current tenant and the partition groups (PGs) of the tenant. The backup task is then distributed to OBServer nodes for parallel execution. Backup files are stored on online storage media."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"1733889089",src:s(12930).A+"",width:"800",height:"477"})}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.em,{children:"Figure: OCP high-availability architecture"})}),"\n",(0,t.jsx)(a.p,{children:"OceanBase Database supports various storage media, such as Network File System (NFS), Alibaba Cloud Object Storage Service (OSS), Tencent Cloud Object Storage (COS), Amazon Simple Storage Service (S3), and object storage services compatible with the S3 protocol. Notably, the backup strategy of OCP requires S3 storage media. If you launch a cluster backup task in OCP, you must store backup files in the specified S3 directory, as shown in the following figure."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"1733889113",src:s(91606).A+"",width:"921",height:"593"})}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"(2) oblogproxy deployment"})}),"\n",(0,t.jsx)(a.p,{children:"oblogproxy is the incremental log proxy service of OceanBase Database. It establishes connections with OceanBase Database to read incremental logs and provides downstream services with change data capture (CDC) capabilities. The binlog mode of oblogproxy is designed for compatibility with MySQL binlogs. It allows us to synchronize MySQL binlogs to OceanBase Database."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"1733889133",src:s(92450).A+"",width:"1080",height:"1034"})}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.em,{children:"Figure: Architecture of oblogproxy"})}),"\n",(0,t.jsx)(a.p,{children:"oblogproxy starts the binlog converter (BC) module to pull clogs from OceanBase Database and converts them into binlogs, which are then written to binlog files. A MySQL binlog tool, such as Canal or Flink-CDC, initiates binlog subscription requests to OBProxy, which forwards the requests to oblogproxy. Upon receiving a request, oblogproxy starts the binlog dumper (BD) module, which reads binlog files and provides subscription services by performing binlog dumps. We deployed oblogproxy across multiple nodes and stored the metadata in shared online storage to ensure high availability."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"(3) OMS deployment"})}),"\n",(0,t.jsx)(a.p,{children:"OMS supports data exchange between a homogeneous or heterogeneous data source and OceanBase Database. OMS provides the capabilities for online migration of existing data and real-time synchronization of incremental data."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"1733889157",src:s(95389).A+"",width:"1080",height:"648"})}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.em,{children:"Figure: Architecture of OMS"})}),"\n",(0,t.jsx)(a.p,{children:"OMS has the following components:"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"DBCat: It collects and converts data objects."}),"\n",(0,t.jsx)(a.li,{children:"Store for pulling incremental data, Incr-Sync for synchronizing incremental data, Full-Import for importing full data, and Full-Verification for verifying full data."}),"\n",(0,t.jsx)(a.li,{children:"Basic service components for the management of clusters, resource pools, high availability mechanism, and metadata. These components ensure efficient scheduling and stable operations of the migration module."}),"\n",(0,t.jsx)(a.li,{children:"Console: It provides all-round migration scheduling capabilities."}),"\n"]}),"\n",(0,t.jsx)(a.p,{children:"We also deployed OMS on three nodes in different IDCs to ensure its high availability. For monitoring and alerting during data migration and synchronization, OMS leverages OCP\u2019s alerting channels instead of implementing redundant components."}),"\n",(0,t.jsx)(a.h2,{id:"3-smooth-migration-to-break-capacity-limits-of-a-single-server",children:"3 Smooth Migration to Break Capacity Limits of a Single Server"}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"(1) Migration from MySQL to OceanBase Database"})}),"\n",(0,t.jsx)(a.p,{children:"To prevent issues during migration, we conducted a feasibility assessment, which included performance stress tests and compatibility tests on, for example, table schemas, SQL statements, and accounts. The test results met our requirements. During partition adaptability testing, we found that applications required table schemas and SQL statements be adapted to partitioned tables, which, considering the modification costs, was within our expectations."}),"\n",(0,t.jsx)(a.p,{children:"Then, we launched OMS to migrate all existing data and incremental data from MySQL to OceanBase Database. OMS ensured real-time synchronization and full data verification. Its reverse incremental synchronization feature enables instant rollback in case of migration failures, ensuring business availability."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"1733889240",src:s(76491).A+"",width:"731",height:"257"})}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.em,{children:"Figure: Process of a Data Migration Task in OMS"})}),"\n",(0,t.jsx)(a.p,{children:"The migration process consists of eight steps:"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"Pre-migration configuration verification."}),"\n",(0,t.jsx)(a.li,{children:"Verification of OceanBase Database tenants and accounts."}),"\n",(0,t.jsx)(a.li,{children:"Data consistency verification."}),"\n",(0,t.jsx)(a.li,{children:"Pausing DDL operations that could modify table schemas."}),"\n",(0,t.jsx)(a.li,{children:"Verification of synchronization latency."}),"\n",(0,t.jsx)(a.li,{children:"Configuring database switchover connections or modifying DNS parameters for applications."}),"\n",(0,t.jsx)(a.li,{children:"Terminating all connections to the source database and ensuring that applications are connected to OceanBase Database."}),"\n",(0,t.jsx)(a.li,{children:"Stopping forward synchronization and enabling reverse synchronization to get ready for rollback."}),"\n"]}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"1733889297",src:s(5760).A+"",width:"485",height:"635"})}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.em,{children:"Figure: Migration process"})}),"\n",(0,t.jsx)(a.p,{children:"To ensure a successful switchover, minimize risks, and maximize business availability and security, we prepared a rollback plan."}),"\n",(0,t.jsx)(a.p,{children:"That time, we migrated nearly 20 TB of data from five MySQL clusters to OceanBase Database, which has brought us the following benefits:"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"With massive and rapidly growing data stored on cloud storage services, the MySQL sharding solution caused huge maintenance and management costs and serious availability risks. OceanBase Database not only provides table partitioning to diminish maintenance costs, and its high compression ratio also saves storage expenses."}),"\n",(0,t.jsx)(a.li,{children:"The high data write volume of the risk control cluster caused considerable master-slave latency, risking data loss. OceanBase Database fixes that issue by ensuring strong consistency, and shrinks the required storage space by 70%."}),"\n",(0,t.jsx)(a.li,{children:"The TokuDB-based archive database of the financial service suffered ineffective unique indexes and lacked technical support from TokuDB. OceanBase Database has resolved these problems. It not only improves query and DDL performance, but also eliminates capacity limits of a single server, thanks to its horizontally scalable distributed architecture."}),"\n"]}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"(2) Migration of another distributed database"})}),"\n",(0,t.jsx)(a.p,{children:"We deployed a distributed database of another vendor to support some peripheral applications, and decided to migrate these applications to OceanBase Database. Two migration methods were considered. One was based on TiCDC, Kafka, and OMS, and the other was based on CloudCanal. Their pros and cons are described in the following figure."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"1733889386",src:s(30935).A+"",width:"1317",height:"228"})}),"\n",(0,t.jsx)(a.p,{children:"The CloudCanal-based method was simple, but it did not support reverse synchronization, and demonstrated unsatisfactory performance in incremental synchronization. The other, despite a more complex architecture, was more compatible with OceanBase Database, and supported reverse synchronization, showing better overall performance. So we chose the TiCDC + Kafka + OMS method for full migration, incremental synchronization, full verification, and reverse incremental synchronization."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"1733889410",src:s(61665).A+"",width:"1172",height:"349"})}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.em,{children:"Figure: Synchronization process"})}),"\n",(0,t.jsx)(a.p,{children:"As shown in the figure above, TiCDC parses incremental data from the business cluster into ordered row-level change data, and sends it to Kafka. OMS consumes this incremental data from Kafka and writes it to OceanBase Database. Kafka retains data for seven days by default, but you can adjust the retention period if the delay is considerable. You can also increase the concurrency of OMS to improve the synchronization speed."}),"\n",(0,t.jsx)(a.p,{children:"The full migration, which involved nearly 50 billion rows, was initially quite slow, running at only 6,000-8,000 rows per second (RPS), and was estimated to take weeks to complete. Analysis revealed that the source and target databases were not under pressure, and OMS host loads were normal. The issue was traced to widely spaced values of the primary key in the source tables, causing OMS to migrate small data chunks as it used the primary key for data slicing."}),"\n",(0,t.jsxs)(a.p,{children:["We set the ",(0,t.jsx)(a.code,{children:"source.sliceBatchSize"})," parameter to ",(0,t.jsx)(a.code,{children:"12000"})," and increased memory, improving RPS to around 39,257, which still fell short of our expectations."]}),"\n",(0,t.jsxs)(a.p,{children:["By analyzing the ",(0,t.jsx)(a.code,{children:"msg/metrics.log"})," file, we found that the value of ",(0,t.jsx)(a.code,{children:"wait_dispatch_record_size"})," reached ",(0,t.jsx)(a.code,{children:"157690"}),", which was pretty high, indicating OMS bottlenecks in partition calculations. So we disabled partition calculation by setting the ",(0,t.jsx)(a.code,{children:"sink.enablePartitionBucket"})," parameter to ",(0,t.jsx)(a.code,{children:"false"}),", and set the ",(0,t.jsx)(a.code,{children:"srink.workerNum"})," parameter to a larger value. After that, the RPS increased to 500,000-600,000."]}),"\n",(0,t.jsx)(a.p,{children:"Here, I would like to talk about three issues occurred during migration."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:'Issue 1: A message reading "The response from the CM service is not success" was reported during the migration task.'})}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.strong,{children:"Solution:"})," The ",(0,t.jsx)(a.code,{children:"connector.log"})," file recorded that ",(0,t.jsx)(a.code,{children:"CM service is not success"}),", but the CM service was normal. So we checked the memory usage of the synchronization task, and found a serious memory shortage, which led to highly frequent full garbage collection, and thus service anomalies. We logged in to the OMS container, opened the ",(0,t.jsx)(a.code,{children:"/home/admin/conf/command/start_oms_cm.sh"})," file, and set the ",(0,t.jsx)(a.code,{children:"jvm"})," parameter to ",(0,t.jsx)(a.code,{children:"-server -Xmx16g -Xms16g -Xmn8g"}),"."]}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"Issue 2: The RPS of incremental synchronization was quite low, around 8,000, despite high concurrency settings and normal loads of databases and OMS."})}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.strong,{children:"Solution:"})," The ",(0,t.jsx)(a.code,{children:"connector.log"})," file of the task indicated serious primary key conflicts when the incremental synchronization caught up the full synchronization timestamp, while no data exceptions were found in the source and target databases. The issue was then traced to TiCDC writing duplicate data, which in turn prevented the OMS from batch writing. Back then, OMS had not been optimized for this specific scenario, so the only way to improve RPS was to increase the write concurrency."]}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"Issue 3: Index space amplification. When an index was created, despite the cluster's disk usage being only around 50%, this error was reported: ERROR 4184 (53100): Server out of disk space."})}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.strong,{children:"Solution:"})," The OBServer log indicated that the index space usage was amplified by 5.5 times, requiring 5.41 TB of space, while the cluster only had 1.4 TB of space remained."]}),"\n",(0,t.jsx)(a.p,{children:"Index space amplification was an issue of OceanBase Database earlier than V4.2.3. The causes were as follows:"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsxs)(a.li,{children:["\n",(0,t.jsx)(a.p,{children:"During sorting, intermediate results were written to disk, and metadata records were also generated simultaneously."}),"\n"]}),"\n",(0,t.jsxs)(a.li,{children:["\n",(0,t.jsx)(a.p,{children:"External sorting involved two rounds of data recording."}),"\n"]}),"\n",(0,t.jsxs)(a.li,{children:["\n",(0,t.jsx)(a.p,{children:"During the sorting process, data was decompressed."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(a.p,{children:"In OceanBase Database V4.2.3 and later, intermediate results are compressed and stored in a compact format, and the disk space is incrementally released during data writing. As a result, the index space amplification has been reduced to 1.5. Therefore, you can use OceanBase Database V4.2.3 or later for scenarios involving large datasets and great incremental data volume."}),"\n",(0,t.jsx)(a.h2,{id:"4-summary",children:"4 Summary"}),"\n",(0,t.jsx)(a.p,{children:"Overall, OceanBase Database has fixed vulnerabilities of vivo's previous MySQL solution, thanks to its excellent performance and data compression capabilities and robust O&M tools. Next, we will continue exploring OceanBase Database\u2019s features and look forward to further enhancements in its O&M tools to address our challenges more effectively."})]})}function h(e={}){const{wrapper:a}={...(0,n.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},12930:(e,a,s)=>{s.d(a,{A:()=>t});const t=s.p+"assets/images/0539554b-07d3-4251-a88d-a5231e38b32d-7e1f4d4427c100e696b85052d1804452.png"},84112:(e,a,s)=>{s.d(a,{A:()=>t});const t=s.p+"assets/images/1f92fd96-c804-4de0-bc43-57f6bb3d47ee-8671365f7f79a8c4cd7be16d5240ec06.png"},95389:(e,a,s)=>{s.d(a,{A:()=>t});const t=s.p+"assets/images/25f4daab-2a49-45a8-80f7-f95ad8ba749b-bc7457a65890f8b2c669eca09d9df2cc.png"},30935:(e,a,s)=>{s.d(a,{A:()=>t});const t=s.p+"assets/images/2c998925-2260-42b0-80e9-28a6f0fdaaa9-a164a740a977ce375ef3eb40615554f6.png"},92450:(e,a,s)=>{s.d(a,{A:()=>t});const t=s.p+"assets/images/4200b9c5-c227-423a-bdf9-b6d0d10d801c-10995c4fd53503531329a9e64edc3559.png"},5760:(e,a,s)=>{s.d(a,{A:()=>t});const t=s.p+"assets/images/5870cde8-ed04-438c-9139-582f38c2676d-feff3750b92549bbe957c92f41f4d4a3.png"},61665:(e,a,s)=>{s.d(a,{A:()=>t});const t=s.p+"assets/images/88955026-f2cc-496e-a177-d64f336072aa-28111021fb481e8b75fc76be38fc7d93.png"},43442:(e,a,s)=>{s.d(a,{A:()=>t});const t=s.p+"assets/images/8d842ddd-3152-4a31-9562-85fa32b8dcd1-0acfe5be8fcb01162d86957be57c94e3.png"},91606:(e,a,s)=>{s.d(a,{A:()=>t});const t=s.p+"assets/images/c2e5b5e4-2375-475c-a22e-e1d6aeb7f773-f616641761ef2af08f8ec8a219cc3893.png"},76491:(e,a,s)=>{s.d(a,{A:()=>t});const t=s.p+"assets/images/d7d158cc-e683-4af5-b0f1-b87b87ad665d-bce87cb7e7a40356701a06036815f0cd.png"},28453:(e,a,s)=>{s.d(a,{R:()=>o,x:()=>r});var t=s(96540);const n={},i=t.createContext(n);function o(e){const a=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function r(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),t.createElement(i.Provider,{value:a},e.children)}}}]);