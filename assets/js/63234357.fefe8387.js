"use strict";(self.webpackChunkmy_docs_website=self.webpackChunkmy_docs_website||[]).push([[7208],{52806:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var o=n(74848),s=n(28453);const i={slug:"row-to-vector",title:"From Rows to Vectors: The Evolution of the Execution Engine of OceanBase Database"},a="From Rows to Vectors: The Evolution of the Execution Engine of OceanBase Database",r={id:"blogs/tech/row-to-vector",title:"From Rows to Vectors: The Evolution of the Execution Engine of OceanBase Database",description:"This article introduces database system concepts without diving into the detailed design and implementation of vectorized operators and expressions in OceanBase Database.",source:"@site/docs/blogs/tech/row-to-vector.md",sourceDirName:"blogs/tech",slug:"/blogs/tech/row-to-vector",permalink:"/docs/blogs/tech/row-to-vector",draft:!1,unlisted:!1,editUrl:"https://github.com/oceanbase/oceanbase.github.io/tree/main/docs/blogs/tech/row-to-vector.md",tags:[],version:"current",frontMatter:{slug:"row-to-vector",title:"From Rows to Vectors: The Evolution of the Execution Engine of OceanBase Database"},sidebar:"blogsSidebar",previous:{title:"How We Approach Improving Distributed Query Performance",permalink:"/docs/blogs/tech/refine-performance"},next:{title:"Insights into OceanBase Database 4.0: Support for Small-Specification Deployment to Make Distributed Databases More Accessible",permalink:"/docs/blogs/tech/small-specification-deployment"}},c={},l=[{value:"Reduce the Overhead of Virtual Function Calls",id:"reduce-the-overhead-of-virtual-function-calls",level:2},{value:"Unleash the Potential of Modern CPUs",id:"unleash-the-potential-of-modern-cpus",level:2},{value:"Compact data layout for better cache efficiency",id:"compact-data-layout-for-better-cache-efficiency",level:3},{value:"Reduced impact of branch mispredictions on the CPU pipeline",id:"reduced-impact-of-branch-mispredictions-on-the-cpu-pipeline",level:3},{value:"Accelerated computation through SIMD instructions",id:"accelerated-computation-through-simd-instructions",level:3}];function h(e){const t={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"from-rows-to-vectors-the-evolution-of-the-execution-engine-of-oceanbase-database",children:"From Rows to Vectors: The Evolution of the Execution Engine of OceanBase Database"})}),"\n",(0,o.jsxs)(t.blockquote,{children:["\n",(0,o.jsx)(t.p,{children:"This article introduces database system concepts without diving into the detailed design and implementation of vectorized operators and expressions in OceanBase Database."}),"\n"]}),"\n",(0,o.jsx)(t.h1,{id:"background",children:"Background"}),"\n",(0,o.jsxs)(t.p,{children:["The OceanBase team has launched ",(0,o.jsx)(t.a,{href:"https://youtube.com/live/3iwhQ4lAqgg",children:"OceanBase DBA: From Basics to Practices"}),", an official course series, to help users resolve issues more efficiently with OceanBase Database Community Edition. However, after the seventh live streaming, many users had difficulty understanding what terms such as ",(0,o.jsx)(t.code,{children:"rowset=16"})," or ",(0,o.jsx)(t.code,{children:"rowset=256"})," mean in a plan similar to the following one:"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-sql",children:"    obclient [test]> create table t1(c1 int, c2 int);\n    Query OK, 0 rows affected (0.203 sec)\n    \n    obclient [test]> explain select count(*) from t1 where c1 = 1000;\n    +------------------------------------------------------------------------------------+\n    | Query Plan                                                                         |\n    +------------------------------------------------------------------------------------+\n    | =================================================                                  |\n    | |ID|OPERATOR         |NAME|EST.ROWS|EST.TIME(us)|                                  |\n    | -------------------------------------------------                                  |\n    | |0 |SCALAR GROUP BY  |    |1       |4           |                                  |\n    | |1 |\u2514\u2500TABLE FULL SCAN|t1  |1       |4           |                                  |\n    | =================================================                                  |\n    | Outputs & filters:                                                                 |\n    | -------------------------------------                                              |\n    |   0 - output([T_FUN_COUNT_SUM(T_FUN_COUNT(*))]), filter(nil), rowset=16            |\n    |       group(nil), agg_func([T_FUN_COUNT_SUM(T_FUN_COUNT(*))])                      |\n    |   1 - output([T_FUN_COUNT(*)]), filter([t1.c1 = 1000]), rowset=16                  |\n    |       access([t1.c1]), partitions(p0)                                              |\n    |       is_index_back=false, is_global_index=false, filter_before_indexback[false],  |\n    |       range_key([t1.__pk_increment]), range(MIN ; MAX)always true                  |\n    +------------------------------------------------------------------------------------+\n    14 rows in set (0.033 sec)\n"})}),"\n",(0,o.jsx)(t.p,{children:"The rowset information in the plan is related to vectorized execution of the OceanBase Database execution engine. This article, the second one in the analytical processing (AP) performance series, answers the question and introduces the vectorized execution technology of OceanBase Database."}),"\n",(0,o.jsx)(t.h1,{id:"execution-engine-built-on-the-volcano-model",children:"Execution Engine Built on the Volcano Model"}),"\n",(0,o.jsx)(t.p,{children:"The vectorized execution engine is one of the key tools for boosting AP performance and played an important role in the championship of OceanBase Database in the 2021 TPC-H test. However, to better understand the vectorized execution engine, it is essential to learn about the Volcano model for conventional database execution engines."}),"\n",(0,o.jsxs)(t.p,{children:["The Volcano model, also known as the Iterator model, is the most renowned query execution model. It was first introduced in the 1990 paper ",(0,o.jsx)(t.a,{href:"https://paperhub.s3.amazonaws.com/dace52a42c07f7f8348b08dc2b186061.pdf",children:"Volcano\u2014An Extensible and Parallel Query Evaluation System"}),". Most conventional relational databases, including Oracle, MySQL, Db2, and SQL Server, are built on this model."]}),"\n",(0,o.jsx)(t.p,{children:"In the Volcano model, a query plan is divided into multiple operators. Each operator is an iterator that implements the next() interface, typically in the following three steps:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsx)(t.p,{children:"Calls the next() method of the child operator to obtain its calculation result."}),"\n"]}),"\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsx)(t.p,{children:"Performs the calculation operation corresponding to the current operator on the calculation result returned by the child operator to obtain a result."}),"\n"]}),"\n",(0,o.jsxs)(t.li,{children:["\n",(0,o.jsx)(t.p,{children:"Returns the result to the parent operator."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(t.blockquote,{children:["\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.strong,{children:"Note"}),":\n\xa0 \xa0The next() interface of operators in the paper is named ObOperator::get_next_row() in the code of OceanBase Database."]}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:"The Volcano model enables the query execution engine to elegantly assemble any operators without the need to consider the specific processing logic of each operator. During the execution of a query, nested get_next_row() methods in the query tree are called from the top down while data is pulled and processed from the bottom up. That is why the Volcano model is also called a pull-based model. To better understand the pull-based execution process of the Volcano model, let's continue with the preceding aggregation example:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-sql",children:"    select count(*) from t1 where c1 = 1000;\n"})}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"11",src:n(99889).A+"",width:"952",height:"1214"})}),"\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.strong,{children:"Note"}),":"]}),"\n",(0,o.jsx)(t.p,{children:"Each tuple in the preceding figure is a result row returned by a lower-level operator to a higher-level operator."}),"\n",(0,o.jsx)(t.p,{children:"The process in the preceding figure is described as follows:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Steps 1\u20123:"})," The AGGREGATE operator first calls the get_next_row() method so that lower-level operators can call the get_next_row() method of their child operators level by level."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Steps 4\u20126:"})," After obtaining data from the storage layer, the TABLE SCAN operator returns the result row to the FILTER operator. After calculating data based on the filter condition ",(0,o.jsx)(t.code,{children:"c1 = 1000"}),", the FILTER operator returns the result row to the AGGREGATE operator."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Step 7:"})," The AGGREGATE operator repeatedly calls the next() method to retrieve the required data, completes the aggregation, and returns the result."]}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:"If you disable vectorization in OceanBase Database, you can find execution plan trees similar to the one in the preceding figure."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-sql",children:"    -- Disable vectorization to force subsequent SQL queries to use the default single-row calculation mode, which is similar to that of the Volcano model.\n    alter system set _rowsets_enabled = false;\n    \n    -- You can observe that no rowset value exists in the following plan.\n    explain select count(*) from t1 where c1 = 1000;\n    +------------------------------------------------------------------------------------+\n    | Query Plan                                                                         |\n    +------------------------------------------------------------------------------------+\n    | =================================================                                  |\n    | |ID|OPERATOR         |NAME|EST.ROWS|EST.TIME(us)|                                  |\n    | -------------------------------------------------                                  |\n    | |0 |SCALAR GROUP BY  |    |1       |6           |                                  |\n    | |1 |\u2514\u2500TABLE FULL SCAN|t1  |1       |6           |                                  |\n    | =================================================                                  |\n    | Outputs & filters:                                                                 |\n    | -------------------------------------                                              |\n    |   0 - output([T_FUN_COUNT_SUM(T_FUN_COUNT(*))]), filter(nil)                       |\n    |       group(nil), agg_func([T_FUN_COUNT_SUM(T_FUN_COUNT(*))])                      |\n    |   1 - output([T_FUN_COUNT(*)]), filter([t1.c1 = 1000])                             |\n    |       access([t1.c1]), partitions(p0)                                              |\n    |       is_index_back=false, is_global_index=false, filter_before_indexback[false],  |\n    |       range_key([t1.__pk_increment]), range(MIN ; MAX)always true                  |\n    +------------------------------------------------------------------------------------+\n    14 rows in set (0.010 sec)\n"})}),"\n",(0,o.jsxs)(t.p,{children:["The plan in OceanBase Database contains only two operators and is simpler than that in the preceding figure. As every operator in OceanBase Database contains the functionality of the FILTER operator, no separate FILTER operator is needed. As shown in the preceding plan, the TABLE SCAN operator contains ",(0,o.jsx)(t.code,{children:"filter([t1.c1 = 1000])"}),". The SCALAR GROUP BY operator in the plan corresponds to the AGGREGATE operator in the figure. It performs aggregations in scenarios where GROUP BY is not used."]}),"\n",(0,o.jsx)(t.p,{children:"The Volcano model has clear processing logic, where operators are decoupled so that each operator focuses only on its own tasks. However, the model has two obvious drawbacks:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:"The virtual function get_next_row() is called for each row processed by every operator, and excessive calls can waste CPU resources. This issue is especially apparent in online analytical processing (OLAP) queries with a large data volume."}),"\n",(0,o.jsx)(t.li,{children:"Processing data row by row does not fully unleash the potential of modern CPUs."}),"\n"]}),"\n",(0,o.jsx)(t.h1,{id:"vectorized-execution-engine-and-its-benefits",children:"Vectorized Execution Engine and Its Benefits"}),"\n",(0,o.jsxs)(t.p,{children:["Vectorized models were first introduced in the paper ",(0,o.jsx)(t.a,{href:"http://cs.brown.edu/courses/cs227/archives/2008/Papers/ColumnStores/MonetDB.pdf",children:"MonetDB/X100: Hyper-Pipelining Query Execution"}),". Unlike the Volcano model which iterates data row by row, a vectorized model adopts batch iterations, allowing a batch of data to be passed between operators at a time. Due to their effective use of CPU resources and modern CPU features, vectorized models have been widely adopted in the design of modern database engines."]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"1",src:n(64338).A+"",width:"952",height:"1136"})}),"\n",(0,o.jsx)(t.p,{children:"As shown in the preceding figure, the vectorized model pulls data from the root node of an operator tree level by level in a similar way as the traditional Volcano model. The difference is that the vectorized engine calls the get_next_batch() function to pass a batch of data at a time and keeps the batch as compact as possible in the memory, rather than calling the get_next_row() function to pass one row at a time."}),"\n",(0,o.jsx)(t.h2,{id:"reduce-the-overhead-of-virtual-function-calls",children:"Reduce the Overhead of Virtual Function Calls"}),"\n",(0,o.jsx)(t.p,{children:"The vectorized engine drastically reduces the number of function calls. Assuming that you want to query a table with 100 million rows of data. In a database based on the Volcano model, each operator must call the get_next_row() function 100 million times to complete the query. If you use the vectorized engine and set the vector size to 1,024 rows, the number of calls to the get_next_batch() function for the same query, which is calculated by dividing 100 million by 1,024, is 97,657. This greatly decreases the number of virtual function calls and reduces CPU overhead."}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"2",src:n(29481).A+"",width:"2040",height:"1178"})}),"\n",(0,o.jsx)(t.p,{children:"In terms of the user question mentioned at the start of this article, the rowset in the plan indicates the number of rows in a batch or vector."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-sql",children:"    -- Enable vectorization.\n    alter system set _rowsets_enabled = true;\n    \n    -- Set the vector size to 16 rows.\n    alter system set _rowsets_max_rows = 16;\n    \n    -- The rowset information (rowset = 16) in the plan indicates that the vector size is 16 rows.\n    explain select count(*) from t1 where c1 = 1000;\n    +------------------------------------------------------------------------------------+\n    | Query Plan                                                                         |\n    +------------------------------------------------------------------------------------+\n    | =================================================                                  |\n    | |ID|OPERATOR         |NAME|EST.ROWS|EST.TIME(us)|                                  |\n    | -------------------------------------------------                                  |\n    | |0 |SCALAR GROUP BY  |    |1       |4           |                                  |\n    | |1 |\u2514\u2500TABLE FULL SCAN|t1  |1       |4           |                                  |\n    | =================================================                                  |\n    | Outputs & filters:                                                                 |\n    | -------------------------------------                                              |\n    |   0 - output([T_FUN_COUNT_SUM(T_FUN_COUNT(*))]), filter(nil), rowset=16            |\n    |       group(nil), agg_func([T_FUN_COUNT_SUM(T_FUN_COUNT(*))])                      |\n    |   1 - output([T_FUN_COUNT(*)]), filter([t1.c1 = 1000]), rowset=16                  |\n    |       access([t1.c1]), partitions(p0)                                              |\n    |       is_index_back=false, is_global_index=false, filter_before_indexback[false],  |\n    |       range_key([t1.__pk_increment]), range(MIN ; MAX)always true                  |\n    +------------------------------------------------------------------------------------+\n    14 rows in set (0.021 sec)\n"})}),"\n",(0,o.jsx)(t.h2,{id:"unleash-the-potential-of-modern-cpus",children:"Unleash the Potential of Modern CPUs"}),"\n",(0,o.jsx)(t.h3,{id:"compact-data-layout-for-better-cache-efficiency",children:"Compact data layout for better cache efficiency"}),"\n",(0,o.jsxs)(t.p,{children:["During vectorized execution, OceanBase Database compactly stores batch data in memory, with intermediate data organized in columns. For example, if a batch contains 256 rows, the 256 rows of data of the c1 column are stored contiguously in memory, followed by those of the c2 column, which are also stored contiguously. For the ",(0,o.jsx)(t.code,{children:"concat(c1, c2)"})," expression, calculation is performed on the 256 rows at a time, with the result stored in the memory space pre-allocated to the expression."]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"3",src:n(74656).A+"",width:"463",height:"468"})}),"\n",(0,o.jsx)(t.p,{children:"Since the intermediate data is contiguous, the CPU can quickly load the data into the L2 cache through the prefetch instruction to reduce memory stalls and improve CPU utilization. Inside an operator function, data is processed in batches rather than row by row, enhancing the efficiency of data cache (DCache) and instruction cache (ICache) in the CPU while reducing cache misses."}),"\n",(0,o.jsx)(t.h3,{id:"reduced-impact-of-branch-mispredictions-on-the-cpu-pipeline",children:"Reduced impact of branch mispredictions on the CPU pipeline"}),"\n",(0,o.jsxs)(t.p,{children:["The paper ",(0,o.jsx)(t.a,{href:"http://www.cs.cmu.edu/afs/cs/academic/class/15740-f03/public/doc/discussions/uniprocessors/databases/wisc_vldb99.pdf",children:"DBMSs On A Modern Processor: Where Does Time Go?"})," discusses the impact of branch mispredictions on database performance. Branch mispredictions have a serious impact on the database performance because the CPU halts the execution of an instruction stream and refreshes the pipeline upon a misprediction. The paper ",(0,o.jsx)(t.a,{href:"https://15721.courses.cs.cmu.edu/spring2018/papers/03-compilation/p1231-raducanu.pdf",children:"Micro Adaptivity in Vectorwise"})," released on the 2013 ACM SIGMOD Conference on Management of Data (SIGMOD'13) also elaborates on the execution efficiency of branching at different levels of selectivity. A figure is provided below for your information."]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{src:"https://gw.alipayobjects.com/zos/oceanbase/e4bdecb0-8536-4713-8b30-8d9218007eb2/image/2022-09-28/5ca51137-a512-4cb7-9244-502b4b6aadb7.png",alt:"4"})}),"\n",(0,o.jsx)(t.p,{children:"The logic of the SQL engine of a database is complicated. Therefore, conditionals appear frequently in the Volcano model."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-c++",children:"    // The following pseudocode outlines the single-row calculation process, where the IF statement is executed 256 times to process 256 rows of data:\n    for (auto row_no : 256) {\n      get_next_row() {\n        if (A) {\n          eval_func_A();\n        } else if (B) {\n          eval_func_B();\n        }\n      }\n    }\n"})}),"\n",(0,o.jsx)(t.p,{children:"In vectorized execution, conditionals are minimized within operators and expressions. For example, no IF statement is within any FOR loops, thus protecting the CPU pipeline from branch mispredictions and greatly improving CPU capabilities."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-c++",children:"    // The following pseudocode outlines the vectorized calculation process, where the IF statement is executed only once to process 256 rows of data:\n    get_next_batch() {\n      if (A) {\n        for (auto row_no : 256) {\n          eval_func_A();\n        }\n      } else if (B) {\n          for (auto row_no : 256) {\n          eval_func_B();\n        }\n      }\n    }\n"})}),"\n",(0,o.jsx)(t.h3,{id:"accelerated-computation-through-simd-instructions",children:"Accelerated computation through SIMD instructions"}),"\n",(0,o.jsx)(t.p,{children:"The vectorized engine handles contiguous data in the memory, and hence can easily load a batch of data into a vector register. It then sends a single instruction, multiple data (SIMD) instruction to perform vector computation instead of using the traditional scalar algorithm. The SIMD instruction enables the CPU to perform the same computation on the batch of data in parallel, reducing the number of CPU cycles required for processing the data."}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"5",src:n(55222).A+"",width:"2096",height:"976"})}),"\n",(0,o.jsx)(t.p,{children:"The right side of the preceding figure shows a typical SIMD computation, where two sets of four contiguous data elements are processed in parallel. The CPU simultaneously performs the same operation on each pair of data elements (A1 and B1, A2 and B2, A3 and B3, and A4 and B4) based on the SIMD instruction. The results of the four parallel operations are also stored contiguously."}),"\n",(0,o.jsx)(t.p,{children:"If a processor supports 4-element SIMD multiplication, it has vector registers that can simultaneously store four integers. As OceanBase Database stores data contiguously during vectorized execution, SIMD code can be written as follows:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Load data (_mm_loadu_si128):"})," First, load the vector with the A1, A2, A3, and A4 elements and the vector with the B1, B2, B3, and B4 elements into two SIMD registers."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Perform SIMD multiplication (_mm_mullo_epi32):"})," Next, use the SIMD multiplication instruction to simultaneously multiply all elements in both registers."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Store data (_mm_storeu_si128):"})," Last, store the results from the SIMD registers in the allocated memory to form the result vector."]}),"\n"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-c++",children:"    // The sample C++ pseudocode based on Streaming SIMD Extensions (SSE) for x86 performs element-wise multiplication of integer vectors by using the SIMD technology.\n    #include <immintrin.h> // Include the SSE header file.\n    \n    // Use the function to perform element-wise multiplication of two integer vectors.\n    void simdIntVectorMultiply(const int* vec1, const int* vec2, int* result, size_t length) {\n      // As SSE registers process four 32-bit integers at a time, make sure that the vector length is a multiple of four.\n      assert(length % 4 == 0);\n    \n      // Execute the loop that uses SSE instructions for optimization.\n        for (size_t i = 0; i < length; i += 4) {\n    \n        // Load four integers into the 128-bit XMM register.\n        __m128i vec1_simd = _mm_loadu_si128(reinterpret_cast<const __m128i*>(vec1 + i));\n        __m128i vec2_simd = _mm_loadu_si128(reinterpret_cast<const __m128i*>(vec2 + i));\n        \n        // Perform vector multiplication.\n        __m128i product_simd = _mm_mullo_epi32(vec1_simd, vec2_simd);\n        \n        // Store the results in the memory.\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result + i), product_simd);\n      }\n    }\n"})}),"\n",(0,o.jsx)(t.h1,{id:"tpc-h-performance-test",children:"TPC-H Performance Test"}),"\n",(0,o.jsx)(t.p,{children:"In the TPC-H test based on the TPC-H 30 TB dataset on OceanBase Database, vectorized execution outperforms single-row execution by 2.48 times. For compute-intensive Q1 queries, performance is improved by over 10 times."}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"1",src:n(32732).A+"",width:"864",height:"462"})}),"\n",(0,o.jsx)(t.p,{children:"In OceanBase Database V4.3, the OceanBase team has optimized and restructured the vectorized execution engine, which has been supported since OceanBase Database V3.x."}),"\n",(0,o.jsx)(t.h1,{id:"summary",children:"Summary"}),"\n",(0,o.jsxs)(t.p,{children:["This article was inspired by the question about the meaning of ",(0,o.jsx)(t.code,{children:"rowset=16"})," in a plan, which was raised during the seventh live streaming of ",(0,o.jsx)(t.a,{href:"https://youtube.com/live/3iwhQ4lAqgg",children:"OceanBase DBA: From Basics to Practices"}),". After answering the question, this article also briefly introduces the vectorized execution technology of OceanBase Database."]}),"\n",(0,o.jsx)(t.p,{children:"I hope both database administrators (DBAs) and kernel developers find this helpful. For any questions, feel free to leave a comment."})]})}function d(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},64338:(e,t,n)=>{n.d(t,{A:()=>o});const o=n.p+"assets/images/1-3185765510904fa6054a5ddccf3ec59e.png"},99889:(e,t,n)=>{n.d(t,{A:()=>o});const o=n.p+"assets/images/11-559e37cc7fab365427ace418b55d9e9c.png"},29481:(e,t,n)=>{n.d(t,{A:()=>o});const o=n.p+"assets/images/2-161ff393aae3787e17d1acf23baff03a.png"},74656:(e,t,n)=>{n.d(t,{A:()=>o});const o=n.p+"assets/images/3-266025943e222a6efc5567aae556b1dd.png"},55222:(e,t,n)=>{n.d(t,{A:()=>o});const o=n.p+"assets/images/5-b7dea0ded49b4e79c6a40f6e931f835b.png"},32732:(e,t,n)=>{n.d(t,{A:()=>o});const o=n.p+"assets/images/image-ca3992f871a9290b64340b135d6c2116.png"},28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>r});var o=n(96540);const s={},i=o.createContext(s);function a(e){const t=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(i.Provider,{value:t},e.children)}}}]);