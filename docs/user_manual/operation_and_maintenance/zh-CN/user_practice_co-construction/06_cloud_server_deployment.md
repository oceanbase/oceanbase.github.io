---
title: 云服务器上 OceanBase 部署实践
weight: 6
---
> 本文作者：庆涛（OceanBase 社区论坛账号：@obpilot）

OB 官网文档对生产环境 OB 部署环境服务器给出指导配置，大部分企业客户都能够接受这个配置要求。少数客户的 IT 基础设施使用了超融合技术或者云的虚拟化技术，初次使用 OB 的时候不能给出物理服务器。还有一部分原因是有些客户出于 POC 或者试用看看的目的。

本文主要分享云服务器或虚拟机环下境部署 OB 一些实践经验。

## 云服务器环境分析
云服务器的原理是云厂商将一批高配物理服务器的资源聚合为一个资源池，然后从中分出不同规格的云虚拟机。

大部分云服务器并不独占物理服务器的资源。云服务器的资源包括 CPU、内存、存储和网络。这些资源都有明确的规格定义，云厂商使用资源隔离技术去保障每个云服务器的资源的 QOS。

云服务器存储使用的多是云盘（只有阿里云少数高配云服务器配有本地 NVMe SSD）。云盘容量可以在线扩展，不同容量和类型的云盘对应的 IOPS、吞吐量和读写耗时都不一样。这其中读写耗时对数据库性能影响最大。

在部署 OB 之前建议先使用 FIO 对云盘性能进行测试。测试场景建议包括 4KB、2MB 的顺序写 IO 和 16K 的随机读 IO 。FIO 测试期间观察云盘读写 IOPS 、吞吐量(`bw`和时延（`latency`)。使用 `iostat` 命令也可以检测 IO i性能，重点观察 `rawait`、`wawait`和 `svctm` 。一般来说云盘在一定并发下的 `svctm` 如果超过`1ms`，那性能就离 NVMe SSD 性能差很多，部署 OB 后高并发读写场景下性能大概率也不会好。

云盘还有个能力就是可以在线扩容和缩容（重点是扩容）。OB 的部署对服务器需求里存储使用的是本地文件系统，OB 倾向于一次性锁定存储空间资源。所以云盘扩容后，新增的容量要体现在 OB 里还需要在磁盘分区、文件系统上做一些相应的扩容命令。

云服务器的网卡一般只有一块，除非你额外配置多个网卡。这个网卡也是虚拟网卡，云的技术会保证这个虚拟网卡链路的高可用，这点不像物理服务器要做双网口绑定技术。云服务器的虚拟网卡的带宽是有限制的，不一定有 10Gb，这点可以通过网络工具 `iperf` 压测网络和`iftop`监测网卡上下行带宽。如果云服务器的网卡带宽太低，也可能会制约 OB 服务器之间数据传输性能。OB 节点进程启动时也会判断网卡的速率，依赖的是操作系统的技术。在物理服务器上，少数国产系统或者少数不常用的网卡可能都会导致 OB 判断网卡速率失败。这个概率比较低。但是在云服务器上，虽然操作系统被 OB 兼容，虚拟网卡的速率操作系统却可能无法给出，导致 OB 判断网卡速率失败，OB 会选择按默认带宽（`1000Mb`）来计算。这个会限制 OB 节点传输时对网卡吞吐的利用率。

云服务器还有热迁移的能力（至少阿里云一早就有这个能力），指的是在线迁移云服务器到其他物理服务器上。云服务器的数据都在云盘上，所以换一个云服务器没什么问题。这里最关键的是 IP 不要变，因为 OB 不能接受节点 IP 变化。云服务器如果更换了 IP 对 OB 意味着是老 IP 节点掉线，新 IP 节点不被集群承认（需要清空数据，走新节点初始化流程加入到 OB 集群中）。

超融合或者 VMWare 等虚拟化平台做出的虚拟机也可能有上面类似的特点。

## 云服务器部署建议
OB 部署环境不推荐云服务器，主要是为性能考虑。不过性能是跟业务需求期望相关。想在云盘上跑出本地盘（NVMe SSD）那样的性能，这个是妄想。撇开这个妄想外，OB 部署在云服务器上也是可以的。OB 技术本身并不限制存储的类型，更何况公有云 OB 也是运行在云服务器上。只要我们规避上面云盘和云虚拟网卡的那个问题就行。

### 云盘性能提升方法
评估云盘的资源时大部分用户首先是从容量需求方面考虑，这个最容易计算。此外就是 IOPS 和吞吐，很多人都说不出业务需要多少数据库 IOPS 和吞吐。这个是正常的，即使在以前传统数据库规划里，这个 IOPS 评估也很难准确给出，都是拍脑袋。业务 SQL 写的好，大部分数据读写都走索引，数据库 IOPS 需求就低，带宽也低；SQL 写的不好，都是全表扫描，IOPS 和带宽需求就高。云盘一般有几万的 IOPS，这是够还是不够呢？实际使用经验表明这个够用了，云盘的问题并不出现在 IOPS 和吞吐上，而是 IO 时延上。

云盘的单块盘的小 IO 的时延在高并发时会很差，表现的跟机械盘一样（云厂商看到这个可能会有意见，也可能是因为我看到的客户买的云盘都不好）。但是如果购买多块云盘，使用 LVM 技术聚合在一起然后划分逻辑盘（LV）使用时，在相同的高并发场景下，单块逻辑盘小 IO 的平均时延却可以稳定在 `1ms` 以下，这个时延水平就比 `NVMe SSD` 差一小点，但是比机械盘好很多。这样使用的效果也能做到很好。

![示例图片](/img/user_manual/operation_and_maintenance/zh-CN/user_practice_co-construction/06_cloud_server_deployment/001.png)

所以，原本分配了1块 4TB 的云盘，可以调整为分配 10块 400G 的云盘。当然云厂商可能会说这种做法意义不大，云盘本来就是在后端将 IO 打散到多块物理盘上。这个说法也有一定道理，但是单盘性能就是有问题这个云厂商解决不了。而采用多块云盘使用 LVM 聚合在一起使用后，性能确实比单盘要好很多。

这里面盘的数量就很重要，容量不是首要考虑因素。建议盘的数量在 10 以上，单盘容量也不要少于 400G 。不过有些云厂商对云服务器能挂载的云盘数量也有上限，那就贴近它那个上限去申请。这样才可能最大化发挥云盘的性能。

多块云盘使用 LVM 技术时也有讲究。聚合为一个 VG 就可以，然后从 VG 里分出两个 LV ，分别给 OB 的事务日志和数据文件。 OB 的软件目录不要跟这个云盘混用。这里先定事务日志 LV 的大小，基本上是内存大小的 3512G ,那够生产环境用了。否则后期内存如果还扩容，需要考虑这个事务日志空间是否够。所以这里事务日志空间的大小也不要太小（在内存很小的时候）.除去给事务日志目录(`/data/log1`） ，VG 剩下的空间就都给数据文件目录（`/data/log1`）。

这里划分 LV 的时候有个关键点就是所有的 LV 都要是条带卷（`stripped`），而不是默认的线性卷，这样才可能做到将 IO 分散到所有的云盘上。这也是逻辑盘时延水平很低的关键原因。

下面是两个示例。`-i` 是 `pv`的数量（也是所用云盘的数量）,`-I` 是条带大小，官网给出的日志盘的条带是 `128B`，数据盘是 `1024B` 。这里条带大小多少最优我没有做过测试。

```bash
lvcreate -i 12 -I 1024 -l 100%free -n lv_oblog vgob
lvcreate -i 12 -I 128 -L 2048G -n lv_obdata vgob
```

LVM 划分正确的情况下，看云服务器的本地文件系统，大概类似下图。

![示例图片](/img/user_manual/operation_and_maintenance/zh-CN/user_practice_co-construction/06_cloud_server_deployment/002.png)

## 网卡速率设置
OB 节点进程 `observer` 启动时会优先从 `{WORK_DIR}/etc/nic.rate.config` 配置文件中获取网卡速率的设置，如果该配置文件不存在，OceanBase 会尝试从操作系统查询网卡速率。 如果以上两者都无法获取到网卡速率时，则会采用默认配置。该配置往往因为和实际不符，导致迁移复制的时候网速很慢，主备库延迟过高，进而产生严重问题。OB 运行日志如果频繁提示类似下面信息，就属于这种情形。

```bash
[2024-10-08 08:58:04.565648] WDIAG [SERVER] get_network_speed_from_sysfs (ob_server.cpp:2692) [3760825][ServerGTimer][T0][Y0-0000000000000000-0-0] [lt=4][errcode=-4000] cannot get Ethernet speed, use default(tmp_ret=0, devname=“lo”)
[2024-10-08 08:58:04.565654] WDIAG [SERVER] runTimerTask (ob_server.cpp:3214) [3760825][ServerGTimer][T0][Y0-0000000000000000-0-0] [lt=5][errcode=-4000] ObRefreshNetworkSpeedTask reload bandwidth throttle limit failed(ret=-4000, ret=“OB_ERROR”)
```

有多种解决办法，最简单的就是直接构建一个 `nic.rate.config` 文件。文件内容具体格式为 `$(IF_NAME)=$(SPEED)`，SPEED 可以是数字 + 单位，也可以是纯数字，纯数字情况下采用默认单位 Mbps。配置文件示例（任选一种）：

```bash
bond0=10000
bond0=10000b
bond0=10000bit
bond0=10000k
bond0=10000kb
bond0=10000kbit
bond0=10000m
bond0=10000mb
bond0=10000mbit
bond0=100g
bond0=100gb
bond0=100gbit
```

配置后重启 OBServer 进程，运行日志会有下面提示就表示成功。

```bash
[2021-06-10 11:10:49.397019] INFO  [SERVER] ob_server.cpp:2018 [72580][0][Y0-0000000000000000-0-0] [lt=8] [dc=0] succeed to init_bandwidth_throttle(sys_bkgd_net_percentage_=60, network_speed=1310720000, rate=786432000):
[2021-06-10 11:14:44.905396] INFO  [SERVER] ob_server.cpp:2385 [72582][0][Y0-0000000000000000-0-0] [lt=11] [dc=0] network speed changed(from=1310720000, to=1048576000)
[2021-06-10 11:14:44.905421] INFO  [SERVER] ob_server.cpp:2055 [72582][0][Y0-0000000000000000-0-0] [lt=3] [dc=0] succeed to reload_bandwidth_throttle_limit(old_percentage=60, new_percentage=60, network_speed=1048576000, rate=629145600)
```

当然这里注意，云服务器的虚拟网卡带宽不一定有 `10Gb`，具体多少要问云厂商。配置的值要贴合实际值。

还需要了解的是， OB 传输数据的时候会自己约束后台网络带宽使用比例，参数是：`sys_bkgd_net_percentage`，默认值是 `60`（百分比）。

## 总结
云服务器跟普通服务器的区别就是其资源是虚拟化出来，都有相应的配额。特别是云盘有 IOPS和带宽限制，导致时延水平在高并发下会很差。通过 LVM 将多块云盘合并在一起使用时能有效降低高并发下 OB 数据盘的读写时延，从而提升 OB 的读写性能（缩小跟本地 NVMe SSD 盘的性能差距）。使用超融合技术分配的云服务器也可以采取类似方法来尽可能提升 OB 的读写性能。云服务器的网卡速率可能不能被 OB 识别到，通过写配置文件方式直接告诉 OB 当前网卡速率即可。

更多阅读参考：

- [OB 4.2 SQL 性能抖动诊断案例分享](https://mp.weixin.qq.com/s/wT_qrEHqgOx1SHFpGy9K6g)