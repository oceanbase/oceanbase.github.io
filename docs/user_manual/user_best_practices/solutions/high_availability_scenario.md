---
title: 异地多活场景
weight: 1
---

> 关键字：高可用 | 多地多活 | 单元化丨极致弹性
>
> 通过 OceanBase 强大的异地部署能力和灵活的副本变换以及负载均衡能力，帮助企业在关键核心场景中进行单元化部署，结合上层调度系统实现任意单元流量在城市和机房间的灵活调拨与切换，实现单元内故障不影响全局服务的效果。

<a name="PxBUG"></a>

# 异地多活场景

&emsp;&emsp;各行各业随着业务数据量和并发访问量呈现指数级的增长，大量业务需要具备城市级别的容灾能力以满足监管的要求，进而达到支撑多种应用系统（例如银行业的存贷汇核心系统）的管理与使用需求。
<a name="hF6SV"></a>

# 行业现状与挑战

- 数据丢失风险高：传统 IT 系统高可用系统的实现主要是以主备的方式进行部署， 这种方案有着非常广泛的应用，该方案虽然已经过长时间的验证，但仍然无法很好解决例如故障发生后切换数据不丢失的需求。
- 故障恢复时效差：传统的双活架构，由于异步复制机制问题，主中心故障发生后不敢切、不能切的情况时有发生。
- 故障爆炸半径大：由于传统架构下业务系统整体强耦合，数据库层面也经常发生单点故障影响全站用户的情况，严重影响业务连续性。
  <a name="cUfb6"></a>

# 解决方案

<a name="FAjh0"></a>

## 方案描述

- 利用 OceanBase 的多地多副本架构，以及高效的 Paxos 一致性协议工程实现，将数据库集群进行“三地五中心”部署，数据副本分别存储在同城和异地，实现异地容灾。
- 在“三地五中心”架构下，OceanBase 可以实现单元化部署，配合应用微服务中间件改造（例如蚂蚁分布式中间件 SOFA），实现单个业务单元发生故障后，业务影响爆炸半径最低可降至 1%，并且即使遇到城市级故障也能在 1 分钟内自动恢复，同时保证零数据丢失。

![image.png](/static/img/solutions/high_availability_scenario/1.png)
<a name="frjM0"></a>

## 方案优势

- “三地五中心”部署 ：任意机房/ 城市级故障 1 分钟内自动恢复，零数据丢失。
- 单元化架构设计：每个单元的业务和数据自包含，可以结合上层调度系统实现任意单元流量在城市和机房间的灵活调拨与切换，单元内故障不影响全局服务。
- 极致弹性：在业务高峰例如大促期间，通过弹出任意比例的单元流量到低负载的机房，实现数据库容量的跨云弹性伸缩。
- 强劲吞吐能力：满足海量数据处理的需要， 结合 OceanBase 原生分布式的并发读写能力，大幅提高批处理性能吞吐，每个批次查询超过百万条，每批次可并发处理百亿账户, 数据量达到 PB 级。
  <a name="LNZAz"></a>

# OceanBase 高可用技术介绍

&emsp;&emsp;这一部分会介绍 OceanBase 数据库如何保证高可用，以及一些常见问题。
<a name="bkuWY"></a>

## 名词解释

| **名词** | **解释**                                                                                                                                                                                                                                                                   |
| -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| RTO      | 系统发生故障（例如掉电、宕机、进程误杀、系统 Crash、机房故障、灾难等）以后，从故障发生影响数据对外提供服务的时间点，到数据恢复可用，可以对外提供服务的时间点，这两点之间的时间段，叫做服务不可用时间。RTO 即服务恢复时间目标，主要指的是业务能够容忍的最大服务不可用时间。 |
| RPO      | 系统在发生故障（例如掉电、宕机、进程误杀、系统 Crash、机房故障、灾难等）后再恢复可用。系统在恢复可用之后的数据与故障发生前的数据差别就是数据丢失量。RPO 即是数据恢复目标，主要指的是业务系统所能容忍的数据丢失量。                                                         |

<a name="vQsab"></a>

## 高可用概述

&emsp;&emsp;IT 系统的高可用（High Availability）指的是系统无中断地执行其功能的能力，代表系统的可用性程度。高可用是在系统设计、工程实践、产品能力各个环节上的综合作用，保证业务的持续连续性。而保障系统高可用最重要的焦点就是尽量消除或者冗余单点故障（Single Point of Failure）并且在单点或者系统出现不可用之后提供急速恢复的能力。<br />
&emsp;&emsp;故障恢复是指当系统出现短时可恢复故障时，系统恢复可用性、恢复数据服务的一系列过程。灾难恢复是指当机房甚至城市出现灾难或故障事件导致机房或者该城市内机房长时间的故障，不能立即恢复可用时，整个系统恢复可用的一系列过程。企业级应用为了保证业务的连续可用，通常会对系统的可用性有很高的要求，需要保证在故障或者灾难发生时，系统的 RTO 尽量低，RPO 尽量接近 0。<br />
&emsp;&emsp;高可用是分布式系统设计中必须考虑的因素，OceanBase 数据库作为一款原生的分布式数据库能够对外提供一致的、高可用的数据服务。OceanBase 数据库的事务一致性，存储持久化保证在出现 OBServer 退出重启情况下能够恢复到重启前相同的数据和状态。另外，OceanBase 数据库的备份恢复、主备库解决方案也保证了在不同场景下 OceanBase 数据库的高可用能力。
<a name="HvF7S"></a>

## OceanBase 用于保证高可用的相关产品能力

<a name="Xybpi"></a>

### OceanBase 数据库分布式选举

<a name="BFPqV"></a>

#### 解决的故障场景

&emsp;&emsp;OceanBase 数据库少数派因为各种原因造成不可用的故障恢复：例如，3 副本 3 台 OBServer 组成的 OceanBase 集群中 1 台 OBServer 宕机、退出；部署在不同机房的少数派副本机房故障；部署在不同城市的少数派副本城市发生灾难。
<a name="MnFGk"></a>

#### 技术原理

&emsp;&emsp;OceanBase 数据库的选举模块保证选举出唯一的主副本对外提供数据服务。同时，通过 Paxos 协议实现了多数派 Clog 强一致同步持久化，在 Paxos 组中任意少数派副本发生故障的情况下，剩下的多数派副本都能保证有最新的 Clog，因此就能避免个别硬件故障带来的数据损失，保证了数据可靠性。当整个 OceanBase 集群中的少数派出现故障时，如果是非 Leader 副本的少数派不可用，不会影响系统的可用性和数据库的性能。如果少数派故障中有 Leader 副本，OceanBase 数据库能够保证从剩余副本选出唯一新主提供数据服务。取决于 OceanBase 集群的搭建部署模式，如果 OBServer 的多个 Zone 分布在不同机房、不同城市时，通过 OceanBase 集群的分布式选举加上 OceanBase Paxos Clog 同步就可以达到跨机房高可用或者是跨城市的高可用方案。 更多信息请参见 [OceanBase 数据库选举](https://www.oceanbase.com/knowledge-base/oceanbase-database-20000000075)。

<a name="doYGD"></a>

### OceanBase Clog 及存储引擎

<a name="MNxoG"></a>

#### 解决的故障场景

&emsp;&emsp;OBServer 多数派故障需要重启；维护计划内的 OBServer 集群重启。
<a name="a3AQc"></a>

#### 技术原理

&emsp;&emsp;OceanBase 数据库的存储引擎将基线数据保存在 SSTable，增量数据保存在 MemTable 中。 OceanBase Clog 是数据的 Redo Log。当有数据修改发生时，请求会到达 Leader 副本所在节点，将数据更改请求更新到 MemTable，事务进行提交，Leader 更新本地 Clog 并通过 Paxos 协议将日志同步写到其他 Follower 的副本节点上。当有多数派节点的日志写盘成功后，数据修改成功，返回客户端。Follower 副本会将 Clog 回放到本地 MemTable 上提供弱一致读服务。<br />
&emsp;&emsp;当 MemTable 到达阈值后，会触发冻结和转储，持久到 SSTable 层。而此时的 Clog 回放位点会推进，类似于做了 Checkpoint。在 OBServer 重启时，能够做到从 SSTable 中源数据还原、更新到最新信息。然后，从分区的元信息中获取 Clog 的回放位点，开始回放 Clog 日志生成 MemTable 中。至此，OceanBase 数据库可以将磁盘中的持久化信息恢复到宕机前的状态，保证了数据的完整性。 当 OceanBase 数据库由于故障（软件退出、异常重启、断电、机器故障等）重启或者计划内停机维护重启时，OBServer 能够在启动中恢复，将 OBServer Store 目录下的日志和数据还原到内存中，将进程的状态恢复到宕机前的状态。如果多数派副本故障并需要重启，数据服务会发生中断，OceanBase 数据库保证在多数派宕机重启后数据可以完全恢复到宕机之前。<br />
&emsp;&emsp;OBServer 对于多数派宕机重启也进行了进一步的优化处理，加速数据副本恢复加载到 MemTable 的速度，尽快对外提供数据服务。在整个集群重启的情景下，需要将磁盘上最新的 Clog 重新回放到 MemTable 里后对外提供数据服务，在 OceanBase 数据库能够重新恢复数据服务时，数据恢复到集群重启前。

<a name="Ylzht"></a>

### OceanBase 数据库备份恢复

<a name="eRJGF"></a>

#### 解决的故障场景

&emsp;&emsp;OceanBase 数据库出现数据损坏、节点 Crash 或者集群故障，OceanBase 数据库可以从备份的基线数据和 Clog 备份中恢复。
<a name="QQ6MS"></a>

#### 技术原理

&emsp;&emsp;当 OceanBase 数据库出现数据损坏、节点 Crash 或者集群故障，OceanBase 数据库可以从备份的基线数据和 Clog 备份中恢复。

<a name="Xrs7T"></a>

### OceanBase 数据库主备库

<a name="lJYQe"></a>

#### 解决的故障场景

&emsp;&emsp;机房级的故障或城市级的灾难恢复。
<a name="JpYW9"></a>

#### 技术原理

&emsp;&emsp;OceanBase 数据库也支持传统的主备库架构。 OceanBase 集群的多副本机制可以提供丰富的容灾能力，在机器级、机房级、城市级故障情况下，可以实现自动切换，并且不丢数据，RPO = 0。<br /> &emsp;&emsp;OceanBase 数据库的主备库高可用架构是 OceanBase 数据库高可用能力的重要补充。当主集群出现计划内或计划外（多数派副本故障）的不可用情况时，备集群可以接管服务，并且提供无损切换（RPO = 0）和有损切换（RPO > 0）两种容灾能力，最大限度降低服务停机时间。 OceanBase 数据库支持创建、维护、管理和监控一个或多个备集群。备集群是生产库数据的热备份。管理员可以选择将资源密集型的报表操作分配到备集群，以便提高系统的性能和资源利用率。

<a name="xm8c0"></a>

## OceanBase 高可用相关问题答疑

<a name="UXqv3"></a>

### 分布式部署时，如果少数派宕机会发生什么？多快可以恢复？

&emsp;&emsp;OceanBase 数据库利用了基于 Paxos 分布式一致性协议保证了在任一时刻只有当多数派副本达成一致时，才能推选一个 Leader, 保证主副本的唯一性来对外提供数据服务。如果正在提供服务的 Leader 副本遇到故障而无法继续提供服务，只要其余的 Follower 副本满足多数派并且达成一致，就可以推选一个新的 Leader 来接管服务，而正在提供服务的 Leader 自己无法满足多数派条件，将自动失去 Leader 的资格。当 Leader 副本出现故障时，Follower 能在多长时间内感知到 Leader 的故障并推选出新的 Leader，这个时间直接决定了 RTO 的大小。<br />
&emsp;&emsp;OceanBase 数据库的选举模型是依赖时钟的选举方案， 同个分区的多个全能副本（及日志型副本）在一个选举周期内进行预投票, 投票，计票广播以及结束投票，最终敲定唯一的主副本。当选举成功后，每个副本会签订认定 Leader 的租约（Lease）。在租约过期前，Leader 会不断发起连任，正常情况下能够一直连任成功。如果 Leader 没有连任成功，在租约到期后会周期性的发起无主选举，保证副本的高可用。 在三副本（全能副本）场景下，当一个副本出现异常（例如该节点机器故障，OBServer 下线等单点故障），OceanBase 数据库选举模块能够做到：如果原主副本连任成功，原主可以连续提供主副本服务；如果原主副本下线，Leader 连任失败，OceanBase 数据库进行无主选举，新主上任的时间在 30s 内完成。<br />
&emsp;&emsp;同时，OceanBase 数据库通过 Paxos 协议实现了多数派 Clog 强一致同步持久化，在 Paxos 组中任意少数派副本发生故障的情况下，剩下的多数派副本都能保证有最新的 Clog，因此就能避免个别硬件故障带来的数据损失，保证了数据可靠性。
<a name="mwbQ5"></a>

### 分布式选举如何避免脑裂（Split Brain) 问题？

&emsp;&emsp;在高可用方案中，一个经典的需要面对的问题就是脑裂问题： 如果两个数据副本因为网络问题互相不知晓对方的状态，并且分别认为各自应当作为主副本提供数据服务，那么这时候就会出现典型的脑裂现象，最后会导致系统混乱，数据损坏。 <br />
&emsp;&emsp;OceanBase 数据库利用了 Paxos 协议中的多数派共识机制来保证数据的可靠性以及主副本的唯一性：在任一时刻只有多数派副本达成一致时，才能推选一个 Leader。如果正在提供服务的 Leader 副本遇到故障而无法继续提供服务，只要其余的 Follower 副本满足多数派并且达成一致，他们就可以推选一个新的 Leader 来接管服务，而正在提供服务的 Leader 自己无法满足多数派条件，将自动失去 Leader 的资格。<br />
&emsp;&emsp;因此，我们可以看到 OceanBase 数据库的分布式选举在高可用方面有明显的优势：从理论基础上就保证了任一时刻至多有一个 Leader，彻底杜绝了脑裂的情况。由于不再担心脑裂，当 Leader 故障而无法提供服务时，Follower 可以自动触发选举来产生新的 Leader 并接管服务，全程无须人工介入。这样一来，不但从根本上解决了脑裂的问题，还可以利用自动重新选举大大缩短 RTO。当然，这里面还有一个很重要的因素，那就是 Leader 出现故障时，Follower 能在多长时间内感知到 Leader 的故障并推选出新的 Leader，这个时间直接决定了 RTO 的大小。
<a name="Rt0Cy"></a>

### 分布式部署时，当多数派故障的时候是否依旧可以服务？

&emsp;&emsp;当 OceanBase 数据库中多数派故障，那么对应的分区是无法对外提供数据服务的。为了保证数据的高可用，在整个系统架构设计和运维时，还应当考虑和优化两个副本不相关的连续故障发生的时间。最终保证 MTTF（Mean Time To Failure）应当相较于故障的修复时间 MTTR（Mean Time To Repair）足够的小，以保证整个数据服务的高可用性。当 OceanBase 集群多数派失败的时候，在保留问题分析需要的信息和日志的前提下，应当采取应急措施将集群尽快恢复可用。
<a name="G7eHq"></a>

### 副本自动补齐功能是如何工作的？副本自动补齐是否能够保证即使在节点宕机的情况下，相应的分区副本依旧齐全？

&emsp;&emsp;当 observer 进程异常终止，若终止时间小于 server_permanent_offline_time，则不作处理，此时有些分区的副本数只有 2 个了（三副本情况下）；当终止时间超过 server_permanent_offline_time 时，则对该 Server 做永久下线处理，OceanBase 数据库会在同个 Zone 的其他 Server 开辟区域（资源充裕的 Server），维持副本个数。当有足够的资源时就会发起 Unit 迁移。
<a name="Gf3Nd"></a>

### 是否支持多机房多地区的部署方式？在部署的时候有哪些基础设施的要求？应该如何选择方案？

&emsp;&emsp;在 OceanBase 集群的搭建中，允许将多个副本（节点）分散到多个机房、多个城市中，跨机房/跨城市实现 Paxos Group。在选择多机房、多城市集群架构时，通常也是为了获取机房级容灾甚至是城市级容灾的能力。从技术上来说，只要支撑 OceanBase 数据库不同副本节点的基础设施足够好，在合理的副本分布下，OceanBase 数据库就可以将数据分布在不同的节点上对外提供数据服务。<br />
&emsp;&emsp;通常情况下，OceanBase 数据库集群对基础设施的基本要求如下：

- 节点间的网络延迟时间（单向延迟）最好保证在 50ms 以内，最差（单向延迟）也要在 100ms 以内。
- OBServer 环境中的时钟服务应该保证时钟同步的偏差在 100ms 以内，且不能够产生 1s 及以上的时钟跳变。

&emsp;&emsp;通常情况下，进行多机房/多城市的的部署方案最重要的是考虑业务和各方面产生的架构需求。因为多机房/多城市的部署方式也会直接带来整个系统成本的大幅提高（例如，跨城市的网络专线部署等）。所以不同的部署方案是一个基于成本、需求、产品能力、方案可行性等多个维度的权衡结果产出。<br />
&emsp;&emsp;从技术上来讲，可以解决机房级容灾或者城市级容灾的集群解决方案如下：

- 同城三机房在同城三机房中，三个机房能力对等，都可以承担全部（或者部分）数据的主副本（Leader）角色，同时也承担其他数据的备份副本 (Follower) 的角色。同城三机房集群架构如下图所示。![image.png](/static/img/solutions/high_availability_scenario/2.png)
- 三地五中心五副本城市 1 和城市 2 能力对等，可以承担全部（或者部分）数据的主副本（Leader）角色，同时也承担其他数据的备份副本 (Follower) 的角色，城市 3 仅承担从副本角色。三地五中心五副本集群架构如下图所示。

  ![image.png](/static/img/solutions/high_availability_scenario/3.png)<br />

&emsp;&emsp;在实际的部署方案中，OceanBase 团队也根据客户的需求定制过同城双机房和两地三中心的方案。两种部署方案都各自解决了一部分相应场景对于系统高可用的需求，但是同时也存在一定容灾能力的短板。在一定时期有可能作为系统架构的中间态成为客户在初始部署的选择，客户也有可能与 OceanBase 数据库主备库架构搭配使用一起达到期望的容灾能力。具体的方案探讨需要考虑多个因素和方面，如果有实际具体的场景，请联系 OceanBase 数据库技术架构师进行方案讨论和对接。
<a name="AUZ76"></a>

# 用户案例

<a name="QafmZ"></a>

## 网商银行

<a name="PQXir"></a>

### 业务挑战

- 网商银行致力于为小微企业、三农用户、大众消费者、中小金融机构提供普惠金融服务，从成立之初就提出低成本、高可用、高弹性的要求。
- 随着互联网金融业务的发展，网商银行的数据量和并发访问量呈现指数级的增长，需要具备城市级别的容灾能力，满足监管要求。
- 需要标准、安全和高效的数据库多租户隔离环境及管理工具，满足全行多应用系统，如存贷汇核心系统，的管理与使用需求。
  <a name="j1iRr"></a>

### 解决方案

- 采用“三地五中心”部署架构，构建异地多活，每个城市都有全量数据，通过不同数据库的读写点交叉，由多个城市共同承担用户的流量访问。
- 凭借混合云架构、高可用等特性，通过分布式中间件、金融套件、移动开发平台集成解决方案，支撑网商银行核心系统数字化转型。

![image.png](/static/img/solutions/high_availability_scenario/4.png)
<a name="XrMOg"></a>

### 用户收益

- 通过“三地五中心”的逻辑架构部署，实现 RPO = 0,RTO < 30 s。一旦其中一到两个中心发生故障，甚至一个城市出现问题，业务也能够实现在 30 秒内自动恢复。
- 分库分表与分区相结合，提高并发跑批的能力，每个批次查询 5 千～ 100 万条，每批次并发最大可处理 130 亿账户。目前集群规模 100 多套，数据量达到 PB 级。
- 混合云的弹性架构，实现大促期间在一朵云资源耗尽的情况下，弹出 20% 的流量到新云上，保证了负载的跨云弹性伸缩。
  <a name="THsx7"></a>

## 工商银行

<a name="RFaI3"></a>

### 业务挑战

- 容灾标准高 ：理财业务支撑着企业客户万亿级别的资产，需要满足 7x24 小时持续服务，高可用容灾要求达到 5 级。
- 建设成本高：原有业务系统基于传统大机和 DB2 数据库的封闭模式运行架构，业务容灾系统建设成本高昂。
- 备机房资源浪费：近年来随着业务并发量的不断增加，数据库系统处理能力不足的问题凸显。冷备机房随时待命但不提供数据服务，资源利用率低。
  <a name="gYNkZ"></a>

### 解决方案

- OceanBase 支持数据多副本，节点间通过 Paxos 协议同步，实现集群高可用和多地灾备。结合中国工商银行实际情况，搭建跨“两地三中心”的分布式集群，以“五副本 +主备”模式进行部署。
- 集群统一管理调度所有服务器资源，实时动态计算，将业务负载调度到最空闲合理的服务器上运行。故障管理服务自动排查故障机器，调度事务到健康机器上执行，保证全局事务强一致性，无需人工干预。

![image.png](/static/img/solutions/high_availability_scenario/5.png)
<a name="tReI7"></a>

### 用户收益

- 数据库服务器资源利用率达到 75%，在系统处理能力遭遇瓶颈时，可进行便捷的水平扩展，增加集群计算资源来提升处理能力。
- 实现数据库同城双活、异地 RPO=0。机房级容灾达到 RPO=0，RTO &lt; 30s， 即故障发生后，从 IT 系统宕机导致业务停顿到系统恢复至可以支持各部门的运作时间，少于 30 秒。  达到工商银行 5 级容灾要求，满足 7x24 小时服务要求。
- 提升高可用水平，为业务提供强连续性保障，支撑万亿级资金交易，并且在保证系统性能和稳定性的前提下，有效降低了成本。
- 系统从大型主机下移到国产化 ARM 服务器，降低整个系统的投入成本。国产服务器+国产操作系统+完全自研的分布式数据库，实现核心系统的分布式改造。

<a name="HUW0V"></a>

## 高德地图

<a name="buWmo"></a>

### 业务挑战

- 亿级 DAU 的高德，每时每刻都在生产大量的数据，如何处理这些数据并降低后续扩容迁移成本、存储成本，成为了摆在高德面前的重要问题
- 随着高德深入本地服务，需要在业务发展期提前做好技术布局，保障现在和未来服务可用性、稳定性，以应对未来业务的飞速成长。
  <a name="mZSly"></a>

### 解决方案

- OceanBase 采用无共享的多副本架构，系统没有单点障碍，保证系统持续可用。支持“两地三中心”和“三地五中心”部署，对要求 CP 的结算类业务来说，可保障数据的城市级容灾。这里附一篇文章：[《促科技创新：高德数据优化篇之 OceanBase 最佳实践》](https://mp.weixin.qq.com/s/8nMqORGIPXF6IrHVAzG-9A)
- OceanBase 作为准内存数据库，采用 LSM-Tree 存储，增量数据操作内存，减少随机写，读写性能超传统关系型数据库。
- 部署架构：
  - 多点写入
    - 三地读写
    - 无网络延迟
  - 同城双主库容灾，异地多活容灾
    - 同城数据库侧容灾切流
    - 三地业务侧容灾切流
  - 三地六向数据同步
    - 三地六向秒级同步

![image.png](/static/img/solutions/high_availability_scenario/6.png)
<a name="DfR6v"></a>

### 用户收益

- 利用 OceanBase “三地五中心”的部署和强一致特性，业务可实现城市级别的容灾，保证数据不丢，极大提升了业务的稳定性。
- 利用 OceanBase 的分布式特性，业务系统的数据存储具备了动态扩容的能力，业务无感知的平滑扩容，保证业务不停机。并节省了业务提量猛增后的数据库扩容和迁移成本，极大降低了数据库容量不足的风险。
- 相比 MySQL，迁移至 OceanBase 后，读性能平均提升 2 倍，写性能平均提升 3 倍。
