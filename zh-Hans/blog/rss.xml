<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>OceanBase Blog</title>
        <link>https://oceanbase.github.io/zh-Hans/blog</link>
        <description>OceanBase Blog</description>
        <lastBuildDate>Tue, 04 Jun 2024 13:18:36 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>zh-Hans</language>
        <item>
            <title><![CDATA[The architectural evolution of OceanBase Database]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/architectural-evolution</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/architectural-evolution</guid>
            <pubDate>Tue, 04 Jun 2024 13:18:36 GMT</pubDate>
            <description><![CDATA[oceanbase database]]></description>
            <content:encoded><![CDATA[<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682303077795.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p><em>Yang Zhifeng, OceanBase’s Chief Architect, lately introduced the evolution of the technical architecture of OceanBase Database from V0.5 to V4.0 and shared his thoughts along the journey. This article is only part of his sharing. Since the content of the sharing is so extensive, we will divide it into the following articles:</em></p>
<p><em>‒ The architectural evolution of OceanBase Database</em></p>
<p><em>‒</em> <a href="https://oceanbase.medium.com/integrated-architecture-of-oceanbase-database-615dcf707f38" target="_blank" rel="noopener noreferrer"><em>What is the integrated architecture of OceanBase Database?</em></a></p>
<p><em>‒ SQL engine and transaction processing in the integrated architecture of OceanBase Database</em></p>
<p><em>‒ Performance of OceanBase Database in standalone mode: a performance comparison with MySQL 8.0</em></p>
<p>Now, let’s roll out the first article and present to you the architectural evolution of the OceanBase Database in over ten years.</p>
<p>The development of the OceanBase Database started in 2010. The first version, OceanBase Database V0.5, consists of a storage layer and a computing layer, as shown in the figure below. The computing layer, which is stateless, provides SQL services, and the storage layer is a storage cluster of two types of servers.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682303106461.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>OceanBase architecture v0.5</p>
<p>This architecture allows OceanBase Database to well support services such as Taobao Favorites. Resources, especially those for data reads, are moderately scalable. The SQL layer is stateless and can be freely scaled.</p>
<p>The most significant vulnerability of this architecture is that data writes are handled by UpdateServer nodes. Such an architecture featuring single-point writes and multi-point reads cannot be scaled to cope with a higher concurrency.</p>
<p>Another issue that may not be found just from the figure is that with the storage layer and SQL layer split that way, we can hardly keep the query latency under control, which is serious. As a matter of fact, the latency of online services fluctuates, which is hard to control if the latency requirement is high.</p>
<p>To address that issue, we laid off the old architecture and developed a new one, which has empowered OceanBase Database V1.0 to V3.0. Generally, the new architecture features equivalent nodes in a cluster. All nodes can handle transactions and save data while processing SQL queries. As shown in the figure below, the new architecture is horizontally replicable and vertically scalable. Horizontally, you can copy zone replicas to ensure the high availability of the service; vertically, you can scale out the cluster by adding more OBServers.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682303185573.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>OceanBase architecture v1.0 — v3.0</p>
<p>This new architecture showed excellent scalability, which gave us the confidence to challenge the Transaction Processing Performance Council Benchmark C (TPC-C) test with OceanBase Database V3.0. The result was encouraging. OceanBase Database became the world’s only distributed database and China’s first database that passed the test back then and got a record-high score that remains untouched so far.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682303213301.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>Horizontal scalability of OceanBase</p>
<p>The stunning performance indicates the great horizontal scalability of OceanBase Database V3.0. You can see in the figure above that, as the number of OBServer nodes increases from three, the performance curve of OceanBase Database swings up linearly before it finally hits the record-high tpmC of 707 million. This large OceanBase cluster of 1,557 OBServers processed 20 million transactions per second within the 8-hour TPC-C stress test. The test results show that the new architecture is highly scalable. Such high scalability and concurrency satisfy the needs of most online service systems in the world today.</p>
<h1>OceanBase Database V4.0</h1>
<p>In response to new business requirements, we have developed an up-to-date architecture to support OceanBase Database V4.0. The most significant update is the introduction of dynamic log streams. In the previous architecture, transaction, and storage resources are scaled by the same granularity. If the storage is sharded and scaled by shard, transaction processing, and high availability capabilities will also be scaled by shard. In the architecture for V4.0, we decouple transaction scaling from storage scaling, so that several storage shards share the same transaction log stream and the high availability capabilities corresponding to that log stream.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682303236782.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>OceanBase 4.0 architecture</p>
<p>The fundamental idea that leads to this update is that we want to support smaller applications. It’s true that the architecture for V3.0 has no problem in supporting huge applications like those of Ant Group. However, as OceanBase Database becomes more popular among customers other than tech giants, if we keep the association between the number of log streams and that of partitions, it cannot be adapted to many scenarios, especially for small and medium-sized companies. This is because the resource overhead of databases that hold small applications becomes greater as the number of log streams increases.</p>
<p>In the next articles, you will learn more details about the integrated architecture of the OceanBase Database. Stay tuned!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Data Compression Technology Explained Balance between Costs & Performance]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/compression-ratio</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/compression-ratio</guid>
            <pubDate>Tue, 04 Jun 2024 13:18:36 GMT</pubDate>
            <description><![CDATA[With more and more data being generated, storage and maintenance costs are increasing accordingly. Data compression seems to be a natural choice to reduce storage costs.]]></description>
            <content:encoded><![CDATA[<p>With more and more data being generated, storage and maintenance costs are increasing accordingly. Data compression seems to be a natural choice to reduce storage costs.</p>
<p>However, there is a dilemma when it comes to the data compression ratio. If the ratio is high, it usually takes a great deal of time to compress and decompress data, which tends to reduce the I/O performance of the memory and disk and is not a good idea for a latency-sensitive key business. If the ratio is low, the compressed files still occupy a large share of disk space, which makes the compression ineffective.</p>
<p>Most database products on the market provide data compression features. However, compression design and performance vary based on the storage engine architecture and database application scenarios.</p>
<p>Generally, conventional transactional databases store data in fixed-length blocks, which is good for read/write performance but causes additional overhead and space waste.</p>
<p>Databases oriented to online transaction processing (OLTP) must support higher transactions per second (TPS) during data write and update operations. Such databases often use row-oriented B-tree storage engines and therefore are more conservative on data compression. A B-tree storage engine usually aligns the size of a fixed-length data node with that of a persistent data block for data management. In some cases, updated data should be written to data blocks in real-time, which leads to the compression of the whole data node even if the DML operations affect only a few rows in the node, bringing more overhead. Besides, the compression of fixed-length data blocks also causes a waste of storage space because it is difficult to predict the post-compression size of a data block.</p>
<p>Analytical databases, however, are naturally more suitable for high-ratio compression but are not good for query and update performance.</p>
<p>For systems oriented to online analytical processing (OLAP), such as a data warehouse, data is usually imported by batch, with little incremental data. Therefore, analytical databases usually use column-oriented storage engines that write logs for incremental data and update baseline data periodically. Such storage engines tend to perform data compression during batch import and data updates in the background, based on compression strategies that aim for a higher compression ratio. For example, a column-oriented storage engine will compress more data into larger data blocks, store the data of the same column in adjacent data blocks, and then encode the data based on the data characteristics of the column to achieve a higher compression ratio. However, the compression may significantly reduce the performance of point queries and decrease the TPS of data updates.</p>
<p><strong>In short, the higher the compression ratio, the higher the compression and decompression overhead, leading to a greater impact on performance.</strong> We found that users are more concerned about database performance than the compression ratio, especially when it comes to critical business. Given that compressing or decompressing data while it is being read or written inevitably consumes computing resources and affects the transaction processing performance, a conventional business database only supports compression of cold data such as archives or backups. Hot data that is frequently queried or updated cannot be compressed for the sake of business performance.</p>
<p><strong>A high compression ratio makes sense for user benefits only when high database performance is guaranteed in the first place.</strong> That way, we can improve the business efficiency at lower costs.</p>
<h1>A new solution from OceanBase: Encoding and compression</h1>
<p>Compression does not consider data characteristics, while encoding compresses data by column based on data characteristics. These two types of methods are orthogonal, meaning that we can first encode a data block and then compress it to achieve a higher compression ratio.</p>
<p>At the stage of compression, OceanBase Database uses compression algorithms to compress a microblock, without considering the data format, and eliminate data redundancy if detected. OceanBase Database supports compression algorithms like zlib, Snappy, zstd, and LZ4. In most cases, Snappy and LZ4 are faster but provide lower compression ratios. LZ4 is faster than Snappy in terms of both compression and decompression. Both zlib and zstd deliver higher compression ratios but are slower, with zstd being faster in decompression. You can use DDL statements to configure and change the compression algorithms for tables based on table application scenarios.</p>
<p>Before data is read from a compressed microblock, the whole microblock is decompressed for scanning, which causes CPU overhead. To minimize the impact of decompression on query performance, OceanBase Database assigns the decompression task to asynchronous I/O threads, which will call the callback function to decompress the microblock when the I/O operations on the microblock are finished and then store the decompressed microblock in the block cache as needed. This, combined with query prefetching, provides a pipeline of microblocks for query processing threads and eliminates the additional overhead due to decompression.</p>
<p>The advantage of compression is that it makes no assumptions about the data to compress and can always find a pattern for any data and then compress it. A relational database has more prior knowledge about the structured data stored in it. We believe that we can make use of the prior knowledge to improve data compression efficiency.</p>
<p>E<strong>ncoding algorithms in OceanBase Database</strong></p>
<p>To achieve a higher compression ratio and help users reduce storage costs, OceanBase Database has developed a variety of encoding algorithms that have worked well. In addition to single-column encoding algorithms, such as bit-packing, Hex encoding, dictionary encoding, run-length encoding (RLE), constant encoding, delta encoding for numeric data, and delta encoding for fixed-length strings, OceanBase Database also provides innovative column equal encoding and column prefix encoding to compress different types of redundant data in one or several columns, respectively.</p>
<ol>
<li><strong>Bit-packing and Hex encoding: Reducing bit width for storage</strong></li>
</ol>
<p>Bit-packing and Hex encoding are similar in that they represent the original data by encoding it with fewer bits if the cardinality of data is small. Given an int64 column with a value range of [0, 7], for example, we can store only the lowest three bits to represent the originals and remove higher zero bits to save storage space. Or, for a string column with a character cardinality of less than 17, we can map each character in the column to a hexadecimal number in the range of [0x0, 0xF]. This way, each original character is represented by a 4-bit hexadecimal number, which occupies less storage space. Moreover, these two encoding algorithms are not exclusive, which means that you can perform bit-packing or hex encoding on numeric data or strings generated by other encoding algorithms to further reduce data redundancy.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682307908847.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>Bit packing</p>
<p>Hex encoding</p>
<p><strong>2. Dictionary encoding and RLE: De-duplicating data in a single column</strong></p>
<p>We can create a dictionary for a data block to compress data with low cardinality. If low-cardinality data within a microblock is not evenly distributed, meaning that rows with the same data in a column cluster together, we can also use RLE to compress the reference values of the dictionary. Further, if most of the rows in the microblock of a low-cardinality column have the same value, we can compress the data even more by using constant encoding, which stores the most frequent data as constants and the non-constant data and corresponding row subscripts in an exception list.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682307917471.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>Dictionary encoding and RLE</p>
<p><strong>3. Delta encoding: Compressing data based on the value range</strong></p>
<p>OceanBase Database supports delta encoding for numeric data and delta encoding for fixed-length strings. Delta encoding for numeric data is suitable for compressing numeric data in a small value range. To compress date, timestamp, or other numeric data of small adjacent differences, we can keep the minimum value, and store the difference between the original data and the minimum value in each row. These differences can usually be compressed by using bit-packing. Delta encoding for fixed-length strings is a better choice for compressing artificially generated nominal numbers, such as order IDs and ID card numbers, as well as strings of a certain pattern, such as URLs. We can specify a pattern string for a microblock of such data and store the difference between the original string and the pattern string in each row to achieve better compression results.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682307925452.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>Numeric difference</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682307934304.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>String difference</p>
<p><strong>4. Span-column encoding: Reducing redundancy across columns</strong></p>
<p>OceanBase Database provides span-column encoding to improve the compression ratio by making use of the data similarity between different columns. Technically, data is encoded within a column in a column-oriented database. In a real-life business database, however, it’s common that data in different columns to be associated.</p>
<ul>
<li>If most data of column A and column B are the same, we can use column equal encoding. In this coding, column A is taken as the reference of column B, and for column B, only rows that are different from those in column A need to be stored.</li>
<li>If data in column A is the prefix of data in column B, OceanBase Database adopts column prefix encoding. In this encoding, all data in column A and data excluding prefixes in column B are stored.</li>
</ul>
<p>Span-column encoding performs better in the compression of composite columns and some system-generated data, and can also reduce data redundancy due to inappropriate table design paradigms.</p>
<p>A<strong>daptive compression: Allow the database to select encoding algorithms</strong></p>
<p>The compression performance of data encoding algorithms is related not only to the table schema but also to data characteristics such as the distribution of data and the value range of data within a microblock. This means it is hard to achieve the best compression performance by specifying a columnar encoding algorithm when you design the table data model. To make your work easier and improve the compression performance, OceanBase Database automatically detects the data in all columns and selects an appropriate encoding algorithm for each column during data compaction. The database supports encoding the same column in different microblocks with different algorithms.</p>
<p>The process of detection and selection involves a lot of computing work, which puts more pressure on the CPU during the compaction. Therefore, in a compaction task, OceanBase Database analyzes the data type, value range, number of distinct values (NDV), and other data characteristics and selects a more suitable encoding algorithm by using a heuristic algorithm based on the encoding algorithm and compression ratio selected for the same column in the previous microblock. In addition to selecting a more suitable encoding algorithm, this method ensures acceptable CPU overhead for encoding during the compaction.</p>
<p>O<strong>ne more step: Optimize the query of encoded data</strong></p>
<p>To better balance the compression result and query performance, we have considered the impact on query performance when designing data encoding methods.</p>
<ol>
<li>Row-level random access</li>
</ol>
<p>In compression scenarios, the whole data block will be decompressed even if you want to access just a slice of data in it. In some analytical systems intended for scan queries rather than point queries, patched frame-of-reference (PFOR) delta coding, for example, is adopted to decode adjacent or all preceding rows before accessing a row in a data block.</p>
<p>To better support transactional workloads, OceanBase Database needs to support more efficient point queries. So, our encoding methods ensure row-level random access to encoded data. To be specific, the database accesses and decodes only the metadata of the target row in a point query, which reduces the computational amplification during random point queries. In addition, OceanBase Database stores all metadata required for decoding data in a microblock just within the microblock, so that the microblock is self-describing with better memory locality during decoding.</p>
<p>2. Cache decoder</p>
<p>OceanBase Database initializes a decoder for data decoding of each column. Creating the decoder certainly consumes some CPU and memory resources. To further reduce the response time for accessing encoded data, OceanBase Database caches the decoder and data block together in the block cache. This way, the cached decoder can directly decode the cached data block. The creation and caching of decoders are also executed by asynchronous I/O callback threads to reduce the decoder initialization overhead. When it fails to hit a decoder in the block cache, OceanBase Database also builds a cache pool for the metadata memory and objects required by the decoder, which are reused in different queries.</p>
<p>After those optimizations, even encoded data in SSTables with a hybrid row-column storage architecture can well support transactional workloads.</p>
<p>When it comes to analytical queries, encoded data, which is stored in a hybrid row-column storage architecture, is distributed more compactly and is more CPU cache friendly. These characteristics of encoded data are similar to those of data stored by column, and therefore we can improve analytical queries by applying some optimization methods that often work on columnar storage, such as Single Instruction Multiple Data (SIMD) processing.</p>
<p>Furthermore, since we store dictionaries, null bitmaps, constants, and other metadata in a microblock to describe the distribution of the encoded data, we can use the metadata to optimize the execution of some filter and aggregate operators during a data scan and to perform calculations directly on the compressed data. Many data warehouses use similar approaches to optimize the query execution. In a SIGMOD 2022 paper titled “CompressDB: Enabling Efficient Compressed Data Direct Processing for Various Databases”, authors also describe a similar idea, where the system efficiency is improved by pushing some computations down to the storage layer and executing them directly on the compressed data. The performance turns out to be pretty good.</p>
<p>OceanBase Database significantly strengthened its analytical processing capabilities in V3.2. In the latest versions, aggregation and filtering are pushed down to the storage layer and the vectorized engine is used for vectorized batch decoding based on the columnar storage characteristics of encoded data. When processing a query, OceanBase Database performs calculations directly on the encoded data by making full use of the encoding metadata and the locality of encoded data stored in a columnar storage architecture. This brings a significant increase in the execution efficiency of the pushed-down operators and the data decoding efficiency of the vectorized engine. Data encoding-based operator push-down and vectorized decoding are also important features that have empowered OceanBase Database to efficiently handle analytical workloads and demonstrate an extraordinary performance in TPC-H benchmark tests.</p>
<h1>Testing the encoding and compression algorithms</h1>
<p>We performed a simple test to see the influence of different compression methods on the compression performance of OceanBase Database.</p>
<p>We used OceanBase Database V4.0 to test the compression ratio with a TPC-H 10 GB data model in transaction scenarios and with an IJCAI-16 Brick-and-Mortar Store Recommendation Dataset in user behavior log scenarios.</p>
<p>The TPC-H model simulated real-life order and transaction processing. We tested the compression ratios of two large tables in the TPC-H model: ORDERS table, which stores order information, and LINEITEM table, which stores product information. Under the default configuration where both the zstd and encoding algorithms were used, the compression ratio of the two tables reached about 4.6 in OceanBase Database, much higher than the compression ratio achieved when only the encoding or zstd algorithm was used.</p>
<p>The IJCAI-16 Taobao user log dataset stored desensitized real behavior logs of Taobao users. The compression ratio reached 9.9 when the zstd and encoding algorithms were combined, 8.3 for the encoding algorithm alone, and 6.0 for the zstd algorithm alone.</p>
<p>As you can see, OceanBase Database works better in real business data compression and also shows an impressive performance on datasets with less data redundancy like the TPC-H model.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682307942389.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>Data compression performance test</p>
<p>This article shares our thoughts and solution for data compression in OceanBase Database. Based on the LSM-Tree storage engine, OceanBase Database has achieved a balance between storage costs and query performance.</p>
<p>If you have any questions, welcome to leave a comment below!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Flink CDC + OceanBase integration solution for full and incremental synchronization]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/flink-cdc</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/flink-cdc</guid>
            <pubDate>Tue, 04 Jun 2024 13:18:36 GMT</pubDate>
            <description><![CDATA[This article introduces OceanBase and explains the application scenarios of Flink CDC and OceanBase.]]></description>
            <content:encoded><![CDATA[<p>This article introduces OceanBase and explains the application scenarios of Flink CDC and OceanBase.</p>
<p>This article was compiled from the speech by Wang He (Chuanfen) (OceanBase Technical Expert) at the Flink CDC Meetup on May 21. The main contents include:</p>
<ol>
<li>An Introduction to OceanBase</li>
<li>The Implementation Principle of Flink CDC OceanBase Connector</li>
<li>The Application Scenarios of Flink CDC + OceanBase</li>
<li>The Future Outlook of Flink CDC OceanBase Connector</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-an-introduction-to-oceanbase">1. An Introduction to OceanBase<a href="https://oceanbase.github.io/zh-Hans/blog/flink-cdc#1-an-introduction-to-oceanbase" class="hash-link" aria-label="1. An Introduction to OceanBase的直接链接" title="1. An Introduction to OceanBase的直接链接">​</a></h2>
<p><img decoding="async" loading="lazy" src="https://oceanbase.github.io/zh-Hans/assets/images/ob-history-57a56856ceaf728f1bc12868cdc347f6.png" width="1280" height="720" class="img_ev3q"></p>
<p>OceanBase is a distributed database developed by Ant Group. The project was established in 2010 and developed iteratively. Its earliest application is to Taobao's favorites. In 2014, the OceanBase R&amp;D Team moved from Taobao to Ant Group, mainly responsible for Alipay's internal de-IOE work. It means replacing the Oracle database used by Alipay. Currently, all the data in Ant Group databases have been migrated to OceanBase. On June 1, 2021, OceanBase was officially opened source to the public, and a MySQL-compatible version was launched.</p>
<p>OceanBase has undergone three generations of architecture upgrades, including a distributed storage system initially applied to e-commerce companies, a general-purpose distributed database, and an enterprise-level distributed database.</p>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/2551bf4e7b277e5a3af59a91d93ec311b2bb5253.png" alt="" class="img_ev3q"></p>
<p>The preceding figure shows the architecture of OceanBase.</p>
<p>The top-level app accesses the server side of the OceanBase database through the OBProxy (SLB proxy). The data on the server side has multiple replicas. The relationship between the replicas is similar to the primary-secondary relationship in the database architecture, but it is table-level. The partition of the partition table is divided into multiple replicas at the table level and then scattered on multiple servers.</p>
<p>The architecture of OceanBase has the following features:</p>
<ul>
<li><strong>No Shared Architecture:</strong> Each node has its complete SQL engines, storage engines, and transaction processing logic. All nodes are completely peer-to-peer, and there are no layered structures.</li>
<li><strong>Partition-Level Availability:</strong> It provides partition-level availability. The partition is the basic unit for implementing reliability and scalability, achieving access routing, SLB, and automatic fault recovery in the OceanBase database.</li>
<li><strong>High Availability and Strong Consistency:</strong> There are multiple data replicas. The consistency protocol of Paxos is used to provide high reliability between multiple replicas and ensure the Persistent Event Log (PEL) is successful at the majority of nodes.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/1ad5f7ab919762810f62c4df3af5e7fa98073068.png" alt="" class="img_ev3q"></p>
<p>The core features of OceanBase:</p>
<ol>
<li><strong>High Availability:</strong> It is based on the Paxos protocol, with strong consistency. A few copies are faulty, but data is not lost, and services are not stopped.</li>
<li><strong>High Scalability:</strong> It supports online horizontal scale-out and capacity reduction. Nodes can automatically achieve SLB.</li>
<li><strong>High Compatibility:</strong> The Community Edition is compatible with MySQL protocols and syntax.</li>
<li><strong>Low Cost:</strong> OceanBase database costs about one-third of MySQL. The <strong>storage compression ratio</strong> is extremely high because of its low hardware quality requirements and a lot of optimization of storage.</li>
<li><strong>Multi-Tenancy:</strong> Resources are completely isolated between tenants. Different service parties only need to manage data in their tenants, which can save costs.</li>
<li><strong>HTAP:</strong> It implements both OLTP and OLAP in one set of engines.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-the-implementation-principle-of-flink-cdc-oceanbase-connector">2. The Implementation Principle of Flink CDC OceanBase Connector<a href="https://oceanbase.github.io/zh-Hans/blog/flink-cdc#2-the-implementation-principle-of-flink-cdc-oceanbase-connector" class="hash-link" aria-label="2. The Implementation Principle of Flink CDC OceanBase Connector的直接链接" title="2. The Implementation Principle of Flink CDC OceanBase Connector的直接链接">​</a></h2>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/23dd7319f3dd972de1ba223fa81c271bb194650c.png" alt="" class="img_ev3q"></p>
<p>The current mainstream CDC mainly relies on the logs of the database. After capturing the incremental logs of the database, it is necessary to ensure their orderliness and integrity, process the logs, and write them to the destination, such as a data warehouse or query engine.</p>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/44b0297d17d2126dd13e890b4a3726efadd953a5.png" alt="" class="img_ev3q"></p>
<p>OceanBase provides some components for capturing incremental data. It is a distributed database, so its data is scattered when it falls into the log. It provides an obcdc component for capturing database logs. It interacts with the OceanBase Server through RPC to capture the original log information. After processing, an orderly log flow can be spit out, and the downstream can consume the orderly log flow by accessing the obcdc component.</p>
<p>There are currently three major downstream consumers:</p>
<ul>
<li><strong>Oblogproxy:</strong> An open-source component that is used to consume log flows. Flink CDC relies on this component to capture incremental data.</li>
<li><strong>OMS Store:</strong> The data migration service provided by OceanBase. The commercial version of OMS has been upgraded in many iterations and supports many data sources. Last year, OMS supported the community edition of the two data sources, OceanBase and MySQL.</li>
<li><strong>JNI Client:</strong> You can directly use obcdc to interact with OBSserver to capture incremental logs. This downstream consumer is under an open-source plan.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/328e4a928e5085a8cec7788caf02dd0270c92724.png" alt="" class="img_ev3q"></p>
<p>Currently, there are two OceanBase CDC components provided by the Open-Source Community:</p>
<ul>
<li><strong>OceanBase Canal:</strong> Canal is an open-source tool of Alibaba for capturing MySQL incremental logs. OceanBase community strengthens the ability to capture and parse incremental logs based on the latest code of Canal in the open-source edition.</li>
<li><strong>Flink CDC:</strong> Flink CDC uses obcdc through oblogproxy to capture incremental logs from OceanBase and then uses another open-source component logproxy-client to consume incremental logs and process them.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/c2e8f3a1e5be13f89433bf823ecd1e17215886a4.png" alt="" class="img_ev3q"></p>
<p>The lower-left corner of the figure above shows how to define a dynamic table. Data streams are converted into tables in Flink in the form of dynamic tables. You can only perform SQL operations if the data stream is converted into a table. After that, Continuous Query queries the growing dynamic table. The obtained data is still presented in the form of a table. Then, the Continuous Query will convert it into a data stream and send it to downstream consumers.</p>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/822e48b74147da81cce8f7445c39df9126e3a195.png" alt="" class="img_ev3q"></p>
<p>The figure above shows the implementation principle of Flink CDC.</p>
<p>Flink CDC Connector only reads the source data. It only reads the data from the data source and then inputs it to the Flink engine.</p>
<p>The current Flink CDC Connectors are mainly divided into the following three categories:</p>
<ul>
<li><strong>MySqlSource:</strong> It implements the latest source interface and achieves concurrent reads.</li>
<li><strong>DebeziumSourceFunction:</strong> It implements SourceFunction based on Debezium and supports earlier versions of MySQL, Oracle, MongoDB, SqlServer, and PostgreSQL.</li>
<li><strong>OceanBaseSourceFunction:</strong> It implements the SourceFunction interface and achieves full and incremental reads based on JDBC and logproxy-client.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/ab26b801cb8343ff90148423812e6a7c6e6eccd8.png" alt="" class="img_ev3q"></p>
<p>The preceding figure shows the data path of the Flink CDC OceanBase Connector.</p>
<p>Incremental data is captured first through logproxy. The logproxy-client monitors the data stream of incremental logs. After Flink CDC reads the incremental logs, the data will be written to Flink in line with the processing logic of Flink CDC. Full data are captured through JDBC.</p>
<p>The functions supported by the current Flink CDC OceanBase Connector are mainly limited by logproxy. Currently, it supports capturing data at a specified time. However, since OceanBase is a distributed database, you cannot accurately find the starting point of incremental log data. However, if you specify the starting point with a timestamp, there may be some duplicate data.</p>
<p>OceanBase Community Edition does not have table locks in the process of capturing full data. Therefore, the data edge points cannot be determined by locking the full data.</p>
<p>Considering the two aspects above, currently, only the at-least-once working mode is supported, and the exactly-once mode has not been realized.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-application-scenarios-of-flink-cdc--oceanbase">3. Application Scenarios of Flink CDC + OceanBase<a href="https://oceanbase.github.io/zh-Hans/blog/flink-cdc#3-application-scenarios-of-flink-cdc--oceanbase" class="hash-link" aria-label="3. Application Scenarios of Flink CDC + OceanBase的直接链接" title="3. Application Scenarios of Flink CDC + OceanBase的直接链接">​</a></h2>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/ee62c27fbff429d58d3ccd5aa342097b437e546c.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="31-scenario-1-data-integration-based-on-database-and-table-sharding">3.1 Scenario 1: Data Integration Based on Database and Table Sharding<a href="https://oceanbase.github.io/zh-Hans/blog/flink-cdc#31-scenario-1-data-integration-based-on-database-and-table-sharding" class="hash-link" aria-label="3.1 Scenario 1: Data Integration Based on Database and Table Sharding的直接链接" title="3.1 Scenario 1: Data Integration Based on Database and Table Sharding的直接链接">​</a></h3>
<p>Flink CDC supports real-time consistent synchronization and processing of full data and incremental data in tables. OceanBase Connector supports regular expression matching for reading data. For database and table sharding, the OceanBase Connector can be used to create dynamic tables to read data from data sources and write all the data into a table to realize the aggregation of table data.</p>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/7f69f6fa9b9350ec1cb117c82b7d542d1c8218fb.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="32-scenario-2-cross-clustertenant-data-integration">3.2 Scenario 2: Cross-Cluster/Tenant Data Integration<a href="https://oceanbase.github.io/zh-Hans/blog/flink-cdc#32-scenario-2-cross-clustertenant-data-integration" class="hash-link" aria-label="3.2 Scenario 2: Cross-Cluster/Tenant Data Integration的直接链接" title="3.2 Scenario 2: Cross-Cluster/Tenant Data Integration的直接链接">​</a></h3>
<p>OceanBase is a multi-tenant system. Currently, cross-tenant access is not available for the tenants of the community edition of MySQL. Therefore, if you need to read data across tenants, you must connect with multiple databases to read data separately. Flink CDC is naturally suitable for cross-tenant data reading, in which each tenant corresponds to a dynamic table for data source reading, and then the data converges in Flink.</p>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/8cb06edf87b1a326c4574193d6b0deacc8327f2e.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="33-scenario-3-data-integration-of-multiple-data-sources">3.3 Scenario 3: Data Integration of Multiple Data Sources<a href="https://oceanbase.github.io/zh-Hans/blog/flink-cdc#33-scenario-3-data-integration-of-multiple-data-sources" class="hash-link" aria-label="3.3 Scenario 3: Data Integration of Multiple Data Sources的直接链接" title="3.3 Scenario 3: Data Integration of Multiple Data Sources的直接链接">​</a></h3>
<p>You can aggregate data from different types of data sources. There is no change in cost for the integration of MySQL, TiDB, and other data sources compatible with the MySQL protocol since the data format is the same.</p>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/7c9962ae45c6441c135d83ec7d56a320ce7c7a53.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="34-scenario-4-building-an-olap-application">3.4 Scenario 4: Building an OLAP Application<a href="https://oceanbase.github.io/zh-Hans/blog/flink-cdc#34-scenario-4-building-an-olap-application" class="hash-link" aria-label="3.4 Scenario 4: Building an OLAP Application的直接链接" title="3.4 Scenario 4: Building an OLAP Application的直接链接">​</a></h3>
<p>OceanBase is an HTAP database that has a strong TP capability and can be used as a data warehouse. The JDBC connector in Flink allows you to write data to databases compatible with the MySQL protocol. Therefore, you can use Flink CDC to read source data and write this data to OceanBase through the Flink JDBC connector. OceanBase is used as a destination.</p>
<p>OceanBase provides three data processing methods: SQL, Table API, and HBase API. All required components are open-source.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-the-future-of-oceanbase-connector">4. The Future of OceanBase Connector<a href="https://oceanbase.github.io/zh-Hans/blog/flink-cdc#4-the-future-of-oceanbase-connector" class="hash-link" aria-label="4. The Future of OceanBase Connector的直接链接" title="4. The Future of OceanBase Connector的直接链接">​</a></h2>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/5ec8f36591abb9ae66d52d3cb8c9bdbc2afe5e99.png" alt="" class="img_ev3q"></p>
<p>The preceding figure lists the current status of the OceanBase CDC solution.</p>
<p><strong>OMS Community Edition:</strong> It is a functional subset of OMS Commercial Edition, but it is not open-source. As a white screen tool, its operation is friendly, and it supports real-time consistent synchronization and processing of full data and incremental data, with data checksum and O&amp;M capabilities. Its disadvantage is that the deployment process is a bit cumbersome. It only supports two data sources, MySQL and OceanBase Community Edition, and does not support incremental DDL.</p>
<p><strong>DataX + Canal/Otter:</strong> It is an open-source solution for data migration. Otter is the parent project of Canal. It is mainly aimed at multi-site high availability and supports two-way data synchronization. Its incremental data reading is based on Canal. The advantage of this solution is that it supports a variety of targets and supports HBase, ES, and RDB. The disadvantage is that Canal and Otter do incremental, DataX do full, incremental and full are separated, and data redundancy will occur in the connection part.</p>
<p><strong>Flink CDC:</strong> It is a pure open-source solution. The community is active, and the users are growing rapidly. It supports multiple sources and targets as well as real-time consistent synchronization and processing of full data and incremental data. At the same time, as an excellent big data processing engine, Flink can be used for ETL. The downside is that OceanBase Connector currently does not support incremental DDL and exactly-once. Therefore, there may be data redundancy in the overlap between incremental and full data.</p>
<p><img decoding="async" loading="lazy" src="https://yqintl.alicdn.com/1b6e0bd4ebf2229a950f47f0327a6891f917ecc8.png" alt="" class="img_ev3q"></p>
<p>We will optimize data reading in the future. Parallelize the full data part and use the new parallel processing framework of the source interface. In the incremental data part, skip the logproxy service and directly capture incremental data from the OceanBase database. Use the obcdc component through the JNI client to directly capture data.</p>
<p>Secondly, enrich the functional features. Currently, Flink CDC only supports OceanBase Community Edition, which uses the same components for incremental log reading as the OceanBase Enterprise Edition. Therefore, OceanBase Enterprise Edition can support incremental log reading, incremental DDL, exactly-once mode, and rate limiting, with only minor changes.</p>
<p>Finally, improve code quality. First, we will operate end-to-end testing. As for format conversion, use runtime converter instead of JdbcValueConverters to improve performance. Implement support for the new source interface (parallel processing framework).</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="qa">Q&amp;A<a href="https://oceanbase.github.io/zh-Hans/blog/flink-cdc#qa" class="hash-link" aria-label="Q&amp;A的直接链接" title="Q&amp;A的直接链接">​</a></h2>
<p><strong>Q: How about the usability and stability after Flink CDC OceanBase Connector is open-source?</strong></p>
<p>A: In terms of usability, we have successively developed many open-source components with the support of non-open-source community editions (such as OMS and OCP). In terms of stability, OceanBase has been widely used in Ant Group, and the MySQL-compatible version has been put into large-scale applications in more than 20 enterprises. Therefore, there is no need to worry about its stability.</p>
<p><strong>Q: Where is metadata (such as the shard information and the index information) stored in OceanBase?</strong></p>
<p>A: They are stored in the OB server and can be directly queried through SQL.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Integrated Architecture of OceanBase Database]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/integrated-architecture</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/integrated-architecture</guid>
            <pubDate>Tue, 04 Jun 2024 13:18:36 GMT</pubDate>
            <description><![CDATA[oceanbase database]]></description>
            <content:encoded><![CDATA[<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302303091.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p><em>Yang Zhifeng, OceanBase’s Chief Architect, lately introduced the evolution of the technical architecture of OceanBase Database from V0.5 to V4.0 and shared his thoughts along the journey. This article is only part of his sharing. Since the content of the sharing is so extensive, we will divide it into the following articles:</em></p>
<p><em>‒</em> <a href="https://medium.com/@oceanbase/the-architectural-evolution-of-oceanbase-database-9ab70506fc15" target="_blank" rel="noopener noreferrer"><em>The architectural evolution of OceanBase Database</em></a></p>
<p><em>‒ What is the integrated architecture of OceanBase Database?</em></p>
<p><em>‒ SQL engine and transaction processing in the integrated architecture of OceanBase Database</em></p>
<p><em>‒ Performance of OceanBase Database in standalone mode: a performance comparison with MySQL 8.0</em></p>
<p><em>This article introduces the integrated architecture of the OceanBase database.</em></p>
<p>The architecture of OceanBase Database V4.0 allows you to deploy a distributed database or a MySQL-like standalone database in the same way that you are familiar with.</p>
<p>If you deploy a standalone OceanBase database or a single-container tenant in an OceanBase cluster, the database provides the same efficiency and performance as a conventional standalone database does.</p>
<p>If you deploy OceanBase Database in a distributed architecture, no additional costs will be incurred in tenant management, transaction processing, and SQL statement execution.</p>
<p>With that in mind, we must design each database layer, such as the SQL layer, storage layer, and transaction layer, in a way that integrates features of a standalone database and a distributed database to meet various needs with a balanced performance at each layer.</p>
<p>In the development of databases, a long-debated question is whether we should design databases in shared-nothing clusters or shared-everything architectures to maximize efficiency.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302331364.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>A physical cluster in a real production environment today is neither completely shared-everything nor completely shared-nothing.</p>
<p>A server may have multiple CPU cores, 96 or more in some cases, with increasingly large memory space. Servers interact with disks and external devices by using high-speed system buses over a high-performance network. The cluster that consists of such servers is not purely shared-nothing. In such a cluster, each server has excellent computing and storage capabilities that we can use. That is how all standalone or centralized databases are scaled up. We have no reason to ignore the vertical scaling option of distributed databases, which are generally horizontally scaled.</p>
<p>The question is, how to vertically scale up a distributed database?</p>
<p>Assume that we have built a distributed database of architecture as shown on the left side of the following figure. This architecture consists of separated computing nodes and storage nodes and is adopted by many distributed databases today. The computing node layer and storage node layer are separated. They interact with each other by using a GTM (Global Transaction Manager) or TSO (Timestamp Oracle), which handles global transactions and is necessary for multi-server interactions.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302372544.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>If this distributed database is deployed on a single node and runs as a standalone database, the biggest problem here is that the interactions between the components on a single server lead to considerable overhead, which is unnecessary for a standalone database such as MySQL. So, a distributed database in such a simple deployment mode cannot compete with a standalone database.</p>
<h1>The integrated architecture: Meeting the needs in both standalone and distributed scenarios</h1>
<p>A standalone database must have separate layers of SQL, transaction, and storage engines. Similarly, you can consider the distributed SQL engine, transaction engine, and storage engine of a distributed database as three separate layers. We want to integrate the three engines of a standalone database with those of a distributed database by using the same code that can dynamically adapt to different deployment modes.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302392770.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>In addition, to maximize the scalability of the single server of a standalone database, the SQL layer must support parallel execution and the transaction layer must provide a concurrency management feature, such as Multi-Version Concurrency Control (MVCC). Techniques like group commit are also required to concurrently execute multiple transactions on a single server. The storage layer must support parallel I/O so that the server can make full use of additional disks and bandwidth.</p>
<p>When you deploy a distributed database that also provides features of a standalone system, it is not enough to focus only on scalability. Each server of the distributed database must also achieve very high efficiency and support the efficient serial execution of distributed transactions so that standalone transactions can be adaptively optimized.</p>
<p>OceanBase Database also provides the standalone LSM-Tree storage engine, a one-of-a-kind innovative technology. In a standalone database, we can only perform major compactions that may affect both the front end and the back end. Fortunately, we are not talking about a 100% standalone database, and we can introduce a distribution strategy so that the major compactions of multiple replicas can be performed in turn during off-peak hours. We have taken into account both standalone and distributed features in the design of the SQL layer, transaction layer, and storage layer so that each layer consumes no extra overhead in both standalone and distributed scenarios.</p>
<p>As a result, OceanBase Database provides two essential features:</p>
<p>First, in standalone mode, no additional components are deployed and OceanBase Database works with only a single process, which means no complex interactions are performed between different processes. In this single-process multi-thread model, interactions are performed by calling functions, which is very important.</p>
<p>Second, components interact vertically between the SQL layer, transaction layer, and storage layer. Function calls are made for interaction between the layers on a single server. RPCs are used only when it is necessary to communicate between nodes.</p>
<p>Like the shared-nothing and shared-everything architecture mentioned above, OceanBase Database performs interactions between the SQL layer, transaction layer, and storage layer within each OBserver by calling functions, which is the same as in a standalone database. When interactions between multiple OBservers are required, you can choose the most efficient solution based on your needs.</p>
<p>For example, if an SQL query is sent from OBServer A to the SQL layer of OBServer B and requests to access the storage layer of OBServer B, the optimal solution depends on the specific task, and the choice may vary based on the workload. The same is true for the transaction layer. For a standalone transaction, interactions with other transaction components are unnecessary.</p>
<p>If you want to learn more about the architecture of OceanBase’s storage, SQL, and transaction engines, here are some references for you:</p>
<ol>
<li><a href="https://medium.com/@oceanbase/design-a-storage-engine-for-distributed-relational-database-from-scratch-a27d69a47fe0" target="_blank" rel="noopener noreferrer">Design A Storage Engine for Distributed Relational Database from Scratch</a></li>
<li><a href="https://medium.com/@oceanbase/designing-a-distributed-sql-engine-challenges-decisions-52bea749b2f0" target="_blank" rel="noopener noreferrer">Designing a Distributed SQL Engine: Challenges &amp; Decisions</a></li>
<li><a href="https://medium.com/@oceanbase/infinite-possibilities-of-a-well-designed-transaction-engine-in-distributed-database-273dd2ad3c5d" target="_blank" rel="noopener noreferrer">Infinite Possibilities of a Well-Designed Transaction Engine in Distributed Database</a></li>
</ol>
<p>In the next articles, you will learn more details about the SQL engine and transaction processing in OceanBase. Stay tuned!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Integrated SQL Engine in OceanBase Database]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/integrated-sql-engine</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/integrated-sql-engine</guid>
            <pubDate>Tue, 04 Jun 2024 13:18:36 GMT</pubDate>
            <description><![CDATA[oceanbase database]]></description>
            <content:encoded><![CDATA[<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682301944833.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p><em>Yang Zhifeng, OceanBase’s Chief Architect, lately introduced the evolution of the technical architecture of OceanBase Database from V0.5 to V4.0 and shared his thoughts along the journey. This article is only part of his sharing. Since the content of the sharing is so extensive, we will divide it into the following articles:</em></p>
<p><em>‒</em> <a href="https://medium.com/@oceanbase/the-architectural-evolution-of-oceanbase-database-9ab70506fc15" target="_blank" rel="noopener noreferrer"><em>The architectural evolution of OceanBase Database</em></a></p>
<p><em>‒</em> <a href="https://medium.com/@oceanbase/integrated-architecture-of-oceanbase-database-615dcf707f38" target="_blank" rel="noopener noreferrer"><em>What is the integrated architecture of OceanBase Database?</em></a></p>
<p><em>‒ SQL engine and transaction processing in the integrated architecture of OceanBase Database</em></p>
<p><em>‒ Performance of OceanBase Database in standalone mode: a performance comparison with MySQL 8.0</em></p>
<p><em>This article introduces the SQL engine and transaction processing in the integrated architecture of the OceanBase database.</em></p>
<p>We have designed the SQL execution engine of OceanBase Database based on many scenarios. Because we want it to be adaptive and provide the best performance in each scenario.</p>
<p>Generally speaking, each SQL statement can be executed in serial or parallel mode.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302044156.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>In serial execution, if the table or partition involved is located on the local server, the execution process is exactly the same as that of an SQL statement on a local or standalone server. If the required data is stored on another server, OceanBase Database either fetches the remote data and processes it on the local server or performs remote execution. In remote execution, if all data required in a transaction is located on another server, OceanBase Database forwards the transaction to that server, which will access the storage, process the data, commit the transaction, and then return the results. If the data required in an SQL statement is located on many servers, to consume the minimal overhead comparable to serial execution on a standalone server, OceanBase Database performs distributed execution, pushes the computing tasks to each server for local processing, and then aggregates the results. This way, the degree of parallelism (DOP) is 1, and no extra resources are consumed. In the case of parallel execution, OceanBase Database supports parallel execution on a standalone server, distributed parallel execution, and parallel DML write.</p>
<p>As mentioned above, serial execution consumes minimal overhead. In this case, OceanBase Database performs serial scans on a single server without context switchover or remote data access, which is highly efficient. For a small business, serial execution is sufficient to meet the requirements. If you need to process a large amount of data, OceanBase Database also supports parallel execution on a standalone server. This capability is not supported by many open-source standalone databases. With enough CPUs, OceanBase Database can linearly shorten the processing time of an SQL statement by performing parallel execution. You only need to deploy OceanBase Database on a high-performance multiprocessor server and enable parallel execution.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302118057.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>OceanBase Database supports the parallel execution of distributed execution plans on multiple servers. This means that the DOP will no longer depend on the number of CPU cores of a standalone server. Hundreds or even thousands of CPU cores can be added to support a higher DOP.</p>
<h1>Serial execution: DAS execution and distributed execution</h1>
<p>DAS execution and distributed execution are two types of serial execution supported in the OceanBase Database.</p>
<p>Data pull is a way to perform DAS execution with minimal resource consumption. If the required data is located on another server and only single-point query or table access by the index primary key is involved, OceanBase Database will pull the required data to the local server. The execution plan has the form of a local execution plan. The executor will automatically pull data. Sometimes, however, it is better to push down the computing tasks. Therefore, OceanBase Database also supports distributed execution. Note that distributed execution does not consume extra resources, and is performed with the same DOP as DAS execution.</p>
<p>For some special queries or large-scale scans, OceanBase Database will adaptively select the execution mode based on the cost to achieve the best results.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682302163423.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>The parallel execution framework of OceanBase Database adaptively handles both parallel executions on a standalone server and distributed parallel execution. Parallel execution can be performed by worker threads on the local server or many different servers. OceanBase Database has an adaptive data transmission layer in its distributed execution framework. In the case of parallel execution on a standalone server, the data transmission layer automatically converts the data interactions between the threads into replicas stored in the memory. In this way, the data transmission layer makes it possible to adaptively handle tasks in two different scenarios. In fact, the parallel execution engine schedules tasks for parallel execution on a standalone server and distributed parallel execution in the exact same way.</p>
<p>In the next articles, you will learn more details about transaction processing in OceanBase. Stay tuned!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Implementing a Vectorized Engine in a Distributed Database]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/vectorized-engine</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/vectorized-engine</guid>
            <pubDate>Tue, 04 Jun 2024 13:18:36 GMT</pubDate>
            <description><![CDATA[oceanbase database]]></description>
            <content:encoded><![CDATA[<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306489558.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>Photo by <a href="https://unsplash.com/@whatispictureperfect?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener noreferrer">What Is Picture Perfect</a> on <a href="https://unsplash.com/s/photos/vectors?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener noreferrer">Unsplash</a></p>
<p><em>This article is written by Qu Bin, a tech expert at OceanBase. Qu Bin has years of experience in the database industry and he had worked on column-oriented and time-series database kernel development. Currently, he is mainly working on vectorized engine development.</em></p>
<p>This article introduces the value, design, and technical solutions of vectorized engines from the following three aspects:</p>
<ul>
<li>What is vectorized engine?</li>
<li>Why vectorized engine?</li>
<li>Implementing a vectorized engine</li>
</ul>
<p>When talking to customers, we found that many users want to perform OLAP tasks such as JOIN queries and aggregate analysis while they are processing online transactions. The SQL execution engine of a database must be highly productive in order to deal with OLAP tasks, which often involve the processing of massive data and complicated computing and queries, and are therefore time-consuming.</p>
<p>We used to use parallel execution to evenly share workloads among multiple CPU cores in the database’s distributed architecture and successfully cut the query response time (RT) by reducing the amount of data processed by each CPU core. As the user data builds up, the workload of each CPU also increases, if no computing resources are added. On-site investigations indicate that the CPU utilization approximates 100% in some OLAP tasks, such as aggregate analysis and JOIN queries when massive data is involved.</p>
<p>To improve single-core processing performance and reduce response time, we have designed a vectorized query engine from scratch.</p>
<h1>What is Vectorized Engine?</h1>
<p>The concept of the vectorized engine was introduced in a 2005 paper titled “MonetDB/X100: Hyper-Pipelining Query Execution”. Prior to the vectorized engine era, the volcano model was widely adopted in the database industry.</p>
<p>Originally known as the iterator model, the volcano model was formally introduced in the 1994 paper “Volcano — An Extensible and Parallel Query Evaluation System”, and was adopted by many early versions of mainstream relational databases at that time.</p>
<p>In the early years of database technologies, when database I/O was slow and memory and CPU resources were expensive, database experts developed the classic volcano model, which allows an SQL engine to compute one data row at a time to avoid memory exhaustion.</p>
<p>Although the volcano model has been widely applied, its design does not bring out the full potential of CPU performance. And it often causes data congestion during complex queries such as JOIN queries, queries with subqueries, and queries containing the ORDER BY keyword.</p>
<p>In the paper “DBMSs On A Modern Processor: Where Does Time Go?”, the authors have minutely dissected the resource consumption of database systems in the framework of modern CPUs.</p>
<p>The following figure clearly shows that the CPU time for computation is not greater than 50% in sequential scans, index-based scans, and JOIN queries. On the contrary, the CPU spends quite an amount of time (50% on average) waiting for resources, due to memory or resource stalls. Plus the cost of branch mispredictions, the percentage of CPU time for computation is often far less than 50%. For example, the minimum percentage of CPU time for computation in index-based scans is less than 20%.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306497765.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>CPU time breakdown for SQL execution</p>
<p>Unlike the traditional volcano model which iterates data row by row, the vectorized engine adopts vector iteration to pass multiple rows of data at a time from one operator to another.</p>
<p>In other words, vectorization makes a great leap from single-cell operations to vector operations.</p>
<h1>Why Vectorized Engine?</h1>
<p>There are two advantages of the vectorized engine.</p>
<p>1. A cache-friendly method that returns vector data with fewer function calls</p>
<p>To further improve CPU utilization and reduce the memory/resource stalls during SQL execution, vectorized engines are introduced and applied to the design of modern databases.</p>
<p>Similar to the traditional volcano model, a vectorized engine also pulls data from the root node of an operator tree. The difference is that the vectorized engine passes vector data at a time and keeps the data as compact as possible in the memory, rather than calling the next() function to pass one row at a time. Since the data is contiguous, the CPU can quickly load the data to the level-2 cache (L2 cache) by instruction prefetching, which reduces memory stalls and thus improves CPU utilization. The contiguous and compact data in the memory also make it possible to process a set of data at a time by running a SIMD instruction. This brings the computing power of modern CPUs into full play.</p>
<p>The vectorized engine drastically reduces the number of function calls. Assuming that you want to query a table of 100 million rows of data, a database of the volcano model must perform 100 million iterations to complete the query. If you use a vectorized engine and set the vector size to 1024 rows, the number of function calls to execute the query is significantly reduced to 97,657, which is calculated by dividing 100 million by 1024. Inside an operator, the function crunches a chunk of data by traversing the data in a loop instead of nibbling one row at a time. Such vector processing of contiguous data is more friendly to the dCache and iCache of CPUs and reduces cache misses.</p>
<p>2. Higher CPU capabilities to process an instruction stream with fewer branch predictions</p>
<p>The paper “DBMSs On A Modern Processor: Where Does Time Go?” also indicates that branch mispredictions have a serious impact on the database performance because the CPU halts the execution of an instruction stream and refreshes the pipeline upon a misprediction. The paper “Micro Adaptivity in Vectorwise” released at the 2013 ACM SIGMOD Conference on Management of Data (SIGMOD’13) also elaborates on the execution efficiency of branching at different selectivities. A figure is provided below for your information.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306505841.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>Impact of branching on execution</p>
<p>The logic of the SQL engine of a database is complicated. Therefore, conditional logic is inevitable for the volcano model. In contrast, the vectorized engine can keep conditionals at the minimum in an operator. For example, the vectorized engine can avoid an IF statement within a FOR loop by overriding data writes by default, thus protecting the CPU pipeline from branch mispredictions and greatly improving CPU capabilities.</p>
<p>3. Faster computation accelerated by SIMD instructions</p>
<p>The vectorized engine handles contiguous data in the memory, hence can easily load a set of data into a vector register. It then sends a single instruction, multiple data (SIMD) instruction to perform vector computation instead of using the traditional scalar algorithm. Note that the SIMD instruction is closely related to the CPU architecture, and corresponding instruction sets are provided for x86, ARM, and PPC architectures. At present, the Intel x86 architecture supports the most instructions. The figure below shows SIMD instructions for the x86 architecture and the data types that each instruction supports. For more information, see the official manual of Intel.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306515132.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>Data types supported by Intrinsic instructions of Intel</p>
<h1>Implementing a Vectorized Engine in OceanBase</h1>
<p>This section details the implementation of the OceanBase vectorized engine from the following aspects: storage, data organization, and SQL operators.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="storage-vectorization">Storage Vectorization<a href="https://oceanbase.github.io/zh-Hans/blog/vectorized-engine#storage-vectorization" class="hash-link" aria-label="Storage Vectorization的直接链接" title="Storage Vectorization的直接链接">​</a></h2>
<p>OceanBase Database stores data in microblocks, the minimum unit of I/O operations. The size of each microblock is 64 KB by default and can be resized.</p>
<p>In each microblock, the data is stored in columns. During a query, sets of data in a microblock are projected to the memory of the SQL engine by columns. Thanks to the compact structure, the data can be easily cached, and the projection process can be accelerated by using SIMD instructions. Since the vectorized engine does not maintain physical rows, it fits well with the data storage mode in a microblock. This makes data processing simpler and more efficient. The data storage and projection logic is illustrated in the following figure.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306524742.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>Vectorized storage of OceanBase Database</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-organization-in-a-vectorized-sql-engine">Data Organization in a Vectorized SQL Engine<a href="https://oceanbase.github.io/zh-Hans/blog/vectorized-engine#data-organization-in-a-vectorized-sql-engine" class="hash-link" aria-label="Data Organization in a Vectorized SQL Engine的直接链接" title="Data Organization in a Vectorized SQL Engine的直接链接">​</a></h2>
<p><strong>Memory orchestration</strong></p>
<p>In the SQL engine, all data is stored in expressions. Expressions are managed by a Data Frame, a contiguous piece of memory of no more than 2 MB in size. In other words, Data Frame holds the data of all expressions involved in SQL queries, and the SQL engine allocates the required memory from the Data Frame.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306532988.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>Memory orchestration in the OceanBase SQL engine</p>
<p>In a non-vectorized engine, an expression processes only one row of data cells at a time, as shown in the left part of the above figure. In a vectorized engine, an expression stores multiple rows of compactly structured data cells, as shown in the right part of the above figure.</p>
<p>This way, the data in an expression is computed as a vector, which is more friendly to the CPU cache. The compact data structure also allows easy computation acceleration by using SIMD instructions.</p>
<p>In addition, the number of cells allocated to each expression, or the vector size, can be adjusted based on the size of the L2 cache of the CPU and the number of expressions in an SQL statement. The principle of vector resizing is to ensure that all cells involved in the computation are stored in the L2 cache of the CPU to reduce the impact of memory stalls.</p>
<p><strong>Design of filter representations</strong></p>
<p>The filter representations of vector engines need to be redesigned. This is because a vector engine returns vector data at a time, and only a part of the data is output. Other data is filtered out. It is important to efficiently identify the data to be output or the valid data. The paper “Filter Representation in Vectorized Query Execution” compares the following two common strategies in the industry:</p>
<ul>
<li>Identifying rows by bitmaps. In this strategy, a bitmap is created to include a number of bits that equals the size of the returned data vector. The bit of a valid row is set to 1, whereas the bit of an invalid row is set to 0.</li>
<li>Recording valid rows by using an additional selection vector. In this strategy, the subscripts of valid rows are stored in a selection vector.</li>
</ul>
<p>OceanBase Database uses the bitmap strategy because, among other advantages, it occupies a small memory space. This prevents the out-of-memory (OOM) error especially when a query involves too many operators or extra-large vectors.</p>
<p>The data identified by a bitmap is sparse when the data selectivity is low, which may lead to unsatisfactory performance. Some databases tackle this by adding sorting methods to densify the data. However, we have found in practice that the SQL execution under HTAP workloads often involves blocking operators, such as Sort, Hash Join, and Hash Group By, or transmission operators. These operators intrinsically output dense data. Extra data sorting only causes unnecessary overheads. Therefore, the OceanBase vectorized engine does not provide a method to modify the bitmap structure.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="implementing-operators-in-a-vectorized-engine">Implementing Operators in a Vectorized Engine<a href="https://oceanbase.github.io/zh-Hans/blog/vectorized-engine#implementing-operators-in-a-vectorized-engine" class="hash-link" aria-label="Implementing Operators in a Vectorized Engine的直接链接" title="Implementing Operators in a Vectorized Engine的直接链接">​</a></h2>
<p>The vectorization of operators is an essential part of the OceanBase vectorized engine. To support the vectorized engine, all query operators are redesigned to fit its characteristics. In accordance with the working principles of the vectorized engine, each operator fetches vector data from the lower-level operator through the vector interface, and the data in each operator is engineered by following guidelines such as branchless programming, memory prefetching, and SIMD instructions. This allows the database to maximize performance improvement. As a large number of operators are involved, I would like to take Hash Join and Sort Merge Group By as examples.</p>
<p><strong>Hash Join</strong></p>
<p>Hash Join implements a Hash lookup of two tables, say tables R and S, by creating and probing a Hash table. When the Hash table is larger than the L2 cache of the CPU, the random access of the Hash table will cause memory stalls and greatly reduce the execution efficiency. Therefore, cache optimization is an important part of Hash Join vectorization, where the impact of cache misses on performance is addressed as a top priority.</p>
<p>It is worth mentioning that the vectorized Hash Join operator of OceanBase Database does not implement hardware-conscious Hash Joins such as Radix Hash Join. Cache misses and memory stalls are avoided by vector-based Hash value computation and memory prefetching.</p>
<p>Radix Hash Join effectively reduces the cache and Translation Lookaside Buffer (TLB) misses, but it needs to scan table R twice and incurs the cost of creating histograms and additional materialization. The vectorization of Hash joins is more streamlined in OceanBase Database. First, a Hash table is created based on the partitions of tables S and R. At the Hash table probe operation, the vectorized Hash value is first obtained by vector computation. Then, the data in the Hash bucket corresponding to that vector is prefetched and loaded into the CPU cache. Finally, the results are compared based on the Join conditions. By controlling the vector size, it is ensured that the prefetched vector data can be loaded into the L2 cache of the CPU. This way, cache misses and memory stalls can be kept at a minimum during data comparison, thereby improving CPU utilization.</p>
<p><strong>Sort Merge Group By</strong></p>
<p>Sort Merge Group By is a common aggregation operation. It sorts the data in order, uses the Group By operator to find the grouping boundaries based on data comparison, and then computes the data in the same group.</p>
<p>For example, the data in column c1 is ordered, as shown in the figure below.</p>
<p><img decoding="async" loading="lazy" src="https://obportal.s3.ap-southeast-1.amazonaws.com/obc-blog/img/d105da79260f4d6a8a03571e4a2b17091682306542965.jpg" alt="oceanbase database" class="img_ev3q"></p>
<p>Vectorization of the Sort Merge Group By operator</p>
<p>In the volcano model that iterates only one data row at a time, eight comparisons are required for Group 1, and sum(c1) needs to perform eight additions to get the result.</p>
<p>The vectorized engine performs the comparison and aggregation separately. In other words, it first performs eight comparisons to determine the number of data rows in Group 1. As the data values in the group are the same, sum() and count() can be further optimized. In this example, sum(c1) can be performed by 1 x 8, and count(c1) can be directly added by 8.</p>
<p>In addition, vectorization can also speed up computation by introducing methods such as dichotomization. Assuming that a 16-row vector is defined for column c1 in the figure above. Under the dichotomization strategy, the engine takes the first step of 8 rows to compare the data from row 0 to row 7. If the data is equal, it sums up the first 8 data rows. In the second step of 8 rows, the engine compares the data from row 7 to row 15. If the data are not equal, it goes back by 4 rows and compares the data again, until it finds the grouping boundary. After that, the engine repeats the preceding steps to find the next grouping boundary. Under the dichotomization strategy, duplicate data can be skipped during comparison, which predicates faster computation. Dichotomization delivers poor performance if the column contains few duplicate data. We can decide whether to enable dichotomization during the execution based on statistics such as the number of distinct value (NDV).</p>
<p>The OceanBase vectorized engine is under constant upgrade. For example, the current SIMD instructions of OceanBase Database are written for the AVX-512 instruction set, which is applicable to the x86 instruction set architecture. With the increase of ARM-based applications, we will provide SIMD instructions for ARM-based systems. Furthermore, as a lot of operators can be optimized to support the vectorized engine, the OceanBase team will continue to work on this and integrate more new operators and technical solutions with the vectorized engine to better support users in their businesses.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[BOSS Zhipin —— How to save 70% storage cost through OceanBase with an archive database of 1 billion rows per day?]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/boss-zhipin</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/boss-zhipin</guid>
            <pubDate>Tue, 04 Jun 2024 09:30:23 GMT</pubDate>
            <description><![CDATA[Author: Zhang Yujie, Database Engineer with BOSS Zhipin]]></description>
            <content:encoded><![CDATA[<p>Author: Zhang Yujie, Database Engineer with BOSS Zhipin</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="i-background">I. Background<a href="https://oceanbase.github.io/zh-Hans/blog/boss-zhipin#i-background" class="hash-link" aria-label="I. Background的直接链接" title="I. Background的直接链接">​</a></h2>
<p>BOSS Zhipin is the first "direct-hiring" online recruitment service in the world, and has become China's largest job searching platform. An important job of my team is to store the conversation records during the recruitment process into a database, which has held a tremendous amount of data and is taking in 500 million to 1 billion data records on a daily basis. However, these conversation records are rarely or never accessed or updated after they are written to the database. The growing volume of online data, especially the cold historical conversation records, has occupied petabytes of storage space of the online business database, resulting in serious waste of hardware resources and escalating IT costs. In addition to bloating the online business database, the increasing data volume also reduces the query efficiency, which hinders data changes and system scaling.</p>
<p>To address these issues, we need to separate hot data from cold historical conversation records. The hot data is stored in a sharded online database of multiple MySQL clusters. Expired data is regularly migrated to the archive database every month.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ii-database-selection">II. Database Selection<a href="https://oceanbase.github.io/zh-Hans/blog/boss-zhipin#ii-database-selection" class="hash-link" aria-label="II. Database Selection的直接链接" title="II. Database Selection的直接链接">​</a></h2>
<p>To build an archive database with a humongous capacity, we compared MySQL, ClickHouse, OceanBase Database, and an open-source distributed database (let's call it "DB-U") in terms of storage costs and high availability.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="i-storage-costs">(I) Storage costs<a href="https://oceanbase.github.io/zh-Hans/blog/boss-zhipin#i-storage-costs" class="hash-link" aria-label="(I) Storage costs的直接链接" title="(I) Storage costs的直接链接">​</a></h3>
<p>We need to retain historical conversation data for three to five years and must control the cost of massive storage. First, we created a table in each of the databases to store historical messages. The table schemas are the same. See the following figure.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241250230.png" alt="1706241250" class="img_ev3q"></p>
<p>Then, we wrote the same 100 million data rows to each of the tables, and compared their disk usage. See the following figure.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241310742.png" alt="1706241310" class="img_ev3q"></p>
<p>Apparently, compared with MySQL and DB-U, columnar storage-based ClickHouse and OceanBase Database boasting an ultra-high compression ratio incur significantly lower storage costs. We then delved deeper into the storage engines of the two winners.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="clickhouse-storage-engine">ClickHouse storage engine<a href="https://oceanbase.github.io/zh-Hans/blog/boss-zhipin#clickhouse-storage-engine" class="hash-link" aria-label="ClickHouse storage engine的直接链接" title="ClickHouse storage engine的直接链接">​</a></h4>
<p>ClickHouse uses smaller storage simply because of its column-based storage engine. Compared with a row-based storage engine, the data stored in the same column of the ClickHouse database is of the same type, and thus can be compressed more compactly. Generally, the compression ratio of columnar storage can reach 10 or higher, saving a lot of storage space and reducing the storage costs significantly.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241332521.png" alt="1706241332" class="img_ev3q"></p>
<p>However, as an archive database often involves more writes and is rarely read, a pure column-based storage engine such as ClickHouse cannot bring its great query performance into full play. Instead, its lame write performance becomes intolerable.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="oceanbase-database-storage-engine">OceanBase Database storage engine<a href="https://oceanbase.github.io/zh-Hans/blog/boss-zhipin#oceanbase-database-storage-engine" class="hash-link" aria-label="OceanBase Database storage engine的直接链接" title="OceanBase Database storage engine的直接链接">​</a></h4>
<p><strong>1. Architecture</strong></p>
<p>Based on the LSM-tree architecture, the storage engine of OceanBase Database stores baseline data in baseline SSTables and incremental data in MemTables and incremental SSTables. The baseline data is read-only and cannot be modified. Incremental data supports read and write operations.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241405527.png" alt="1706241405" class="img_ev3q"></p>
<p>In OceanBase Database, DML operations such as INSERT, UPDATE, and DELETE are first written into MemTables in memory. This means that the data write performance is equivalent to that of an in-memory database, which agrees with the write-intensive scenario of our archive database. When the size of a MemTable reaches the specified threshold, data in the MemTable is dumped to an incremental SSTable on the disk, as shown by the red chevrons in the preceding figure. The dumping process is performed sequentially in batches, delivering much higher performance compared with the discrete random writing of a B+ tree-based storage engine.</p>
<p>When the size of an incremental SSTable reaches the specified threshold, incremental data in the SSTable is merged with the existing baseline data to produce new baseline data. This process is referred to as a major compaction. Then, the new baseline data remains unchanged until the next major compaction. OceanBase Database automatically performs a major compaction during off-peak hours early in the morning on a daily basis.</p>
<p>A downside of the log-structured merge-tree (LSM-tree) architecture is that it causes read amplification, as shown by the green arrows in the preceding figure. Upon a query request, OceanBase Database queries SSTables and MemTables separately, merges the query results, and then returns the merged result to the SQL layer. To mitigate the impact of read amplification, OceanBase Database implements multiple layers of caches in memory, such as the block cache and row cache, to avoid frequent random reads of baseline data.</p>
<p><strong>2. Data compression technology</strong></p>
<p>Given the optimized storage architecture, OceanBase Database compresses data during the major compaction, when data is written to the baseline SSTable. This way, online data updates are independent of the major compaction.</p>
<p>OceanBase Database supports both compression and encoding. Compression does not consider data characteristics, while encoding compresses data by column based on data characteristics. The two methods are orthogonal, meaning that we can first encode a data block and then compress it to achieve a higher compression ratio.</p>
<p>OceanBase Database also supports batch persistence. This feature allows it to adopt more aggressive compression strategies. OceanBase Database uses the Partition Attributes Cross (PAX) storage mode, which features a hybrid row-column storage architecture based on microblocks. In a microblock, a group of rows are stored and encoded based on data characteristics by column, fully leveraging the locality and type characteristics of data in the same column. Variable-length data blocks and batch-compressed continuous data allow OceanBase Database to guide the compression of the next data block by using the prior knowledge derived from the compressed data blocks in the same SSTable, so that it can compress as many data rows as possible into a data block by selecting a better encoding algorithm.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241431761.png" alt="1706241431" class="img_ev3q"></p>
<p>Unlike some database implementations that specify data encoding on the schema, OceanBase Database employs adaptive data encoding, which works without manual intervention, reducing the workload of users and storage costs. We just need to adjust a couple of compression and encoding parameters for the archive database to start working.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ii-high-availability-and-stability">(II) High availability and stability<a href="https://oceanbase.github.io/zh-Hans/blog/boss-zhipin#ii-high-availability-and-stability" class="hash-link" aria-label="(II) High availability and stability的直接链接" title="(II) High availability and stability的直接链接">​</a></h3>
<p>We also compared the high availability and stability of ClickHouse and OceanBase Database.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="clickhouse">ClickHouse<a href="https://oceanbase.github.io/zh-Hans/blog/boss-zhipin#clickhouse" class="hash-link" aria-label="ClickHouse的直接链接" title="ClickHouse的直接链接">​</a></h4>
<p>We fully tested the automatic data synchronization performance of the ClickHouse database through data replication across different servers in the cluster to ensure high availability and fault tolerance. ZooKeeper was used to coordinate the replication process, track the status of replicas, and guarantee data consistency among them. This way, multiple data replicas are hosted on different servers to minimize the risk of data loss.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241453557.png" alt="1706241453" class="img_ev3q"></p>
<p>However, we noticed some drawbacks of this ClickHouse-based high availability solution in handling massive amounts of data. During data replication between multiple replicas, a lot of information was stored on ZooKeeper. The problem is that, ZooKeeper does not support linear scaling, which means its performance is limited by the capacity of a server. As more data was written to the archive cluster, ZooKeeper services soon became unavailable.</p>
<p>In fact, when working with ClickHouse, ZooKeeper is more like a multitasker than just a coordination service. For example, it works as a log service and stores information such as behavior logs. It also plays the role of a catalog service, verifying the schema information of tables. As a result, the data volume handled by ZooKeeper increases linearly with that of the database. Given the expected data growth rate of our archive database, this ClickHouse + ZooKeeper solution cannot survive the full data archiving over a time span of three to five years.</p>
<p>Furthermore, data replication in the ClickHouse database depends heavily on ZooKeeper, which, as an external coordination service, introduces additional complexity in terms of system configuration and maintenance. Exceptions of ZooKeeper are likely to affect the replication process in the ClickHouse database. This high availability solution also prolongs troubleshooting paths, making it more difficult to locate a fault. The recovery process becomes more complicated, requiring manual intervention. Data loss was common in the test.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="oceanbase-database">OceanBase Database<a href="https://oceanbase.github.io/zh-Hans/blog/boss-zhipin#oceanbase-database" class="hash-link" aria-label="OceanBase Database的直接链接" title="OceanBase Database的直接链接">​</a></h4>
<p>OceanBase Database is a native distributed database system that guarantees the data consistency between multiple replicas based on the Paxos distributed consensus protocol. The Paxos protocol ensures that a unique leader is elected to provide data services only when the majority of replicas in the OceanBase cluster reach a consensus. In other words, OceanBase Database guarantees high database availability by using multiple replicas and the Paxos protocol.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241467288.png" alt="1706241467" class="img_ev3q"></p>
<p>Compared with MySQL and ClickHouse, the high availability solution based on OceanBase Database makes our database O&amp;M and business updates easier. OceanBase Database also supports multireplica and multiregion architecture, so that data replicas can be stored in IDCs in the same city or different regions for the purpose of geo-disaster recovery.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241475029.png" alt="1706241475" class="img_ev3q"></p>
<p>Featuring a distributed architecture, OceanBase Database inherently supports dynamic data storage scaling. As the data volume of the archive database keeps growing, our database administrators (DBAs) only need to run a command or two to scale up a node by modifying the quota of hardware resources, or scale out a cluster by adding more nodes. After a new node is added to the cluster, OceanBase Database automatically balances the workload among the new and old nodes. The scaling process is smooth without business interruptions or downtime. This feature also saves the costs of database scale-out and data migration in response to business surges, leading to a great reduction of the risk due to insufficient database capacity.</p>
<p>The best part is, when we want to increase the capacity of a single node, add more nodes into a zone, or add a new zone to reach higher availability, we can perform these operations on OceanBase Cloud Platform (OCP), a GUI-based OceanBase Database management tool. The following figure shows a page of OCP in the process of scaling a single-zone cluster to a three-zone one.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241482608.png" alt="1706241482" class="img_ev3q"></p>
<p>Compared with a command-line tool, OCP is more user-friendly for the deployment and O&amp;M of OceanBase Database, according to our DBAs, who recommended OCP.</p>
<p>To sum up, unlike MySQL and ClickHouse, OceanBase Database natively guarantees strong storage consistency. It does not compromise the eventual consistency for other capabilities, or rely on miscellaneous complex peripherals to ensure data consistency. Multi-replica disaster recovery is applicable to individual clusters. Transaction logs are persisted, and logs are synchronized between multiple replicas. The Paxos protocol guarantees log persistence for the majority of replicas. OceanBase Database provides high availability with a recovery point objective (RPO) of 0 and a recovery time objective (RTO) of less than 8s when a minority of replicas fail. In the test, OceanBase Database outperformed MySQL, ClickHouse, and DB-U, demonstrating higher system stability.</p>
<p>Based on a comprehensive assessment, factoring in the storage costs, high availability, and O&amp;M difficulty, we decided to build our archive database by using OceanBase Database.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="iii-implementation">III. Implementation<a href="https://oceanbase.github.io/zh-Hans/blog/boss-zhipin#iii-implementation" class="hash-link" aria-label="III. Implementation的直接链接" title="III. Implementation的直接链接">​</a></h2>
<p>Our online database is a MySQL cluster deployed in primary/standby mode. We use it to store hot data, mainly user conversation records in the last 30 days. Our archive database consists of several OceanBase clusters, which are managed by OCP. We regularly migrate expired data from the MySQL database to the archive database on a monthly basis using our internal data transmission service (DTS) tool. The following figure shows the overall architecture.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241500535.png" alt="1706241500" class="img_ev3q"></p>
<p>So far, eight OceanBase archive clusters, comprising more than 20 tenants, are managed on OCP. And, our app keeps writing data by user ID hash to the MySQL database, which has hosted more than 10,000 table shards. Expired data is directly imported into the OceanBase archive database.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241507211.png" alt="1706241507" class="img_ev3q"></p>
<p>Our old ClickHouse archive cluster is still in use to support reads of some historical data. However, considering the stability and security issues of ClickHouse, we will gradually replace it with OceanBase Database.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="iv-benefits">IV. Benefits<a href="https://oceanbase.github.io/zh-Hans/blog/boss-zhipin#iv-benefits" class="hash-link" aria-label="IV. Benefits的直接链接" title="IV. Benefits的直接链接">​</a></h2>
<p>To begin with, the powerful compression capability of OceanBase Database helps us archive cold data with ease, and saves storage resources by more than 70%.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241521817.png" alt="1706241521" class="img_ev3q"></p>
<p>In addition, OceanBase Database is a native distributed system with great scalability. It provides high availability with an RPO of 0 and an RTO of less than 8s when a minority of replicas fail, making the database more stable.</p>
<p>Last but not least, OceanBase Database comes with GUI-based OCP, which is a lifesaver for our DBAs in handling deployment and O&amp;M tasks. OCP allows us to manage objects such as clusters, tenants, hosts, and software packages over their entire lifecycle, including their installation, O&amp;M, performance monitoring, configuration, and upgrade. OCP of the latest version supports custom alerts. For example, we can set custom disk and memory usage thresholds. Once the usage exceeds the thresholds, we receive alerts immediately. OCP also supports backup and restore, as well as automated diagnostics during O&amp;M.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241530556.png" alt="1706241530" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="v-outlook">V. Outlook<a href="https://oceanbase.github.io/zh-Hans/blog/boss-zhipin#v-outlook" class="hash-link" aria-label="V. Outlook的直接链接" title="V. Outlook的直接链接">​</a></h2>
<p><strong>1. Support for online distributed database capabilities</strong></p>
<p>As mentioned above, we still use a MySQL online database. Compared with using a single table in a distributed database, performing database and table sharding in the MySQL database is more complex. If the data in multiple data tables or databases is associated, the difficulty of maintaining data consistency increases drastically.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241545453.png" alt="1706241545" class="img_ev3q"></p>
<p>In addition to data consistency challenges, the complexity of O&amp;M and management of multiple database and table shards also makes system troubleshooting and maintenance a headache. As data is stored in multiple database and table shards, it is hard to trace historical data in this online MySQL database.</p>
<p>At present, many of our upper-level business modules rely on the online MySQL database, and it may take some time for us to replace it with OceanBase Database.</p>
<p>The good news is, after the introduction of OceanBase Database, we have improved our capabilities to support natively distributed database tables, and developed more convenient and feasible solutions for transforming business modules with large storage capacity and complex sharding logic.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706241553431.png" alt="1706241553" class="img_ev3q"></p>
<p><strong>2. ODC and Binlog service</strong></p>
<p>We learned that since V4.2.2, OceanBase Developer Center (ODC), a GUI-based database development tool tailored for OceanBase Database, supports data archiving from MySQL to OceanBase Database and within OceanBase Database. It also supports configuration of automatic tasks at multiple levels. Considering the high compression ratio of OceanBase Database and the data archiving capabilities of ODC, it is quite easy to build an archive database based on OceanBase Database.</p>
<p>Currently, our business team mainly uses the internal RDS platform to call the DTS tool for data archiving. ODC is used as a supplement to the DTS tool. We will continue to learn other features of ODC to strengthen our capabilities in database O&amp;M.</p>
<p>OceanBase Database V4.2.1 and later provide the Binlog service, which allows downstream services, such as data warehouses, of the sharded MySQL database to subscribe to binlogs in a unified way, rather than separately subscribing to binlogs of each MySQL shard, making the binlog subscription easier.</p>
<p><strong>3. Exploration of best practices</strong></p>
<p>The introduction of the new database system brings more challenges to our DBAs. To better tap the potential of OceanBase Database, they must select hardware of proper specifications and continuously optimize service configurations, while maintaining the stability of the database. We will keep working with the OceanBase team to figure out the most efficient and cost-effective way in using OceanBase Database, and provide strong support for the rapid and stable development of our business.</p>]]></content:encoded>
            <category>User Case</category>
        </item>
        <item>
            <title><![CDATA[DMALL —— a summary of database selection experience in SaaS scenarios]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/dmall</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/dmall</guid>
            <pubDate>Tue, 04 Jun 2024 09:30:23 GMT</pubDate>
            <description><![CDATA[Feng Guangpu, head of the Dmall database team, is responsible for the stability of OceanBase, TiDB, MySQL, Redis, and other databases of Dmall and the construction of its database platform as a service (PaaS) model. Feng has a wealth of experience in multi-active database architecture and data synchronization schemes.]]></description>
            <content:encoded><![CDATA[<blockquote>
<p>Feng Guangpu, head of the Dmall database team, is responsible for the stability of OceanBase, TiDB, MySQL, Redis, and other databases of Dmall and the construction of its database platform as a service (PaaS) model. Feng has a wealth of experience in multi-active database architecture and data synchronization schemes.</p>
</blockquote>
<blockquote>
<p>Yang Jiaxin, a senior DBA at Dmall, is an expert in fault analysis and performance optimization and loves exploring new technologies.</p>
</blockquote>
<p>As the largest retail cloud solution and digital retail service provider in Asia and the only end-to-end service provider of omnichannel retail cloud solutions in China, Dmall conducts business in six countries and regions. Therefore, its model has been widely verified. The development of Dmall is a microcosm of the retail digitalization process in China and even the world.</p>
<p>However, the rapid business growth and impressive business achievements have been accompanied by many challenges in retail software as a service (SaaS) scenarios and business system bottlenecks. This article takes a look at the data processing pain points in retail SaaS scenarios from the business perspective. It gives a glimpse into how Dmall improves its read and write performance and lowers storage costs while ensuring the stability and reliability of its business database.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="characteristics-and-pain-points-of-retail-saas-scenarios">Characteristics and pain points of retail SaaS scenarios<a href="https://oceanbase.github.io/zh-Hans/blog/dmall#characteristics-and-pain-points-of-retail-saas-scenarios" class="hash-link" aria-label="Characteristics and pain points of retail SaaS scenarios的直接链接" title="Characteristics and pain points of retail SaaS scenarios的直接链接">​</a></h2>
<p>Dmall markets its services across borders. In China, our customers include large supermarkets, such as Wumart and Zhongbai, as well as multinational retailers, such as METRO AG and 7-Eleven. Dmall also serves many Chinese and international brands. It links brand owners, suppliers, and retailers with smooth data and information flows so that they can better support and serve consumers.</p>
<p>The long service chain, from manufacturers, brand owners, and suppliers, to retailers in various shopping malls and stores, and finally to consumers, generates massive data volumes. The system complexity increases exponentially with the data volume. As a result, the retail SaaS system of Dmall faces three major challenges:</p>
<ul>
<li>
<p>High O&amp;M complexity</p>
<p>Dmall uses a microservice architecture that involves many business links in the overall process and a large system application scale. Dmall already has more than 500 databases. Moreover, as our system continue to iterate, the data scale continues to increase, and O&amp;M management is becoming more and more difficult.</p>
</li>
<li>
<p>Fast business growth and frequent need for horizontal scaling</p>
<p>Dmall formulated a global expansion strategy to cope with its business growth. According to the requirements of regional data security laws, we need to deploy a new system to undertake business traffic outside China. It is hard to predict future business scale and data growth in the initial deployment phase. Therefore, database resource allocation in the initial deployment phase is quite difficult. The common practice is to deploy the system with limited resources at low costs. However, rapid business growth and exponential data increase will require quick scaling.</p>
</li>
<li>
<p>The need to serve a large number of merchants in the same cluster</p>
<p>The number of stock keeping units (SKUs) of different convenience stores and supermarket chains ranges from thousands to tens of thousands. Therefore, it is impossible for us to deploy an independent system for each customer. This means that our SaaS system must support hundreds of small and medium-sized business customers, and the data generated by all merchants share database resources at the underlying layer. Moreover, we have massive individual tenants in our system, such as large chain stores. We want to isolate the resources for these tenants from those for others.</p>
</li>
</ul>
<p>In short, our database needs to support a huge data scale and cope with rapid data growth.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-we-chose-a-distributed-database">Why we chose a distributed database<a href="https://oceanbase.github.io/zh-Hans/blog/dmall#why-we-chose-a-distributed-database" class="hash-link" aria-label="Why we chose a distributed database的直接链接" title="Why we chose a distributed database的直接链接">​</a></h2>
<p>To address the aforementioned issues and requirements, we started looking for a new database solution. A distributed database provides larger capacity, transparent scaling, financial-level data security, higher development efficiency, and lower O&amp;M costs. Therefore, it can better support our business development. Due to these benefits and advantages, we believe that distributed databases will become the prevalent trend in the data field. This is why we only looked at distributed database products during our database selection process.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="business-based-database-selection-considerations">Business-based database selection considerations<a href="https://oceanbase.github.io/zh-Hans/blog/dmall#business-based-database-selection-considerations" class="hash-link" aria-label="Business-based database selection considerations的直接链接" title="Business-based database selection considerations的直接链接">​</a></h3>
<p>First of all, we have many MySQL databases each with over 4 TB of data, and the data size is still growing rapidly. After we migrated our largest MySQL database to a distributed database, the data size increased to 29 TB. Our DBAs are very concerned about the capacity bottleneck of MySQL:</p>
<ul>
<li>We have a few choices. We can continue to push R&amp;D for data cleanup and data archiving, but this may delay other R&amp;D jobs such as upgrades to better address our business needs.</li>
<li>Alternatively, we can continue to expand the disks. This method may be easier to carry out on the cloud. The size of a single cloud storage disk can reach 32 TB or even larger. However, as the data volume continues to grow, this method simply puts the problem off, rather than solving it. In the end, the problem will simply become more and more difficult to solve.</li>
</ul>
<p>We can also choose the database and table sharding solution, but this is an intricate and highly risky solution, and requires several months for code reconstruction to guarantee SQL capabilities.</p>
<p>Therefore, we want to utilize the transparent scaling capabilities of distributed databases to smoothly support rapid business growth.</p>
<p>First, we aim to reduce the O&amp;M complexity and costs while ensuring system stability. If we think of a MySQL database as an egg and a MySQL instance as a basket, how many MySQL instances should we deploy to house 1000 databases? Which databases should we put on the same instance? If we put two resource-demanding databases in the same instance, resource preemption may occur. In addition, some workloads have special requirements. For example, although payment transactions generate only a small amount of data, they have high business requirements. Therefore, a payment transaction database cannot be deployed in the same instance as other databases. Because of different businesses, different priorities, different data growth rates, and different QPS requirements, DBAs often need to "move eggs from one basket to another". This creates major O&amp;M challenges, not to mention high resource costs. We hope that we can solve this problem with distributed databases, enabling automatic "egg relocation".</p>
<p><img decoding="async" loading="lazy" src="https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/64d2cad9b954a7dde0bfe3b57ebb85c3c3a3b61a" alt="" class="img_ev3q"><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993792381.png" alt="1693993792" class="img_ev3q"></p>
<p>Second, we expect distributed databases to help us achieve high cluster availability. We mainly use MHA and Orchestrator to implement high availability for MySQL clusters. However, they are all in the form of "plug-ins", which does not solve the split-brain issue caused by network partitions. The database and high-availability component are two independent pieces of software. Therefore, they lack consistency and coordinated control. High-availability architectures like MySQL Group Replication are reliable because they are based on the Paxos or Raft distributed consensus protocol just like distributed databases such as OceanBase Database and TiDB. Such an architecture can achieve a recovery point objective (RPO) of 0 and a recovery time objective (RTO) of less than 30s.</p>
<p><img decoding="async" loading="lazy" src="https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/6566a6165303bd34aca1fc2adaf2db473d4c5900" alt="" class="img_ev3q"><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993807766.png" alt="1693993807" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="database-selection-based-on-test-data">Database selection based on test data<a href="https://oceanbase.github.io/zh-Hans/blog/dmall#database-selection-based-on-test-data" class="hash-link" aria-label="Database selection based on test data的直接链接" title="Database selection based on test data的直接链接">​</a></h3>
<p>Based on the above database selection considerations, we chose the native distributed database OceanBase Database. We then compared the storage costs and QPS performance of OceanBase Database with MySQL Database in terms of table reading and writing, table reading, and table writing. The following table shows the configurations used for this test.</p>
<table><thead><tr><th></th><th>OceanBase Database</th><th>MySQL Database</th></tr></thead><tbody><tr><td>Community Edition</td><td>v4.1.0</td><td>v5.7.16</td></tr><tr><td>Memory</td><td>Tenant memory size: 16 GB</td><td>innodb_buffer_pool_size: 16 GB</td></tr><tr><td>Single-node configuration</td><td>32C RAID10 SSD</td><td>32C RAID10 SSD</td></tr><tr><td>Disk flush configuration</td><td>Forcible disk flush by default</td><td>sync_binlog = 1 and innodb_flush_log_at_trx_commit = 2</td></tr><tr><td>Degree of parallelism (DOP)</td><td>5, 10, 20, 30, 60, and 120</td><td>5, 10, 20, 30, 60, and 120</td></tr><tr><td>Test modes</td><td>read_write, read_only, and write_only</td><td>read_write, read_only, and write_only</td></tr><tr><td>Duration of a single test</td><td>300s, for a total of 18 tests (Concurrency x Number of test modes)</td><td>300s, for a total of 18 tests (Concurrency x Number of test modes)</td></tr><tr><td>Test method</td><td>Run the <code>obd test sysbench</code> command (that comes with OBD), which will run the <code>sysbench prepare</code>, <code>sysbench run</code>, and <code>sysbench cleanup</code> commands in sequence.</td><td>Run the <code>obd test sysbench</code> command (that comes with OBD), which will run the <code>sysbench prepare</code>, <code>sysbench run</code>, and <code>sysbench cleanup</code> commands in sequence.</td></tr></tbody></table>
<p>Given the same configurations, MySQL Database performs slightly better than OceanBase Database in the Sysbench test with ten tables, each containing 30 million data rows, when the DOP is less than 200. However, in terms of both QPS and latency, the performance of OceanBase Database approaches that of MySQL Database when the DOP increases.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="oceanbase-database-in-different-configurations">OceanBase Database in different configurations<a href="https://oceanbase.github.io/zh-Hans/blog/dmall#oceanbase-database-in-different-configurations" class="hash-link" aria-label="OceanBase Database in different configurations的直接链接" title="OceanBase Database in different configurations的直接链接">​</a></h4>
<p>In the standalone deployment mode, the read and write performance is also affected by the mode of accessing the OBServer nodes.</p>
<p>The performance is 30% to 50% higher when the OBServer nodes are directly accessed than when they are accessed through OBProxy.</p>
<p>Therefore, for the standalone deployment mode, we recommend that you directly connect to OBServer nodes to avoid the extra costs of accessing OBServer nodes through OBProxy.</p>
<p><img decoding="async" loading="lazy" src="https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/1194670243918372120898947e2d916ebbf8df2e" alt="" class="img_ev3q"><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993851203.png" alt="1693993851" class="img_ev3q"></p>
<p>Performance also varies with tenant memory configurations. We can see that the performance of a tenant with 32 GB of memory is 14% higher than that of a tenant with 16 GB of memory.</p>
<p><img decoding="async" loading="lazy" src="https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/6f219cc28f657a56eaf7a3cff84dc4d448adbc12" alt="" class="img_ev3q"><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993879936.png" alt="1693993879" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="tablespace-comparison-between-oceanbase-database-and-mysql-database">Tablespace comparison between OceanBase Database and MySQL Database<a href="https://oceanbase.github.io/zh-Hans/blog/dmall#tablespace-comparison-between-oceanbase-database-and-mysql-database" class="hash-link" aria-label="Tablespace comparison between OceanBase Database and MySQL Database的直接链接" title="Tablespace comparison between OceanBase Database and MySQL Database的直接链接">​</a></h4>
<p>In the monitoring snapshot scenario in the production environment, after we migrated 20 tables with a total of 0.5 billion data rows from MySQL Database to OceanBase Database, the tablespace usage was reduced by a factor of six.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993904435.png" alt="1693993904" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/ecb2401843a0b597b3c9999138346056c2e10681" alt="" class="img_ev3q"></p>
<p>Based on the above test results, we decided to deploy OceanBase Database for the following considerations:</p>
<ul>
<li>In the monitoring snapshot scenario in the production environment, OceanBase Database shows excellent data compression performance, with the space required for a single replica in OceaBase storage being 1/6 that in MySQL storage.</li>
<li>When OceanBase Database is deployed in standalone mode and is connected through OBProxy, it achieves a minimum QPS value of over 10,000 and a minimum average latency of 3 ms, which are slightly inferior to those of MySQL Database. However, the query performance of OceanBase Database increases as the memory size increases. Performance indicators of OceanBase Database show greater improvement than those of MySQL Database as the DOP increases. When the DOP exceeds 200, the performance of OceanBase Database approaches and can even exceed that of MySQL Database.</li>
<li>The MySQL architecture has only one layer, while OceanBase has two, including the OBProxy layer and the OBServer node layer. In standalone deployment, the performance of OceanBase Database is 30% to 50% higher when the OBServer nodes are directly accessed than when they are accessed through OBProxy. This is because each additional layer incurs additional network latency.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-did-we-choose-oceanbase-database-and-what-sets-it-apart">Why did we choose OceanBase Database and what sets it apart?<a href="https://oceanbase.github.io/zh-Hans/blog/dmall#why-did-we-choose-oceanbase-database-and-what-sets-it-apart" class="hash-link" aria-label="Why did we choose OceanBase Database and what sets it apart?的直接链接" title="Why did we choose OceanBase Database and what sets it apart?的直接链接">​</a></h2>
<p>First, OceanBase Database provides an integrated architecture that supports both standalone and distributed deployment modes. What does that mean? A standalone database, like MySQL, can achieve low latency and high performance, while a distributed database supports easy scaling. What problems can these two types of databases address?</p>
<p>In the early business stage, when your data volumes are relatively small, a standalone database like MySQL can offer outstanding performance. Then, when your business starts growing rapidly, a distributed database that supports transparent scaling with almost unlimited capacity can allow you to easily migrate data without code changes or downtime while ensuring high performance.</p>
<p>Second, OceanBase Database, as a native distributed database, naturally supports automatic sharding and migration, load balancing, and other scaling capabilities, enabling transparent scaling without interrupting or affecting your business. Based on the Paxos protocol, OceanBase Database V4.x is further optimized to achieve an RPO of 0 and an RTO of less than 8s, ensuring high system availability.</p>
<p>Third, compared to MySQL Database, OceanBase Database ensures high database performance by minimizing the overhead of the distributed architecture and reduces storage costs by over 80% with a high compression ratio. In addition, the multitenancy feature of OceanBase Database is perfectly suited to SaaS customers, offering easy resource isolation and capacity scaling.</p>
<p>In a distributed database, data processing involves the interaction of memory, disks, and networks. The latency of data reading and writing in memory is 0.1 microseconds, the latency of SSD reading and writing is about 0.1 milliseconds, and the network latency within an IDC is about 0.1 milliseconds, while the network latency across IDCs in the same city is about 3 milliseconds. On the whole, the latency of SSD reading and writing as well as the network latency are about 100 to 1000 times greater than the in-memory read and write latency. The in-memory read and write throughput can reach 100 Gbit/s, the SSD read and write throughput is about 1 Gbit/s to 2 Gbit/s, and the read and write throughput over a 10 Gigabit network is about 1.2 Gbit/s. The SSD and network read and write throughputs are about 100 times less than the in-memory read and write throughput, a difference of two orders of magnitude.</p>
<p>As a standalone database, MySQL places the InnoDB and Server layers in the same process, and therefore offers highly efficient data interaction, making it the undisputed leader in performance and latency. However, for distributed databases in a computing-storage separated architecture, network I/O overheads between the computing and storage layers are inevitable. It is very difficult to mitigate the resulting performance restriction. The unique architecture design of OceanBase Database implements the SQL engine, storage engine, and transaction engine in one process. That is, an OBServer node does both computing and storage. When an application is connected to the OceanBase cluster by using OBProxy, the OBServer nodes report the data routing information to OBProxy. After OBProxy receives an SQL statement from the application layer, it directly forwards the SQL statement to the most appropriate OBServer node for execution based on the routing information. If the data is on one OBServer node, the SQL statement is executed in standalone mode, just like MySQL. This minimizes the network I/O overhead.</p>
<p><img decoding="async" loading="lazy" src="https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/9fb9fc45c8819738607f8b3daabf23a16c12308f" alt="" class="img_ev3q"><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993921469.png" alt="1693993921" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-does-oceanbase-database-perform-better">Why does OceanBase Database perform better?<a href="https://oceanbase.github.io/zh-Hans/blog/dmall#why-does-oceanbase-database-perform-better" class="hash-link" aria-label="Why does OceanBase Database perform better?的直接链接" title="Why does OceanBase Database perform better?的直接链接">​</a></h3>
<p>OceanBase Database significantly outperforms MySQL Database when working with large data volumes or high concurrency. To find out why, we conducted an in-depth study of its architecture. Here are the key points that contribute to the high performance of OceanBase Database:</p>
<p>First, OceanBase Database offers both low latency and high throughput. For production business data, the proportion of single-server transactions in OceanBase Database can reach more than 80%. This is because, in OceanBase Database, data sharding is performed at the table or partition granularity. Therefore, if we update a non-partitioned table, or a single partition of a partitioned table, OceanBase Database can implement single-server transactions with low latency. Moreover, transactions involving multiple tables in the same server are also executed in single-server mode.</p>
<p>Second, OceanBase Database allows users to use table groups to turn cross-server join operations into single-server transactions. The sound partition granularity design can ensure that 80% of transactions are single-server transactions. The proportion can be further improved by optimizing high-frequency cross-server join operations with table groups. The performance of the database will be superb if more than 90% of all business transactions are single-server transactions.</p>
<p>Third, OceanBase Database distinguishes query priorities. Small queries are given the top priority. Large queries occupy up to a percentage of the worker threads as defined in the <code>large_query_worker_percentage</code> parameter. When there are no small queries, large queries can take 100% of the worker threads. The overall mechanism is similar to highway traffic rules, with large vehicles allowed to take the rightmost lane only, allowing other vehicles to overtake them in the passing lanes. This mechanism can prevent slow SQL queries and large queries from congesting the system or causing it to crash.</p>
<p>These architecture designs and practical optimization ensure the high performance of OceanBase Database. So how does OceanBase Database offer higher performance at lower costs?</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-does-oceanbase-database-cost-less-while-offering-superior-performance">Why does OceanBase Database cost less while offering superior performance?<a href="https://oceanbase.github.io/zh-Hans/blog/dmall#why-does-oceanbase-database-cost-less-while-offering-superior-performance" class="hash-link" aria-label="Why does OceanBase Database cost less while offering superior performance?的直接链接" title="Why does OceanBase Database cost less while offering superior performance?的直接链接">​</a></h3>
<p>OceanBase Database uses the log-structured merge-tree (LSM-tree)-based storage engine, which supports both compression through encoding and general compression to offer a high compression ratio. As you can see in the following figure, our test data shows that it uses 75% less cluster storage space than MySQL Database.</p>
<p><img decoding="async" loading="lazy" src="https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/c8607d66d96e86236d749dbaa9e3769f9a0e32b7" alt="" class="img_ev3q"><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993956548.png" alt="1693993956" class="img_ev3q"></p>
<p>As the business of Dmall grows, our data is increasing rapidly, and the number of nodes is increasing exponentially. The costs of a MySQL system will soon exceed those of OceanBase Database. As shown in the figure below, the compression ratio of 6:1 is verified by the test conducted in our production environment. Our business data will continue to grow in the future. As OceanBase Database supports unlimited new nodes, its storage costs will grow much slower than those of MySQL Database.</p>
<p><img decoding="async" loading="lazy" src="https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/41f4c6bb40ce230e482c27e6385fb746232085ef" alt="" class="img_ev3q"><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993976054.png" alt="1693993976" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="multitenancy-and-resource-isolation-capabilities-of-oceanbase-database">Multitenancy and resource isolation capabilities of OceanBase Database<a href="https://oceanbase.github.io/zh-Hans/blog/dmall#multitenancy-and-resource-isolation-capabilities-of-oceanbase-database" class="hash-link" aria-label="Multitenancy and resource isolation capabilities of OceanBase Database的直接链接" title="Multitenancy and resource isolation capabilities of OceanBase Database的直接链接">​</a></h3>
<p>The multitenancy feature of OceanBase Database perfectly suits the SaaS scenario because it provides resource isolation among tenants and fast elastic scaling of tenants.</p>
<p>Resource isolation among tenants: OceanBase Database tenants are physically isolated from each other in terms of CPU, memory, and I/O resources. This ensures zero resource preemption among different businesses and ensures that issues in one business will not affect other tenants.</p>
<p>Fast elastic scaling for tenants: Assume that a tenant has three zones, with each zone having two nodes. The tenant has a total of six nodes, and each node has one resource unit. To scale up the tenant, you only need to execute one SQL statement. For example, you can add Zone 4 and Zone 5 to scale the tenant up from 6 resource units to 10, implementing horizontal scaling with ease. Vertical scaling is simple to carry out as well. For example, if you start out with 2 CPU cores and 8 GB of memory and do not want to add nodes, you can scale up the tenant to 6 CPU cores and 12 GB of memory without adding any nodes. The whole process is dynamic and lossless without affecting or interrupting your business. Vertical scaling requires the DBA to run only a single SQL statement, greatly reducing the workload. Therefore, the multitenancy feature perfectly meets our need for a new system for SaaS businesses that is cost-saving and easy to expand.</p>
<p><img decoding="async" loading="lazy" src="https://weboffice-zjk.docs.dingtalk.com/api/v3/office/copy/QTh3bUEvNjFxcEZDNzlUK0RreXZCamNIS3VIMDBkd0NDS0d1bHd3WXZCMUJRMUpXUXFFb01vYVFMd1pVTHZtN1RwU0RzaVlyaGp3Rzh3TDg0N2hYaUNnNUZWNDVSNzVubU14TnhGR1IvV0NoRm82MU1oM1F6N0VGTE5EZHprR014TWlKeHFzaWNRS2ZxN01BYm9NMk8rNWZGVmlaZElHNjc3UWh0QWFrdDk0Si9iRWVHR2FVYkhMUXk3QUNVdHBuQ2JMdGxGS3lIeDNucFdwWEdpUT0=/attach/object/c75160228d41e619d04531f05b1b71937a3a64a3" alt="" class="img_ev3q"><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-09/1693993992089.png" alt="1693993992" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="summary">Summary<a href="https://oceanbase.github.io/zh-Hans/blog/dmall#summary" class="hash-link" aria-label="Summary的直接链接" title="Summary的直接链接">​</a></h2>
<p>As a SaaS service provider, Dmall faces many pain points, such as massive databases, fast data growth, high resource costs, and complex O&amp;M. Distributed databases provide excellent support for our business growth, improve development efficiency, and alleviate the DBA workload. Distributed databases are the path of advancement for database technology. We verified the advantages of OceanBase Database in scalability, performance, and cost effectiveness by running our own tests. Based on our current business development needs and the multitenancy capability that suits our retail SaaS scenario, we are sure that we will continue to expand our cooperation with OceanBase in the future.</p>]]></content:encoded>
            <category>User Case</category>
        </item>
        <item>
            <title><![CDATA[Sichuan Hwadee —— The practice of lightweight data warehouse construction of health big data based on OceanBase]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/sichuan-hwadee</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/sichuan-hwadee</guid>
            <pubDate>Tue, 04 Jun 2024 09:30:23 GMT</pubDate>
            <description><![CDATA[Introduction: This article introduces Sichuan Hwadee's practice of migrating its data computing platform from Hadoop to OceanBase Database. This case demonstrates the advantages of OceanBase Database in terms of performance and storage costs. With OceanBase Database, hardware costs are reduced by 60% and the O&M work is significantly cut down, relieving maintenance personnel from responsibility for the many Hadoop components. OceanBase Database enables Hwadee to meet the needs of hybrid transaction/analytical processing (HTAP) scenarios with just one system, simplifying O&M for the company.]]></description>
            <content:encoded><![CDATA[<blockquote>
<p><strong>Introduction:</strong> This article introduces Sichuan Hwadee's practice of migrating its data computing platform from Hadoop to OceanBase Database. This case demonstrates the advantages of OceanBase Database in terms of performance and storage costs. With OceanBase Database, hardware costs are reduced by 60% and the O&amp;M work is significantly cut down, relieving maintenance personnel from responsibility for the many Hadoop components. OceanBase Database enables Hwadee to meet the needs of hybrid transaction/analytical processing (HTAP) scenarios with just one system, simplifying O&amp;M for the company.</p>
</blockquote>
<blockquote>
<p><strong>About the author:</strong> Xiang Ping, the Technical Director of the Smart Healthcare and Elderly Care R&amp;D Department of Sichuan Hwadee Information Technology Co., Ltd., is responsible for big data and AI architecture design and team management in the smart healthcare and elderly care sector.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="i-mining-the-value-of-medical-data-with-a-data-computing-platform">I. Mining the value of medical data with a data computing platform<a href="https://oceanbase.github.io/zh-Hans/blog/sichuan-hwadee#i-mining-the-value-of-medical-data-with-a-data-computing-platform" class="hash-link" aria-label="I. Mining the value of medical data with a data computing platform的直接链接" title="I. Mining the value of medical data with a data computing platform的直接链接">​</a></h2>
<p>As the Chinese population grows older, care for the elderly is an increasingly important topic for our society. Providing healthcare for the elderly is a demanding job calling for wisdom, resources, and effort from society as a whole.</p>
<p>We, Sichuan Hwadee Information Technology Co., Ltd. (hereinafter referred to as "Hwadee"), have created a big data public service platform for smart medical care called Qijiale by integrating and innovating next-generation information technologies such as big data, cloud computing, Internet of Things, AI, and mobile Internet. We cooperate with the local governments and competent government departments with jurisdiction over project demo sites to explore and create a new smart healthcare model that joins efforts of institutions, communities, and enterprises. Through these efforts, we hope to provide professional, efficient, convenient, and safe healthcare services for the elderly by drawing on the collective efforts of residential communities, relatives of the elderly, elderly care institutions, medical institutions, medical schools, local governments, science and technology enterprises, life service institutions, and other groups.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622574673.png" alt="1697622574" class="img_ev3q"></p>
<p>The Qijiiale platform is a resource integration platform that brings together medical and nursing services. It provides a work, service, and publicity platform for health information and warnings, assistance in chronic and geriatric disease diagnosis, integrated medical and nursing services, health knowledge learning, and other features. It establishes comprehensive and professional public service networks for healthcare and elderly care at the provincial, municipal, county (district), township (street), and village (community) levels and creates a multi-level intelligent healthcare service system that covers health data collection, big data analysis and early warning, intervention services, and performance evaluation. Qijiiale establishes a multi-scene healthcare service model that focuses on the elderly by combining efforts of families, communities, medical care institutions, hospitals, and governments. By providing intelligent modern community healthcare services in real time in a dynamic and continuous manner based on real names, the platform links healthcare information with healthcare resources by connecting people, data, and devices.</p>
<p>We have created a data resource pool based on the medical data, elderly care data, and industrial data collected by hospitals and governments, and uses a big data system to store and compute resources. We have also established a data computing platform with powerful processing capabilities and high scalability. The data computing platform can store, cleanse, process, model, and analyze massive data, making full use of every piece of data in the resource pool to aggregate data according to a wide range of metrics. The platform provides ample reference data for statistical decision analysis, algorithm analysis services, big data prediction, and other applications, helping the company mine more value from its medical data resources.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ii-technology-selection-to-address-pain-points-of-hadoop-applications">II. Technology selection to address pain points of Hadoop applications<a href="https://oceanbase.github.io/zh-Hans/blog/sichuan-hwadee#ii-technology-selection-to-address-pain-points-of-hadoop-applications" class="hash-link" aria-label="II. Technology selection to address pain points of Hadoop applications的直接链接" title="II. Technology selection to address pain points of Hadoop applications的直接链接">​</a></h2>
<p>Currently, we have accumulated a total of about 20 TB of data. The following figure shows our early-stage data computing platform that we built based on the Hadoop ecosystem.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622688331.png" alt="1697622688" class="img_ev3q"></p>
<p>We encountered many problems in using and operating this data computing platform, such as excessive components, complex builds, and high O&amp;M costs. The most critical problem was that this complex environment was difficult to promptly troubleshoot when a fault occurred.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622700907.png" alt="1697622700" class="img_ev3q"></p>
<p>To solve these pain points, we looked into distributed databases. By reading the OceanBase official documentation along with blogs and Q&amp;A in the open source community, we learned that OceanBase Database supports thousands of OBServer nodes in a single cluster. It can support nearly a petabyte of data in a single cluster and trillions of rows in a single table. Real-time online transactions and real-time data analysis can be supported with the same set of data and the same engine. Multiple replicas of a dataset can be stored in different formats for different workloads. Moreover, OceanBase provides an automatic migration tool, OceanBase Migration Service (OMS), which supports migration assessment and reverse synchronization to ensure data migration security.</p>
<p>We found that the open source ecosystem products of OceanBase could meet our data scale and data computing platform needs, so we conducted preliminary tests. We immediately took note of the HTAP capabilities of OceanBase Database, and also the following five capabilities, which are critical to us.</p>
<ul>
<li>
<p>Easy O&amp;M</p>
<p>The OBServer nodes are peer-to-peer, and each contains a storage engine and a computing engine. OceanBase Database boasts a small number of components, simple deployment, and easy O&amp;M. With OceanBase Database, we no longer need to add other components to the database to achieve high availability and automatic failover.</p>
</li>
<li>
<p>High compression ratio</p>
<p>OceanBase Database organizes data based on the Log-structured merge-tree (LSM-tree) architecture, where the full data consist of baseline data plus incremental data. Incremental data is first written to a MemTable in memory, achieving a write performance comparable to that of an in-memory database. The baseline data is static and will only change at a major compaction. Therefore, a more aggressive compression algorithm can be used, reducing the storage space required by at least 60%. After we migrated the data from Oracle Real Application Clusters (RAC) to OceanBase Database, the disk capacity required is only 1/3 the original size, even when we adopt a three-replica storage mode. For more information about the core data compression technologies of OceanBase, you can read <a href="https://open.oceanbase.com/blog/5269500160" target="_blank" rel="noopener noreferrer">Save at least 50% of the storage costs for history databases with core data compression technologies of OceanBase</a>.</p>
</li>
<li>
<p>High compatibility</p>
<p>At present, OceanBase Database Community Edition is almost perfectly compatible with MySQL syntaxes and features. Most statistical analysis tasks can be completed by using SQL statements. OceanBase Database also supports advanced features that we commonly use, such as stored procedures and triggers.</p>
</li>
<li>
<p>High scalability</p>
<p>OceanBase Database provides linear scalability. DBAs can simply execute a single command to add server nodes and achieve linear performance scale-out. After new nodes are added to an OceanBase cluster, the system automatically rebalances the data among nodes. DBAs no longer need to migrate data manually.</p>
</li>
<li>
<p>High availability</p>
<p>OceanBase Database natively supports high availability. It uses Paxos to achieve high availability at the partition level. When a minority of nodes fail, it can still provide services, so your business is not affected.</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="iii-benefits-and-issues-arising-from-migration-from-hadoop-to-oceanbase-database">III. Benefits and issues arising from migration from Hadoop to OceanBase Database<a href="https://oceanbase.github.io/zh-Hans/blog/sichuan-hwadee#iii-benefits-and-issues-arising-from-migration-from-hadoop-to-oceanbase-database" class="hash-link" aria-label="III. Benefits and issues arising from migration from Hadoop to OceanBase Database的直接链接" title="III. Benefits and issues arising from migration from Hadoop to OceanBase Database的直接链接">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="i-changes-in-architecture">i. Changes in architecture<a href="https://oceanbase.github.io/zh-Hans/blog/sichuan-hwadee#i-changes-in-architecture" class="hash-link" aria-label="i. Changes in architecture的直接链接" title="i. Changes in architecture的直接链接">​</a></h3>
<p>Our original data computing platform was a Hadoop environment that was deployed on 10 servers and used more than 20 different open source components. These components carried out tasks such as data import and export, data cleansing, and analytical processing. We first used the extract-transform-load (ETL) tool to transfer raw data to Hadoop Distributed File System (HDFS), then used Hive commands to load the data, and finally used Spark SQL for data analysis.</p>
<p>Under this architecture, data needed to be transferred back and forth. Moreover, we needed professionals to work intensively on version adaptation and performance tuning for these components. The key problem lay in troubleshooting. Due to the excessive components and long links, we had a hard time quickly finding the faulty component.</p>
<p>At first, we just wanted to use OceanBase Database to integrate and cleanse data: We used a dedicated line to pull data to the front-end machine (Oracle RAC) and then used the ETL tool DataX to pull data from the front-end machine to OceanBase Database. Then, we decrypted, cleansed, and integrated the data in OceanBase Database. Finally, we pulled the cleansed data from OceanBase Database to Hadoop for analytical processing. You can see the full process in the figure below.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622871684.png" alt="1697622871" class="img_ev3q"></p>
<p>Later, from the OceanBase official website, we learned that OceanBase Database supported HTAP capabilities. So we tried analytical processing on the data directly in an OceanBase cluster with three nodes. We were surprised to see that even if the data was not partitioned in OceanBase and parallel execution was not used, we were able to complete analytical processing on 0.5 billion rows of data in less than a minute.</p>
<p>Analytical processing in an OceanBase cluster with three nodes outperformed Hadoop that had 10 servers of the same specification. To our surprise, OceanBase Database supports SQL syntax directly. We do not need to load the data into Hive and then use Spark SQL for analysis, nor do we need to use various open-source components. This simplifies the data computing platform link, as shown in the following figure.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622887380.png" alt="1697622887" class="img_ev3q"></p>
<p>We initially planned to import data into OceanBase Database for data integration, and then import it back to Hadoop. However, we found that the entire downstream Hadoop cluster was useless. Moreover, as a distributed database, OceanBase Database supports horizontal scaling. This led us to abandon Hadoop completely.</p>
<p>We haven't yet done any performance tuning for OceanBase Database. In the future, we will use it to perform analytical processing on 10 billion data rows in more data environments, and then further study its partitioning and parallel execution features.</p>
<p>Also, we have changed our data computing platform to an OceanBase cluster consisting of 4 servers, with one OceanBase Cloud Platform (OCP) server and three OBServer nodes with the following specifications.</p>
<table><thead><tr><th>Server</th><th>Operating system</th><th>Memory</th><th>CPU</th><th>Disk space</th></tr></thead><tbody><tr><td>OCP</td><td>CentOS 7 (64-bit)</td><td>64 GB</td><td>20 vCPUs</td><td>2 TB</td></tr><tr><td>OBServer node 1</td><td>CentOS 7 (64-bit)</td><td>64 GB</td><td>20 vCPUs</td><td>2 TB</td></tr><tr><td>OBServer node 2</td><td>CentOS 7 (64-bit)</td><td>64 GB</td><td>20 vCPUs</td><td>2 TB</td></tr><tr><td>OBServer node 3</td><td>CentOS 7 (64-bit)</td><td>64 GB</td><td>20 vCPUs</td><td>2 TB</td></tr></tbody></table>
<p>After data is imported to OceanBase Database by using the ETL tool, data decryption, cleansing, aggregation, and analytical processing are all completed in OceanBase Database, and some performance improvements are made for analytical processing. The OceanBase cluster consists of only the deployed cluster management tool OCP and OBServer nodes, and the OBServer nodes are completely equivalent to each other, greatly reducing the O&amp;M complexity.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622916987.png" alt="1697622917" class="img_ev3q"></p>
<blockquote>
<p>Terms:</p>
</blockquote>
<blockquote>
<p>1. Operational data store (ODS): The ODS layer stores and directly loads raw data without processing the data.</p>
</blockquote>
<blockquote>
<p>2. Data warehouse (DW): The DW layer stores structured and aggregated clean data to support decision-making and data analysis services for enterprises.</p>
</blockquote>
<blockquote>
<p>3. Data warehouse detail (DWD): The DWD layer cleanses data at the ODS layer by removing empty values, dirty data, and data that does not meet the metadata standards. It is used to store detailed, complete data that is used for cross-departmental and cross-system sharing and enterprise data query.</p>
</blockquote>
<blockquote>
<p>4. Data warehouse summary (DWS): The DWS layer provides business data summary and analysis services, and computes, aggregates, and processes raw data for enterprise decision-making.</p>
</blockquote>
<blockquote>
<p>5. Application data service (ADS): The ADS layer stores custom statistical metrics and report data for data products.</p>
</blockquote>
<blockquote>
<p>6. Data mart (DM): The DM is a collection of data that is focused on a single subject and separated from the data warehouse for a specific application. It provides a clear, targeted, and scalable data structure.</p>
</blockquote>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697622998311.png" alt="1697622998" class="img_ev3q"></p>
<p>Figure: Overall technical architecture</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ii-storage-costs">ii. Storage costs<a href="https://oceanbase.github.io/zh-Hans/blog/sichuan-hwadee#ii-storage-costs" class="hash-link" aria-label="ii. Storage costs的直接链接" title="ii. Storage costs的直接链接">​</a></h3>
<p>We tested the data import and export performance of an Oracle cluster and an OceanBase cluster each deployed on five servers of the same specifications, with a 372 GB data file containing 0.5 billion rows of data. For data import, we compared the storage space required by the imported data:</p>
<ul>
<li>The data file occupies 220 GB of storage space after it is imported to the Oracle cluster.</li>
<li>After the same data file is imported to the OceanBase cluster by using OBLOADER, it occupies only 78 GB of storage space, even when it is stored in three replicas.</li>
</ul>
<p>In short, even when storing data in three replicas, OceanBase Database requires only about 1/3 to 1/4 of the storage space required by Oracle Database.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="iii-ecosystem-tools">iii. Ecosystem tools<a href="https://oceanbase.github.io/zh-Hans/blog/sichuan-hwadee#iii-ecosystem-tools" class="hash-link" aria-label="iii. Ecosystem tools的直接链接" title="iii. Ecosystem tools的直接链接">​</a></h3>
<p>OceanBase has a robust ecosystem that provides more than 400 upstream and downstream ecosystem products and in-house tools such as OCP and OBLOADER &amp; OBDUMPER.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697623053024.png" alt="1697623053" class="img_ev3q"></p>
<p><strong>1. OCP</strong></p>
<p>OCP is an O&amp;M management tool that provides visual performance monitoring.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697623067245.png" alt="1697623067" class="img_ev3q"></p>
<p><strong>2. OBLOADER &amp; OBDUMPER</strong></p>
<p>OBLOADER &amp; OBDUMPER is a data import and export tool for logical backup and restore of data. It takes OBDUMPER only several dozen minutes to generate a backup file for 0.5 billion rows of data, with the backup file sized about 400 GB. OBLOADER &amp; OBDUMPER is easy to get started with, and allows you to import and export data with commands.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697623086269.png" alt="1697623086" class="img_ev3q"></p>
<p><strong>3. Integration and fusion with other open source products</strong></p>
<p>OceanBase Database Community Edition has implemented in-depth integration and fusion with more than 400 upstream and downstream products in the ecosystem, such as Flink, Canal, Otter, and DataX. We find this very convenient. For example, we can use DataX to extract data to an OceanBase cluster. Our cluster runs a total of 168 ETL tasks in real time.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697623102230.png" alt="1697623102" class="img_ev3q"></p>
<p><strong>4. Some usage issues encountered</strong></p>
<ul>
<li>When we manage stored procedures in OceanBase Developer Center (ODC), the <code>REPLACE PROCEDURE</code> command is provided but unavailable. To replace a stored procedure, we have to drop it and then create a new one. However, we often need to modify only a small part of a large stored procedure, so we hope that ODC can support <code>REPLACE PROCEDURE</code> to make it easier to modify stored procedures.</li>
<li>The earlier OceanBase Database Community Edition V3.x does not support DBlinks and does not allow access to and manipulation of tables, views, and data in another database. DBlinks are supported in OceanBase Database Community Edition V4.x.</li>
<li>The deployment was cumbersome because OceanBase Database Community Edition did not provide the one-click installation package OceanBase All in One at that time.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="iv-summary">IV. Summary<a href="https://oceanbase.github.io/zh-Hans/blog/sichuan-hwadee#iv-summary" class="hash-link" aria-label="IV. Summary的直接链接" title="IV. Summary的直接链接">​</a></h2>
<p>We have been paying close attention to OceanBase since 2021 and have used OceanBase Database Community Edition since its initial open source version V3.x. We have verified the feasibility of using OceanBase Database to directly analyze 20 TB of data in real business scenarios. A task that originally required 10 servers can now be completed with only 4 servers, reducing our hardware costs by 60% and freeing engineers from the demanding O&amp;M work caused by the many components of Hadoop.</p>
<p>OceanBase Database supports online analytical processing (OLAP) and online transaction processing (OLTP) workloads with one set of engines. It not only meets our analytical processing performance requirements, but also simplifies O&amp;M and greatly reduces the costs. The advantages of OceanBase Database in performance and storage costs are proven in our business environment.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-10/1697623167264.png" alt="1697623167" class="img_ev3q"></p>
<p>In the future, we will strengthen our cooperation with OceanBase and try to use the wide array of peripheral tools in the OceanBase open source ecosystem to create enterprise-level products.</p>]]></content:encoded>
            <category>User Case</category>
        </item>
        <item>
            <title><![CDATA[KUAYUE EXPRESS —— In the real-time warehouse scenario, utilizing OceanBase to make up for the shortcomings of MySQL and StarRocks]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/kuayue-express</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/kuayue-express</guid>
            <pubDate>Tue, 04 Jun 2024 09:30:23 GMT</pubDate>
            <description><![CDATA[Introduction: This article is based on the speech made by Zhang Jie, Big Data Architect of KUAYUE EXPRESS (hereinafter referred to as KYE), at the DTCC conference. The speech introduced the pain points faced by KYE in data analysis and the company's ideas and practices in the development of its query engine solution.]]></description>
            <content:encoded><![CDATA[<p>Introduction: This article is based on the speech made by Zhang Jie, Big Data Architect of KUAYUE EXPRESS (hereinafter referred to as KYE), at the DTCC conference. The speech introduced the pain points faced by KYE in data analysis and the company's ideas and practices in the development of its query engine solution.</p>
<p>Hello everyone. I'm Zhang Jie, a Big Data Architect at KYE. Today, I want to talk about how we selected our query engine for HTAP scenarios. My talk will be divided into four parts:</p>
<ol>
<li>Business background and the pain points encountered in daily data analysis in the logistics industry</li>
<li>Our considerations and product testing for query engine selection</li>
<li>Our benefits from using OceanBase Database</li>
<li>Our experience in using OceanBase Database Community Edition V4.0</li>
</ol>
<p><strong>Business background of daily data analysis in the logistics industry</strong></p>
<p>In the Internet industry, data may be directly oriented to consumers and end users. But at KYE, we mainly serve our internal employees. Every day, more than 100 BI developers intensively use our big data platform for development. We have built more than 10,000 data service interfaces, which are called by various production systems through gateways. We handle tens of millions of calls per day, and the various data services we provide support the daily work of more than 50,000 employees throughout the entire group. To ensure a good user experience, we have high requirements for interface latency. Basically, the 99th percentile latency is required to be less than 1 second.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/1a31d7a2-e247-43fb-bce9-46ca43585e11/image/2022-12-28/1258e2ef-7ee1-456c-bc95-5284d117052b.png" alt="" class="img_ev3q"></p>
<p>In addition to online analytical processing (OLAP) scenarios, our data services also provide key data in core business processes, such as performance and salary query, waybill cost allocation, and real-time waybill tracking. The actual revenue of each waybill is not fixed. Therefore, a large number of users, such as drivers and operators, query their daily revenue by using our interfaces, including the costs of each waybill and the real-time tracking information of each waybill. All of this data comes from the big data platform.</p>
<p>In the logistics industry, core analysis scenarios all concern waybills. Waybill analysis requires data from dozens of upstream enterprise resource planning (ERP) systems, including transportation management, customer management, and performance management systems. This data is converged on the big data platform. The big data platform collects, merges, and calculates all upstream data related to waybills, qualifies the data for waybill domains of our data warehouse through complex analysis and processing, and then provides the data to the users of various platforms and services.</p>
<p>With the rapid development of the company business, database application scenarios are becoming more and more complex. In the early stage, users queried results based on fixed parameters, for example, by day, week, or month, or with fixed conditions. Back then, we could preprocess data and place the data aggregation results in MySQL Database so that users could directly find the results they were looking for. Today, user needs are becoming increasingly complex. They want to query data by any field and any time range. In this case, MySQL Database no longer meets our needs.</p>
<p><strong>Our business scenario has changed in the following aspects:</strong></p>
<ol>
<li>
<p>More table joins</p>
<p>In the past, we could prepare a single wide table in advance to serve users. As requirements change more frequently, a query interface may involve dozens of table joins.</p>
</li>
<li>
<p>Higher requirement for real-time performance</p>
<p>In the past, users could accept our offline and batch data updates. Now they require real-time data updates, including the real-time status of each waybill. This imposes higher requirements on real-time data processing for downstream databases.</p>
</li>
<li>
<p>Higher requirements for latency</p>
<p>Previously, users could wait 3 to 4 seconds for data results. Now users require a response to data requests within one second.</p>
</li>
</ol>
<p>To address these new business requirements, we need to select a more suitable database solution. To improve user experience and real-time performance, the new database must provide <strong>exceptional query performance</strong> and <strong>support real-time writes, updates, and deletion</strong>. The database must also be easy to use, so <strong>standard SQL support and rich functions</strong> are essential. It would be even better if it was <strong>highly compatible with MySQL</strong>. In addition, the database must allow <strong>data to flow in and out</strong>, and its data integration feature must support data sources in our usage scenarios. <strong>Database stability and maintainability</strong> are also important factors that concern us.</p>
<p><strong>Database tests: performance and feature comparison of five databases</strong></p>
<p>The following performance testing and feature testing results explain why we chose OceanBase Database from among other distributed databases.</p>
<p><strong>Performance testing</strong></p>
<p>A unified test comparison standard is essential for database evaluation and selection. Common benchmark tests in the industry include TPC-H, TPC-DS, and SSB. Every database product provides these benchmark test results in the official documents. We used these test results for reference in database selection. However, we cannot solely rely on these test results because they do not account for our business characteristics and the test sets are usually specially optimized for the tests. Therefore, we built a set of benchmark standards based on our own data analysis requirements. This set of benchmark standards adopts a unified test model and environment, and defines unified tests based on the SSD disks of Alibaba Cloud.</p>
<p>We prepared a standard data set of more than a dozen tables related to waybills, and compiled a set of standard SQL statements in accordance with our daily application scenarios based on actual waybill analysis cases. We also developed a feature test set based on our actual needs. Then we carried out benchmark tests with these test data sets on different databases available on the market.</p>
<p>After preliminary screening, we selected five popular commercial query engines for testing and comparison. They were TiDB, OceanBase, StarRocks, Doris, and Trino. The following figure shows the performance test results.</p>
<table><thead><tr><th>SQL No</th><th>SQL description</th></tr></thead><tbody><tr><td>SQL 21</td><td>SQL heavily relying on CPU for computation</td></tr><tr><td>SQL 19</td><td>Multi-table join aggregation with high cardinality in Snowflake schema</td></tr><tr><td>SQL 18</td><td>Multi-table join aggregation with high cardinality in Star schema</td></tr><tr><td>SQL 17</td><td>Multi-table join point query in Snowflake schema</td></tr><tr><td>SQL 16</td><td>Multi-table join point query in Star schema</td></tr><tr><td>SQL 15</td><td>Large table join aggregation with high cardinality</td></tr><tr><td>SQL 14</td><td>Large table join aggregation with low cardinality</td></tr><tr><td>SQL 13</td><td>Large table and small table join aggregation with high cardinality</td></tr><tr><td>SQL 12</td><td>Large table and small table join aggregation with low cardinality</td></tr><tr><td>SQL 11</td><td>Large table and small table join point query with index miss</td></tr><tr><td>SQL 10</td><td>Large table and small table join point query with index hit</td></tr><tr><td>SQL 9</td><td>Range scan on a non-indexed datetime column</td></tr><tr><td>SQL 8</td><td>Aggregation on high cardinality column</td></tr><tr><td>SQL 7</td><td>Aggregation on low cardinality column</td></tr><tr><td>SQL 6</td><td>Window functions</td></tr><tr><td>SQL 5</td><td>ORDER BY with LIMIT</td></tr><tr><td>SQL 4</td><td>ORDER BY</td></tr><tr><td>SQL 3</td><td>Point query with filter condition as IN clause containing a thousand values</td></tr><tr><td>SQL 2</td><td>Point query with index miss</td></tr><tr><td>SQL 1</td><td>Point query with index hit</td></tr></tbody></table>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/08b1c279-4af3-4f55-aa09-9e4e98c56bb0/image/2022-12-28/6d1cb039-34c7-48f4-a75d-669e358e0eee.png" alt="" class="img_ev3q"></p>
<p>We can see that, in pure OLAP scenarios, StarRocks has the best performance. To our surprise, OceanBase Database came in second. Before we tried OceanBase Database, we tested TiDB, whose performance was far inferior to StarRocks. Therefore, we didn't expect much from the performance of HTAP databases. However, after the testing on OceanBase Database was completed, we unexpectedly found that its performance in OLAP scenarios was also good.</p>
<p><strong>Feature testing</strong></p>
<p>In addition to the performance tests, we also tested and verified the general features of the databases. The test results show that HTAP databases have richer features, and one HTAP database can be basically considered a collection of several distributed MySQL databases. Despite their slight differences, these HTAP databases all support updates, deletion, indexed column customization, and consistency guarantees.</p>
<ul>
<li><strong>Big data ecosystem integration</strong>. Our business scenarios primarily involve waybill analysis. Therefore, we must consider the integration of databases with big data platforms. In this regard, StarRocks and Doris are the clear winners.</li>
<li><strong>Maintainability.</strong> We are primarily concerned with online expansion, online upgrade, automatic balancing, resource isolation, and management tools. HTAP databases perform better in these areas. Among them, OceanBase and TiDB were the best. For example, OceanBase provides OceanBase Cloud Platform (OCP), and TiDB provides TiUP. Both can help us easily deploy, upgrade, monitor, and maintain the corresponding databases.</li>
</ul>
<p>After comprehensive comparison, we concluded that MySQL is a transaction processing (TP) database that is widely used in our business, and its strengths are stability, transaction processing, and concurrency capabilities. StarRocks is an analytical processing (AP) database that is widely used in our analysis scenarios. StarRocks boasts a distributed architecture, good AP analysis performance, and a high compression ratio due to columnar storage. StarRocks provides a good solution to our requirements in pure analysis business scenarios. However, in addition to TP scenarios and AP scenarios, we have many businesses that require databases with both TP and AP capabilities, such as real-time waybill analysis. HTAP databases, such as OceanBase Database and TiDB, can support these scenarios.</p>
<p>We carried out <strong>a further round of testing and comparison between OceanBase Database and TiDB</strong> and found that:</p>
<ul>
<li>The performance of OceanBase Database is 4x to 5x higher than that of TiDB in all scenarios.</li>
<li>OceanBase Database has a simpler architecture and integrates storage and computing. It needs only observer and obproxy processes. However, TiDB requires multiple services such as PD, TiDB, TiKV, and TiFlash, resulting in higher complexity during subsequent maintenance and troubleshooting.</li>
<li>OceanBase Database is more advantageous in data storage space because it supports hybrid row and column storage and supports AP and TP scenarios with the same set of data. In contrast, TiDB needs another set of data stored in the columnar format generated with TiFlash to support AP scenarios, so the data storage space is doubled.</li>
</ul>
<p>In view of this, we chose OceanBase Database to help us solve business pain points in real-time analysis scenarios.</p>
<p><strong>Our benefits from using OceanBase Database</strong></p>
<p>After we decided to use OceanBase Database, we carried out the following tasks.</p>
<p><strong>First, we verified the data integration links</strong></p>
<p><strong>Verification of offline data synchronization.</strong> We use the Sqoop tool to synchronize data from Hive to OceanBase Database in batches. When we write data to a 220-field wide table in a 3-node OceanBase cluster at a degree of parallelism (DOP) of 50, the write performance is 2 million rows per minute, which meets our requirements for offline batch data synchronization.</p>
<p><strong>Verification of real-time data writing.</strong> Based on our real-time computing platform, we use Flink JDBC Connector to write upstream data from Kafka and other sources into OceanBase Database in real time. The real-time write performance is 0.15 million rows per minute for a 220-field wide table with 10 partitions. Limited by the implementation of the JDBC connector, the real-time performance is far inferior to offline performance, but can still meet our requirements for real-time synchronization of incremental data. The OceanBase community will launch Flink OceanBase Connector in January 2023, which has been comprehensively optimized in terms of write and concurrency performance. We will verify it as soon as possible. We believe that its performance will be better than that of the native Flink JDBC Connector.</p>
<p>OceanBase also provides OceanBase Migration Service (OMS), which can synchronize incremental data to Kafka in the Canal format in real time, so that downstream systems can consume the incremental data. The AP database that we are currently using does not support this feature. It can be of great help to us in secondary real-time data computing as well as data backup and disaster recovery.</p>
<p><strong>Second, we explored upgrading the waybill analysis architecture</strong></p>
<p>After data link verification, we run a trial to upgrade the waybill analysis architecture. The following figure shows our original waybill analysis architecture, which was basically an offline processing architecture. The basic upstream waybill data was synchronized from binlogs to Kafka through Canal. The data of multiple topics was written into different Hive tables through our in-house platform. The tables were merged and processed on a two-hourly or daily basis to generate a final waybill wide table for business teams.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/f981915a-ab34-4739-b160-01db66a38944/image/2022-12-28/6a627b5a-8bd0-40a3-9231-1dd51abedfa0.png" alt="" class="img_ev3q"></p>
<p>This architecture had two main problems: First, the timeliness of data was poor. Data was processed offline, so it was already at least two hours old. Second, the analysis performance was poor. We carried out waybill analysis based on Presto. The analysis generally took about 1 to 10 seconds, and even longer for more complex analysis. This was hard on our business teams, but we were limited by the technology at that time.</p>
<p>Currently, we use an upgraded architecture based on this earlier version, with HBase carrying out real-time aggregation of waybill wide tables and the CDC capability of HBase synchronizing the merged data to StarRocks in real time. This is a significant improvement. Offline data is turned into real-time data, and the performance of real-time waybill analysis based on StarRocks has also been greatly improved.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/3ec17c7c-652f-4964-8a6f-058a22537e64/image/2022-12-28/1d94b7d2-204d-4f90-8874-426b107ca4f3.png" alt="" class="img_ev3q"></p>
<p>However, we encounter several problems after the launch of the upgraded architecture:</p>
<ul>
<li>The solution is too complex and hard to reuse.</li>
<li>The data link is long, resulting in difficulties in troubleshooting data synchronization problems.</li>
<li>The maintenance costs are high.</li>
</ul>
<p>Therefore, we tried to optimize the current architecture with the AP capabilities of OceanBase Database. We use OceanBase Database to help us further solve problems in real-time waybill analysis. During the test, we used Flink JDBC Connector to directly write the upstream waybill data of multiple tables into OceanBase Database, and merged the multi-field data of the waybill wide tables in OceanBase Database in real time.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/52ff144b-0eae-4c4a-9ab3-caad91f4a341/image/2022-12-28/15bd01c5-0e8b-46b1-a4cf-8a85c15843e3.png" alt="" class="img_ev3q"></p>
<p>Based on the excellent AP capabilities of OceanBase Database, we connected the waybill wide tables to the external analysis system. The analysis performance was unchanged. The analysis of a 220-field wide table with 20 million rows by using a complex SQL statement took about 4 seconds, and could be further accelerated. Upon testing, we found that we no longer need a complete set of HBase systems, so we were able to <strong>greatly simply the architecture for real-time waybill analysis, reducing the number of components by a third and cutting costs by half.</strong></p>
<ul>
<li>The number of components required for data merging is significantly reduced. This shortens the data link by half and increases the timeliness of data processing. For example, a query that took 5 seconds before now takes only 2 seconds in OceanBase Database.</li>
<li>The data synchronization link is also shortened, facilitating maintenance and troubleshooting.</li>
<li>Server costs are also reduced because a complete set of HBase clusters is no longer required.</li>
<li>The reproducibility of the whole solution has increased, and we can quickly reuse it in other similar analysis scenarios.</li>
</ul>
<p><strong>Our experience in using OceanBase Database Community Edition V4.0</strong></p>
<p>After OceanBase Database Community Edition V4.0 was launched on November 3, 2022, KYE tried out its many new features. This version implements a new distributed cost model and distributed query optimization framework, applies a comprehensive set of parallel pushdown techniques, and enables the development of adaptive techniques. The new version outperforms OceanBase Database Community Edition V3.x significantly, as shown in the following TPC-DS 100 GB benchmark test results.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/92a3f581-1bf7-4e93-92fa-9804bf7ea772/image/2022-12-28/e47eb7bd-9ac2-4d83-83a2-7cccc2da9e1f.png" alt="" class="img_ev3q"></p>
<p>KYE has also tested OceanBase Database Community Edition V3.2. The following figure shows the test results and comparison of these two versions.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/10f21275-ffec-45e0-bacb-26200484c28a/image/2022-12-28/a4888a1b-47d6-4a13-8c10-2cfb627d69b0.png" alt="" class="img_ev3q"></p>
<p>We can see that almost all SQL sets see performance improvements in the new version, and some are improved by 4x to 5x. In some high-base aggregation scenarios, the performance improvement is 3x to 4x. The overall AP performance is improved significantly.</p>
<p>Compared with OceanBase Database Community Edition V3.2, the new version delivers optimal performance without requiring much parameter optimization. You only need to take the following simple steps:</p>
<ul>
<li>Manually perform a major compaction to merge the data and flush it to disk.</li>
<li>Manually collect table statistics one time to increase the accuracy of the execution plan.</li>
<li>Set a reasonable number of partitions and DOP based on the number of CPU cores of the OBServer nodes to give full play to the CPU capacity.</li>
</ul>
<p><strong>Suggestions for OceanBase</strong></p>
<p>When using OceanBase, we also found some issues and reported them to the OceanBase community. The first issue is the compatibility of peripheral tools. Tools such as OBLOADER and OMS are not fully adapted to OceanBase Database Community Edition V4.0. The second issue is the performance in some aggregation scenarios. After analysis, we found that we could solve this issue with hints. According to the OceanBase community, these issues will be resolved in the next version.</p>
<p>Finally, I would like to express our gratitude to the OceanBase community team for their help and support.</p>
<p>That's all for me. Thank you.</p>
<blockquote>
<p>Company Profile: Established in 2007, Kuayue Express Group Co., Ltd. is a modern comprehensive express delivery company specializing in time-sensitive delivery services. It provides delivery on the same day, the next day, the third day, the same day in the same city, the next day in the same city, land transportation, and fresh goods delivery services, as well as 24-hour pickup and delivery and one-to-one exclusive services.</p>
</blockquote>
<p>Follow us in the <a href="https://open.oceanbase.com/blog" target="_blank" rel="noopener noreferrer">OceanBase community</a>. We aspire to regularly contribute technical information so we can all move forward together.</p>
<p>Search🔍 DingTalk group 33254054 or scan the QR code below to join the OceanBase technical Q&amp;A group. You can find answers to all your technical questions there.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/f4d95b17-3494-4004-8295-09ab4e649b68/image/2022-08-29/00ff7894-c260-446d-939d-f98aa6648760.png" alt="" class="img_ev3q"></p>]]></content:encoded>
            <category>User Case</category>
        </item>
        <item>
            <title><![CDATA[Momo —— Exploration and practice of persistent cache based on OceanBase KV storage]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/momo</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/momo</guid>
            <pubDate>Tue, 04 Jun 2024 09:30:23 GMT</pubDate>
            <description><![CDATA[About the author: Ji Haodong, head of the database division at Momo, part of the Hello Group. He is now responsible for the database teams of Momo and Tantan and the database storage and operation throughout the Hello Group. He has a wealth of professional experience and practical know-how in the fields of large-scale data source stability construction, team building, cost optimization, and IDC migration.]]></description>
            <content:encoded><![CDATA[<p>About the author: Ji Haodong, head of the database division at Momo, part of the Hello Group. He is now responsible for the database teams of Momo and Tantan and the database storage and operation throughout the Hello Group. He has a wealth of professional experience and practical know-how in the fields of large-scale data source stability construction, team building, cost optimization, and IDC migration.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="business-scenario-characteristics-of-social-media-applications">Business scenario characteristics of social media applications<a href="https://oceanbase.github.io/zh-Hans/blog/momo#business-scenario-characteristics-of-social-media-applications" class="hash-link" aria-label="Business scenario characteristics of social media applications的直接链接" title="Business scenario characteristics of social media applications的直接链接">​</a></h2>
<p>Hello Group launched Momo in August 2011. This open-style mobile video social media application is based on geographical location services, unique among the social media platforms in China. As the mainstream social applications that allow strangers to interact, Momo and Tantan involve a variety of core business modules, including livestreaming, nearby events, instant messaging (IM), and value-added services. Each business scenario has its own unique characteristics and challenges.</p>
<p>In this article, Ji Haodong, the head of the database team of Momo, will focus on their experience in choosing a KV system architecture for Momo, giving an in-depth analysis of the decision-making process. He will further share the trials that the Momo team conducted in choosing and using OBKV of OceanBase and discuss their practical experience.</p>
<p>Livestreaming holds a prominent place among these business modules. The main characteristic of this module is the possibility of sudden traffic spikes. Due to the requirements for low latency and high concurrency, this module places high demands on the real-time processing capabilities of the database system. The platform needs to ensure that data is processed and distributed in a timely and accurate manner when a large number of users are simultaneously viewing content and interacting online.</p>
<p>The nearby events module involves complex data, including geographic location information, activity trajectories, and social relationships of users. This kind of data accumulates quickly and forms large-scale data sets over time. Data can be classified into hot data and cold data. For example, some data may become hot at a certain time, for example, when a user's post creates a lively discussion. This requires the system to effectively manage and quickly respond to hot data access needs.</p>
<p>The core features of IM business scenarios are highly concurrent communication requiring low latency. The delivery time of information must be accurate. Therefore, the requirements for real-time performance are extremely high. To ensure a sound user experience, the application needs to ensure that messages are delivered to users instantly and reliably.</p>
<p>Value-added services mainly focus on data consistency and real-time performance. When processing operations such as purchasing and giving virtual gifts or exercising member privileges, the system needs to ensure data accuracy and update user account statuses in a timely manner. At the same time, real-time performance of data is also essential for high-quality value-added services, such as real-time calculation of bonus points, grades, and benefits.</p>
<p>When operating these businesses, both Momo and Tantan need a powerful data processing and management system to deal with various characteristics and challenges in order to deliver an efficient, stable, and personalized social media experience to users. How should we choose an appropriate KV system for these business scenarios?</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="kv-storage-architecture-in-different-business-stages">KV storage architecture in different business stages<a href="https://oceanbase.github.io/zh-Hans/blog/momo#kv-storage-architecture-in-different-business-stages" class="hash-link" aria-label="KV storage architecture in different business stages的直接链接" title="KV storage architecture in different business stages的直接链接">​</a></h2>
<p>Companies usually have different requirements for storage systems in different development stages.</p>
<p>In the initial stage, the main goal of a company is to start business operations. In the startup stage, the company often needs to carry out rapid iteration on a newly developed app. As a result, they do not have high requirements for the storage system. The storage system is only expected to meet the basic technical needs of the business and then evolve gradually. In this stage, the common choices include the Redis master-slave architecture, Redis Cluster, and other native architectures.</p>
<p>The advantage of the Redis master-slave cluster architecture is that you can quickly build master-slave clusters or sharded clusters and carry out many designs directly on the client. However, this simple operation mode may lead to a high degree of coupling between the design and the client service code, causing difficulties in elastic scaling in the future.</p>
<p>In contrast, the Redis Cluster architecture supports dynamic scaling and high availability. However, with Redis Cluster, the business relies on clients to perceive node changes. If the client fails to handle node changes correctly, service interruption or business performance degradation may occur. Therefore, for error-sensitive business, Redis Cluster may introduce additional complexity. Although Redis Cluster has the advantages of decentralization, fewer components, smart clients, and support for horizontal scaling, they also have some downsides, such as unfriendly batch processing and lack of an effective flow control mechanism.</p>
<p>In the second stage, as the company grows and gains more users, the architecture must support rapid scaling. Basic Redis sharding architectures, such as Codis and Twemproxy, are popular choices for companies in this stage. Among them, Codis provides server-side sharding, centralized management, automatic failover, horizontal node scaling (with 1024 slots), dynamic capacity scaling, and support for pipelines and batch processing. However, Codis only offers an outdated official version V3.2.9. Any updates require much repair and adaption work, and can consume many resources due to the large number of components involved.</p>
<p>In the third stage of company development, as the business further develops and becomes relatively stable, the company may identify issues introduced in the previous stages, such as excessive memory usage and lack of hot and cold data separation. These issues need to be re-examined and addressed. So in the third stage, optimization is the focus. In this stage, common choices are persistence architectures, including oneStore-Pika, Tendis, and Pika.</p>
<p>Finally, in the fourth stage, the business and technology of the company become more complex and advanced. Simple optimization and adjustments may no longer provide significant improvements, and further optimization may not be possible. At this point, a more stable architecture or solution may be introduced to respond to these challenges. We recommend that companies at this stage adopt a multi-modal architecture, which can accommodate multiple data types and workload types and thus provide greater flexibility and potential for optimization.</p>
<p>In general, companies must choose a storage system that suits their business needs, technology maturity, cost-effectiveness requirements, and future scalability and optimization demands in their development stages. As companies grow and business complexity increases, storage architectures need to evolve and adapt to ensure system stability, efficiency, and sustainability.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="momos-in-house-kv-storage-architecture-onestore">Momo's in-house KV storage architecture oneStore<a href="https://oceanbase.github.io/zh-Hans/blog/momo#momos-in-house-kv-storage-architecture-onestore" class="hash-link" aria-label="Momo's in-house KV storage architecture oneStore的直接链接" title="Momo's in-house KV storage architecture oneStore的直接链接">​</a></h2>
<p>In view of the current business situation of Momo, the most crucial challenge we face is the continuously growing cluster size. When the number of shards in a single cluster exceeded 1000, the data volume exceeded 10 TB, and the QPS exceeded 1 million, the Codis architecture and Redis Cluster architecture no longer met our growing capacity requirements.</p>
<p>In order to break through this bottleneck, we developed a proprietary storage product called oneStore. The following figure shows its architecture. This architecture has undergone a phased optimization and improvement process, aiming to overcome the original limitations to accommodate more shards, larger data volumes, and more intensive query requests. We strive to use the oneStore architecture to achieve seamless business expansion and performance improvement.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061557148.png" alt="1706061557" class="img_ev3q"></p>
<p><strong>In the first phase, this architecture provides a server proxy scheme.</strong> Our proprietary oneStore Watcher sentinel component is used to streamline the architecture. In this way, only one sentinel cluster needs to be deployed to effectively manage a business domain.</p>
<p><strong>In the second phase, the client SDK solution is provided.</strong> Although the server proxy solution performed well, as the company business stabilized, we aimed to reduce costs and increase efficiency throughout the company. The client SDK solution can be used directly to perceive cluster topology changes and directly connect to the backend Redis endpoint. In this way, the server proxy component can be eliminated, reducing our costs. However, we have not completely abandoned the server proxy scheme. This is because our client SDK solution currently only supports Java and C++, and users of other languages such as PHP and Python still need to access the data source through the server proxy. The successful application of these two solutions helped us unify access to Redis at the company level and significantly improved IDC migration efficiency.</p>
<p>As the business has further stabilized, we began to optimize the architecture in terms of cost. We used Pika to replace some Redis clusters with a low request volume, and then improved the persistence capability of the architecture, as shown in the following figure, while reducing storage costs. However, in this stage, Pika was mainly used to store some relatively cold data, and its performance in processing hot data needed to be further improved. We hope to further improve performance in this aspect.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061595578.png" alt="1706061595" class="img_ev3q"></p>
<p>In short, the scenarios to be addressed and optimized include:</p>
<p>1. Reciprocal impact among multiple instances on a single server: We urgently need to eliminate the reciprocal impact among different instances on a single server to ensure the stable operation of and efficient cooperation among these instances. Such impact undermines the overall stability and synergy of the system, and must be addressed by targeted optimizations and adjustments.</p>
<p>2. Data persistence support: We plan to enhance data persistence support to form a comprehensive data persistence solution, to ensure data integrity and reliability not just for cold data, but also for a wider range of data types. It will be an important guarantee for the long-term stability of the system.</p>
<p>We need a simple, reliable, and scalable KV system to solve the above problems.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="reliable-distributed-kv-system--obkv-of-oceanbase">Reliable distributed KV system — OBKV of OceanBase<a href="https://oceanbase.github.io/zh-Hans/blog/momo#reliable-distributed-kv-system--obkv-of-oceanbase" class="hash-link" aria-label="Reliable distributed KV system — OBKV of OceanBase的直接链接" title="Reliable distributed KV system — OBKV of OceanBase的直接链接">​</a></h2>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061629024.png" alt="1706061629" class="img_ev3q"></p>
<p>OBKV is a module that OceanBase provides for accessing the table and HBase models over APIs. We chose OBKV because it offers the following major advantages:</p>
<p><strong>1. Better performance</strong></p>
<p>OBKV is built based on a table model and matches the typical table model of the Redis data structure persistence solution. It has better performance than traditional persistent storage architecture and can build richer data structures.</p>
<p>The following figures show the performance of OBKV in different aspects in a massive data writing scenario (with a TPS value of 17000). The TPS curve is quite steep because tasks write data at different stages, and the response delay is within 2 milliseconds. The response time details of the transactions are as expected.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061648498.png" alt="1706061648" class="img_ev3q"></p>
<p>The following figures show the CPU performance. We can see the CPU utilization remains below 10% in a relatively stable state. The MemStore usage is also within the normal range of below 24% with small fluctuations as expected.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061656861.png" alt="1706061656" class="img_ev3q"></p>
<p>On the whole, OBKV achieves low fluctuations and stable resource usage in a production environment.</p>
<p><strong>2. High stability</strong></p>
<p>OBKV is based on OceanBase Database. The storage engine has been verified in a variety of large-scale TP scenarios and can provide high concurrency and low latency. The multitenancy feature ensures system stability, as shown in the following figure. The black curve represents the TPS of a tenant for which OBKV is implemented, while the blue curve represents the TPS of a regular MySQL tenant of OceanBase Database. After the stress test was initiated around 11:30, the tenant for which OBKV is implemented responded normally, and the MySQL tenant was not affected. At the server layer, the CPU load increased due to the stress test, while the MySQL tenant was not affected.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061668943.png" alt="1706061668" class="img_ev3q"></p>
<p>Therefore, it can be concluded that the multitenancy feature can effectively solve reciprocal impact among different instances on the same server.</p>
<p>The following figures show the performance of the online MySQL tenant in a production environment. The TPS value was 5000, and the overall performance was quite stable. The fluctuations in CPU utilization and memory usage were mild and in line with expectations.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061679951.png" alt="1706061680" class="img_ev3q"></p>
<p>In addition, we can easily use the KV interface to store data in the database and use SQL statements to query data. OBKV further enhances this convenience by supporting secondary indexes and service-side TTL features to help simplify the upper-level service architecture. Nevertheless, OBKV has some limitations. For example, it only provides local transaction processing capabilities. If distributed transaction processing is enabled, it may affect the performance and increase the response latency of the system in a highly concurrent environment. However, in view of the current requirements of Momo's business, we believe that the local transaction processing capability of OBKV fully meets our needs. Therefore, we have built a storage solution that combines OBKV and Redis.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="obkv-redis-cluster-architecture">OBKV-Redis cluster architecture<a href="https://oceanbase.github.io/zh-Hans/blog/momo#obkv-redis-cluster-architecture" class="hash-link" aria-label="OBKV-Redis cluster architecture的直接链接" title="OBKV-Redis cluster architecture的直接链接">​</a></h2>
<p>We worked with the OceanBase open source team to create a project internally named Modis. The overall architecture of the project covers multiple layers, including the access layer, data structure layer, cache layer, storage layer, and management plane, as shown in the following figure. It is worth noting that the cache layer will be used to effectively cope with the challenges of hot reading and large-key issues in our future plans. At the storage layer, we will build standard storage structures based on the principles of standardization and abstraction, to allow flexible access to a variety of storage solutions, including OBKV.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061706135.png" alt="1706061706" class="img_ev3q"></p>
<p>During the test and evaluation process, after we successfully migrated about 158 GB of data from Pika to the OBKV-Redis cluster, the occupied storage space was significantly reduced to 95 GB. The migration cut storage costs by about 40%.</p>
<p>To assess the performance, we built a special test environment with the specifications shown in the figure to simulate various thread concurrency scenarios.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061754200.png" alt="1706061754" class="img_ev3q"></p>
<p>Based on the multitenancy management concept, we do not allocate excessive resources to a single tenant. Instead, we check for performance bottlenecks in each tenant in the test process to calculate the corresponding QPS value of a single core. Currently, the standard specification is 12 CPU cores and 40 GB of memory. To better adapt to future changes in business needs, we may release an architecture with lower specifications, such as 4 CPU cores and 8 GB of memory, or 8 CPU cores and 16 GB of memory, depending on the specific needs of the actual business.</p>
<p>The following figure shows the performance of OBKV-Redis given 128 threads and a QPS value of 70000:</p>
<ul>
<li>P90 response latency: 1.9 ms;</li>
<li>P95 response latency: 2.2 ms;</li>
<li>P99 response latency: 6.3 ms;</li>
</ul>
<p>On average, the single-core read-write ratio is 4:1, and the single-core capacity is close to 6000 QPS.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1706061776775.png" alt="1706061776" class="img_ev3q"></p>
<p>We also compared the O&amp;M differences among OBKV, Pika, and TiKV. Currently, only OBKV provides native multitenancy support, which effectively solves the problem of reciprocal impact among different instances deployed on a single server. Notably, OBKV is a thoughtful and efficient solution for database O&amp;M personnel because it provides GUI-based management tools and supports immediate effectiveness of parameter changes.</p>
<p>In summary, OBKV-Redis substantially improves performance, reduces disk usage, and greatly simplifies O&amp;M management.</p>
<p>This is made possible by the following advantages of the solution:</p>
<ul>
<li><strong>Multi-tenant isolation</strong>, which solves reciprocal impact among multiple instances on a single server.</li>
<li><strong>Lower storage costs</strong> from the encoding framework and common compression algorithms used for the storage of table models.</li>
<li><strong>Higher performance</strong> because request filters are pushed down directly to the storage, without serialization or deserialization, and the Time To Live (TTL) feature is supported for servers.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="vision-for-the-future">Vision for the future<a href="https://oceanbase.github.io/zh-Hans/blog/momo#vision-for-the-future" class="hash-link" aria-label="Vision for the future的直接链接" title="Vision for the future的直接链接">​</a></h2>
<p>At present, OBKV-Redis is integrated with strong support for core data structures, such as strings, hashes, and sets. It now supports 93% of Redis KV database commands, and is expected to provide full compatibility with common management commands and support containerized deployment and level-2 caches by the end of the first quarter of 2024. Moreover, we also plan to implement in-depth integration with the Capture Data Change (CDC) scheme. In the second quarter of 2024, our R&amp;D team will further develop data migration features and introduce data tiering technology.</p>
<p>We have great confidence and high hopes for OBKV-Redis. We hope it will allow us to implement more refined data management throughout the data life cycle. As the company enters the fourth stage of our business development, where business and technological challenges deepen, we see stability and innovation as our key mission. Therefore, we have high anticipation of the multi-model ecosystem, and we believe it will empower enterprises by solving legacy problems and reducing costs.</p>]]></content:encoded>
            <category>User Case</category>
        </item>
        <item>
            <title><![CDATA[Work with developers to create an all-in-one database]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/all-in-one</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/all-in-one</guid>
            <pubDate>Mon, 03 Jun 2024 13:23:07 GMT</pubDate>
            <description><![CDATA[1713958543]]></description>
            <content:encoded><![CDATA[<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958543012.png" alt="1713958543" class="img_ev3q"></p>
<p>Good morning, OceanBase developers! I'm so glad to meet you again here in Shanghai for our second developers conference, after our first acquaintance in OceanBase DevCon 2023 in Wangjing, Beijing.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958831452.png" alt="1713958832" class="img_ev3q"></p>
<p>Let's first have a brief review of the architecture of OceanBase Database. The research and development of OceanBase Database started in 2010. It has been nearly 14 years. Throughout this period, we have made two major upgrades to the technical architecture and a key product upgrade.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958839876.png" alt="1713958840" class="img_ev3q"></p>
<p>The first upgrade to the technical architecture took place in 2016, where the original OceanBase Database V0.5 in the single-write multi-read architecture was upgraded to OceanBase Database V1.0 in a fully distributed architecture. The second architecture upgrade was carried out in OceanBase Database V4.0 in 2022. This version introduced an integrated architecture that supports both standalone and distributed deployment modes. This way, OceanBase Database can be applied not only to large enterprises (SMEs), but also to small and medium-sized enterprises and even start-ups.</p>
<p>In the second half of 2023, we proposed the concept of all-in-one database based on the integrated architecture, enabling OceanBase Database to incorporate capabilities such as OLTP, OLAP, KV, OBKV, and even AI in the future, to cope with a wide variety of workloads.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="i-the-path-to-open-source"><strong>I. The path to open source</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#i-the-path-to-open-source" class="hash-link" aria-label="i-the-path-to-open-source的直接链接" title="i-the-path-to-open-source的直接链接">​</a></h2>
<p>OceanBase Database was officially available as an open source project on the Children's Day — June 1, 2021. Before OceanBase Database was launched open source, the open source community in China already had a prevailing native distributed database. Then, why another one?</p>
<p>Simply because we believed that users and developers would love it, for it has showed high stability, high performance, and high cost effectiveness in more than ten years' service in all core scenarios of Ant Group in Double 11 shopping festivals.</p>
<p>At that time, we were serious on turning OceanBase Database into open source, and real open source. Now I have a deeper understanding of "real open source". Is OceanBase Database really open-source? Statistics speak louder than our words.</p>
<p>OceanBase Database has been available as an open source project for two years. The last two years have seen rapid development. Up to now, the number of clusters deployed with OceanBase Database Community Edition has exceeded 10,000. We have seen phenomenal growth in OceanBase users and clusters after the release of OceanBase Database V4.0.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958890484.png" alt="1713958891" class="img_ev3q"></p>
<p>Today, mainstream Internet companies, including Ctrip, Kuaishou, Zhihu, Vivo, and NetEase, are using OceanBase Database Community Edition in various scenarios. In the open source community, OceanBase is recognized as the best open source distributed database with top technical feature performance.</p>
<p>OceanBase Database is widely recognized and has more than 1000 customers. According to the China Distributed Relational Database Vendors Report 2023 by IDC (Document number:# CHC50734323), OceanBase ranked in the industry leaders and was in a leading position in product capabilities. OceanBase also received an Honorable Mention in the 2023 Gartner Magic Quadrant for Global Cloud Database Management Systems, and has topped Modb China Database Rankings for 14 consecutive months.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958903766.png" alt="1713958904" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="i-oceanbase-database-community-edition-in-distributed-oltp-scenarios"><strong>i. OceanBase Database Community Edition in distributed OLTP scenarios</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#i-oceanbase-database-community-edition-in-distributed-oltp-scenarios" class="hash-link" aria-label="i-oceanbase-database-community-edition-in-distributed-oltp-scenarios的直接链接" title="i-oceanbase-database-community-edition-in-distributed-oltp-scenarios的直接链接">​</a></h3>
<p>OceanBase Database was initially designed for mission-critical core business scenarios such as transaction processing and payment. Therefore, early open source users apply OceanBase Database to core business scenarios.</p>
<p>CR Vanguard originally used the sharding solution of MySQL Database, which does not support scaling. By deploying OceanBase Database, a native distributed database solution, in place of MySQL Database, Vanguard realizes on-demand scaling, reduces the required storage size by 60% from 15 TB to 6 TB, and achieves a recovery point objective (RPO) of zero.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958917144.png" alt="1713958918" class="img_ev3q"></p>
<p>With its infrastructure, including the databases, built in Baidu Cloud, Alibaba Cloud, and Tencent Cloud, Zuoyebang is in dire need of a system that can tackle this multi-infrastructure environment. Against this backdrop, it replaces MySQL with OceanBase. Backed by the high availability and disaster tolerance capabilities of OceanBase Database, Zuoyebang now sets up a multi-active architecture in a multi-infrastructure environment, handling hybrid transaction/analytical processing (HTAP) loads with just one system. Moreover, with OceanBase Database, the number of servers required is reduced from 24 to 9, reducing hardware costs by 60%.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958947472.png" alt="1713958948" class="img_ev3q"></p>
<p>Dmall is a digital retail solution provider that serves many supermarkets in various sizes, such as Wumart. Dmall once used a large number of scattered MySQL servers, which do not support scaling and incur high O&amp;M costs. The multitenancy capability of OceanBase Database enables database consolidation for Dmall, greatly reducing O&amp;M costs. OceanBase Database also achieves a compression ratio of 6:1 while delivering higher performance even in standalone mode.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958984052.png" alt="1713958985" class="img_ev3q"></p>
<p>Ctrip is one of the earliest customers of OceanBase Database Community Edition. Ctrip had a history database that used the sharding solution of MySQL Database, which did not support dynamic online scaling, failing to cope with the drastic data volume increase of the history database. Therefore, they migrated their history database to OceanBase Database. The native distributed database solution of OceanBase avoids database and table sharding, reduces the required storage space by 85%, and improves write performance several times.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713958998115.png" alt="1713958999" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ii-oceanbase-database-community-edition-in-real-time-ap-and-multi-model-scenarios"><strong>ii. OceanBase Database Community Edition in real-time AP and multi-model scenarios</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#ii-oceanbase-database-community-edition-in-real-time-ap-and-multi-model-scenarios" class="hash-link" aria-label="ii-oceanbase-database-community-edition-in-real-time-ap-and-multi-model-scenarios的直接链接" title="ii-oceanbase-database-community-edition-in-real-time-ap-and-multi-model-scenarios的直接链接">​</a></h3>
<p>Although OceanBase Database was initially designed to handle mission-critical core business scenarios, our developers also use OceanBase Database in real-time analytical processing (AP) and multi-model scenarios.</p>
<p>Kuayue Express once used the "HBase + Kafka + StarRocks" architecture for its analytics scenario. They developed a system called HBase CDC, which pulls data from HBase to Kafka and then synchronizes the data to StarRocks. This solution features high development costs and a complex link, which also results in a long data processing period. With OceanBase Database, Kuayue Express now needs only one system, reducing hardware costs by 50% while improving data processing efficiency by 50%.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959020294.png" alt="1713959021" class="img_ev3q"></p>
<p>Beike has a dictionary service that was once established on HBase, which is frequently criticized for two major issues:</p>
<p>○ Massive complex components due to its dependence on the Hadoop system</p>
<p>○ Lack of support for secondary indexes</p>
<p>In view of this, Beike replaced HBase with OBKV, which improves the query performance by 2 to 5 times and the write performance by 5 times with a simplified system complexity.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959057480.png" alt="1713959058" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="iii-all-in-one-database-driven-by-developers"><strong>iii. All-in-one database driven by developers</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#iii-all-in-one-database-driven-by-developers" class="hash-link" aria-label="iii-all-in-one-database-driven-by-developers的直接链接" title="iii-all-in-one-database-driven-by-developers的直接链接">​</a></h3>
<p>Sometimes I wonder why developers would use a distributed database that was initially designed for mission-critical core business scenarios in real-time AP or multi-model scenarios as an all-in-one database. I believe the answer lies in the two initial design concepts of OceanBase Database.</p>
<p>○ Distributed architecture: The distributed architecture of OceanBase Database enables it to process massive data and support automatic scaling.</p>
<p>○ Log-structured merge-tree (LSM-tree)-based storage: This achieves a high compression ratio, enabling processing of massive data at a low cost.</p>
<p>With these capabilities, OceanBase Database is particularly suitable for scenarios involving massive data, including not only TP scenarios, but also AP and multi-model scenarios, which naturally involve large amounts of data.</p>
<p>Therefore, it is fair to say that it is the demand of developers and users that has driven OceanBase Database to grow from the early distributed TP system to the distributed AP system, and then into the present-day all-in-one database. With this integrated architecture, OceanBase Database will surely rise up to the future challenges posed by multi-model and AI scenarios, achieving lower IT costs for users.</p>
<p>What lies behind the all-in-one database is the integrated architecture, which includes not only the integrated storage engine, integrated transaction engine, and integrated SQL engine in the kernel, but also the multi-model engine built on the integrated architecture, and the capability to adapt to multiple infrastructures.</p>
<p>OceanBase Database is also a multi-cloud native all-in-one database, which can natively support both public and private clouds with different architectures in one system, including Huawei Cloud, Tencent Cloud, Alibaba Cloud, Amazon Web Services (AWS), Google Cloud Platform (GCP), and Azure. This means that developers can use OceanBase services in different clouds with the same experience.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ii-significance-of-integration-for-developers"><strong>II. Significance of integration for developers</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#ii-significance-of-integration-for-developers" class="hash-link" aria-label="ii-significance-of-integration-for-developers的直接链接" title="ii-significance-of-integration-for-developers的直接链接">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="i-transparent-deployment-in-standalone-and-distributed-modes"><strong>i. Transparent deployment in standalone and distributed modes</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#i-transparent-deployment-in-standalone-and-distributed-modes" class="hash-link" aria-label="i-transparent-deployment-in-standalone-and-distributed-modes的直接链接" title="i-transparent-deployment-in-standalone-and-distributed-modes的直接链接">​</a></h3>
<p>OceanBase Database supports an integrated architecture for standalone and distributed modes. The deployment is transparent to users. Today, the first question facing developers when choosing a database system is whether to choose a centralized or distributed one.</p>
<p>OceanBase Database resolves the dilemma by providing an integrated architecture that supports both standalone and distributed modes. This integrated architecture enables OceanBase Database to support smooth on-demand scaling and small-specification deployment, making OceanBase Database a stand-out choice for large enterprises, SMEs, and even start-ups. Moreover, OceanBase Database also supports three replicas based on Paxos and primary/standby synchronization, and achieves an RPO of 0 and a recovery time objective (RTO) of less than 8 seconds.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959350210.png" alt="1713959351" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ii-obkv-adding-a-query-api-rather-than-a-database"><strong>ii. OBKV: adding a query API rather than a database</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#ii-obkv-adding-a-query-api-rather-than-a-database" class="hash-link" aria-label="ii-obkv-adding-a-query-api-rather-than-a-database的直接链接" title="ii-obkv-adding-a-query-api-rather-than-a-database的直接链接">​</a></h3>
<p>SQL and non-SQL models are integrated to achieve multi-model integration. I believe that multi-model integration is not just about providing a new data model, but more about integrating different data models to combine their respective strengths.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959360450.png" alt="1713959361" class="img_ev3q"></p>
<p>Take HBase, which has been well-explored by many developers, as an example. The write interface of HBase is relatively simple, efficient, and easy to use. However, HBase does not support SQL syntax, and therefore its query interfaces have limited features and are inconvenient to use. With OBKV, we can write data to OceanBase Database in an HBase-compatible way and query data from OceanBase Database by using standard SQL syntaxes, giving full play to technical advantages of both SQL and NoSQL systems.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="iii-htap--oltp-plus"><strong>iii. HTAP = OLTP Plus</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#iii-htap--oltp-plus" class="hash-link" aria-label="iii-htap--oltp-plus的直接链接" title="iii-htap--oltp-plus的直接链接">​</a></h3>
<p>Speaking of the integration of TP and AP systems, we often mention HTAP. As I have said in many occasions, HTAP is not omnipotent.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959393533.png" alt="1713959394" class="img_ev3q"></p>
<p>In most cases, HTAP is merely a plus version of online transaction processing (OLTP) that supports online analytical processing (OLAP) capabilities on the basis of OLTP. The following two deployment modes are supported:</p>
<p>(1) Based on the multi-replica distributed architecture of OceanBase Database, all replicas use the same storage architecture, either row-based storage or hybrid row-column storage, and the leader directly provides services. This deployment mode achieves zero data latency while ensuring data consistency, but provides only moderate support for AP capabilities due to the lack of columnar storage. Therefore, this deployment mode is suitable for "OLTP + lightweight OLAP" scenarios.</p>
<p>(2) The leader uses row-based storage or hybrid row-column storage, while one or multiple followers use columnar storage. This service mode incurs data latency between the leader and the followers, but can better support OLAP capabilities. Therefore, it is suitable for "OLTP + medium-load OLAP" scenarios.</p>
<p>To wrap up, HTAP is a solid option in scenarios involving hundreds of GB to hundreds of TB of data, but it is not omnipotent even with these two flexibly deployment modes. Many large companies with a larger data amount usually deploy separate TP and AP systems.</p>
<p>Take Haidilao as an example. Haidilao originally used PolarDB and PolarDB-X for OLTP and AnalyticDB for OLAP, and used Data Transmission Service (DTS) to synchronize data from PolarDB to AnalyticDB.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959410909.png" alt="1713959411" class="img_ev3q"></p>
<p>Haidilao often do promotions in holidays and need to make recommendations according to customer preferences in real time. The HTAP capabilities of OceanBase Database implement both TP and AP based on the same set of data, helping Haidilao reduce the total cost of ownership (TCO) by 35% and improving AP performance by 30%.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="iii-use-tp--ap-integration-to-fuse-core-capabilities-of-distributed-tp-systems-into-ap-systems"><strong>III. Use TP &amp; AP integration to fuse core capabilities of distributed TP systems into AP systems</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#iii-use-tp--ap-integration-to-fuse-core-capabilities-of-distributed-tp-systems-into-ap-systems" class="hash-link" aria-label="iii-use-tp--ap-integration-to-fuse-core-capabilities-of-distributed-tp-systems-into-ap-systems的直接链接" title="iii-use-tp--ap-integration-to-fuse-core-capabilities-of-distributed-tp-systems-into-ap-systems的直接链接">​</a></h2>
<p>I've mentioned earlier that traditional HTAP is not omnipotent. Here is a new concept that I'd like to share with you: using TP &amp; AP integration to fuse core capabilities of distributed TP systems into AP systems.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959636626.png" alt="1713959637" class="img_ev3q"></p>
<p>What does it mean? Instead of providing TP and AP services with one system, we can directly integrate TP capabilities into an AP system to build a new real-time analytical database that is easier to use for developers.</p>
<p>We all know that traditional OLAP systems often provide strong large query capabilities, while AP systems feature robust ecological adaptation but do not support real-time writing due to the lack of support for row storage. Therefore, traditional AP systems do not support real-time point queries or real-time serving. Moreover, AP systems are no match for TP systems in terms of syntax compatibility and functionality. Regardless of their diversity, AP systems today are rarely applied in core business scenarios, and therefore fall short of adequate testing in terms of reliability and stability.</p>
<p>OceanBase Database offers sound distributed capabilities, high reliability, and high availability with an RPO of zero. It also provides favorable TP capabilities and supports real-time writing and table access by secondary index primary key for point queries, achieving dynamic serving in AP scenarios. It is also compatible with MySQL Database. Besides, OceanBase also provides a GUI-based O&amp;M tool called OceanBase Cloud Platform (OCP).</p>
<p>OceanBase combines these capabilities with traditional AP capabilities to offer you a new-generation real-time AP system. This system offers better realtimeness and allows you to directly perform both large queries and serving. It is compatible with MySQL Database and is easy to use. Moreover, you can directly use OceanBase control tools for integrated control.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="iv-release-of-oceanbase-database-v430"><strong>IV. Release of OceanBase Database V4.3.0</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#iv-release-of-oceanbase-database-v430" class="hash-link" aria-label="iv-release-of-oceanbase-database-v430的直接链接" title="iv-release-of-oceanbase-database-v430的直接链接">​</a></h2>
<p>Today, I'm honored here to announce the official release of OceanBase Database V4.3.0. This version launches three core technological upgrades:</p>
<p>○ Columnar storage engine</p>
<p>○ Further strengthened TP &amp; AP integration</p>
<p>○ Near-petabyte-scale real-time analytical database that supports data sized [1TB, 1PB)</p>
<p>Let's dive into the core capabilities of OceanBase Database V4.3.0.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959734334.png" alt="1713959735" class="img_ev3q"></p>
<p>(1) Enhanced TP &amp; AP integration and OLTP capabilities</p>
<p>In KV scenarios, OceanBase Database V4.3.0 greatly outperforms V4.2.1, with 70% higher single-row reading and writing performance and 80% to 220% higher batch reading and writing performance. This version is also optimized in terms of SQL, transaction, and log features.</p>
<p>This version also comes with the fast tenant cloning feature. I believe you would love it. The feature allows you to clone a tenant to generate a snapshot before performing risky operations, so that you can quickly restore the tenant if these operations go wrong.</p>
<p>(2) Enhanced real-time AP capabilities</p>
<p>The new version supports bypass import, external tables, columnar storage, and dynamic conversion between row-based storage and columnar storage.</p>
<p>Many features useful in OLAP scenarios are also provided, such as materialized views, federated queries, window functions, common table expressions (CTEs), hierarchical queries, and operator pushdown.</p>
<p>The distributed computing engine is also significantly enhanced with a Massively Parallel Processing (MPP) architecture and supports the vectorized engine and auto DOP. Moreover, this version also enhances support for semi-structured data, such as JSON and geographic information system (GIS) data. OceanBase Database V4.3.0 is compatible with most mainstream streaming databases in the industry, such as Kafka and Flink.</p>
<p>(3) Improved OLAP performance</p>
<p>Compared with OceanBase Database V4.2.1, the new version offers 25% higher TPC-H 1 TB test performance, 111% higher TPC-DS 1 TB test performance, and six times higher bypass import performance. Some may argue that the 25% TPC-H performance increase is but a mild one. I must remind you here that OceanBase Database has topped the TPC-H benchmark test. So, it's a laudable climb from the top.</p>
<p>(4) Improved AP compatibility</p>
<p>(5) Greater ease of use</p>
<p>This version provides AP parameter templates and scenario-based AP documents, greatly improving the ease of use. We believe that the scenario-based documents can be a great help during your exploration of the new version.</p>
<p>Although OceanBase Database V4.3.0 is officially released today, articles about experiencing the new version are already available on the Internet. You are welcome to learn more about OceanBase Database V4.3.0 in our exhibition area and on the official website.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="i-benchmarking-based-on-analytical-loads-oceanbase-database-versus-top-notch-columnar-databases-in-wide-table-queries"><strong>i. Benchmarking based on analytical loads: OceanBase Database versus top-notch columnar databases in wide-table queries</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#i-benchmarking-based-on-analytical-loads-oceanbase-database-versus-top-notch-columnar-databases-in-wide-table-queries" class="hash-link" aria-label="i-benchmarking-based-on-analytical-loads-oceanbase-database-versus-top-notch-columnar-databases-in-wide-table-queries的直接链接" title="i-benchmarking-based-on-analytical-loads-oceanbase-database-versus-top-notch-columnar-databases-in-wide-table-queries的直接链接">​</a></h3>
<p>As a routine of OceanBase launch events, here comes the benchmarking part, which is also one of my favorite parts of this conference. Now we will do the analytical load-based benchmark tests together.</p>
<p>In fact, before this event, we've already run benchmark tests on OceanBase Database and some mainstream real-time analytical databases in the industry and compared the test results. We will use ClickBench, a standard-bearer benchmark for analytical databases in the industry proposed by ClickHouse (which also tops the ClickHouse benchmark rankings), to evaluate the performance of OceanBase Database and ClickHouse.</p>
<p>We have run a benchmark test on OceanBase Database V4.3.0 Beta and ClickHouse 23.11 last year. We later commercialized and evolved OceanBase Database V4.3.0 Beta into V4.3.0. ClickHouse also issued a new release, namely release 24.4, in April this year, which outperforms its predecessor 23.11. Fortunately, we have OceanBase Database V4.3.x Beta.</p>
<p>So today, we will have four systems for the test: OceanBase Database V4.3.0 and V4.3.x Beta, as well as ClickHouse 23.11 and 24.4, which are respectively the latest two versions of OceanBase and ClickHouse.</p>
<p>Now the benchmark test has started. As you know, ClickBench has 43 queries, respectively named Q0 to Q42. In the test, each query is executed 3 times, and the result of the execution that takes the shortest time is taken as the final result of the query.</p>
<p>The systems in the test are displayed either in green or red. A green background indicates that the system runs faster, while a red one indicates slower running. Now that the benchmark test is done, you can see that OceanBase Database V4.3.0 takes 14.5 seconds, ClickHouse 23.11 takes 14.8 seconds, OceanBase Database V4.3.x Beta 12.85 seconds, and ClickHouse 24.4 14.26 seconds. In summary, OceanBase Database V4.3.0 outperforms ClickHouse 23.11, but is outshined by ClickHouse 24.4, while OceanBase Database V4.3.x Beta surpasses all, including ClickHouse 24.4. They are in close tie though.</p>
<p>According to the test results, we can see that under the same hardware conditions, OceanBase Database V4.3.0 achieves a wide table query performance comparable with that of ClickHouse, which is best in class in the real-time analytical database industry.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959829606.png" alt="1713959830" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ii-aspiring-to-build-an-omnipotent-real-time-analytical-database-for-sub-petabyte-scale-scenarios"><strong>ii. Aspiring to build an omnipotent real-time analytical database for sub-petabyte-scale scenarios</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#ii-aspiring-to-build-an-omnipotent-real-time-analytical-database-for-sub-petabyte-scale-scenarios" class="hash-link" aria-label="ii-aspiring-to-build-an-omnipotent-real-time-analytical-database-for-sub-petabyte-scale-scenarios的直接链接" title="ii-aspiring-to-build-an-omnipotent-real-time-analytical-database-for-sub-petabyte-scale-scenarios的直接链接">​</a></h3>
<p>I've mentioned earlier that the all-in-one architecture of OceanBase Database enables it to tackle various scenarios such as TP, AP, multi-model, KV, and AI scenarios. It sounds as if OeanBase Database suits all purposes. Then what are the scenarios that OceanBase Database is truly suitable for? The scenarios can be classified into the following categories:</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959858594.png" alt="1713959859" class="img_ev3q"></p>
<p>(1) Express OLTP</p>
<p>Such scenarios generally involve simple read and write operations at high concurrency in the Internet industry. MySQL Database usually fits these scenarios.</p>
<p>(2) Complex OLTP</p>
<p>Such scenarios involve some complex queries, batch operations, and stored procedures in addition to simple read and write operations. These scenarios are often encountered in conventional industries for which commercial databases such as Oracle are well-suited.</p>
<p>(3) OBKV</p>
<p>Systems such as HBase and Redis are apt for these scenarios.</p>
<p>(4) HTAP</p>
<p>An HTAP system processes both OLTP and real-time AP loads. In my opinion, HTAP is applicable to scenarios involving hundreds of GB to hundreds of TB of data.</p>
<p>(5) Real-time AP scenarios with larger data amounts</p>
<p>The difference between real-time AP and HTAP is that the data source for HTAP is a system, while the data source for real-time AP is not only the TP system of OceanBase Database, but also may be other databases, Kafka or Flink instances, files, or storage systems. The real-time AP system will further extend the AP capabilities of OceanBase Database.</p>
<p>Real-time AP is suitable for scenarios with a data amount of 1 TB to 1 PB. A scenario with a data amount of more than 1 PB can be taken as a large enterprise scenario. Large enterprises, such as Alibaba and Tencent, have their own in-house data lakes or big data analysis systems, which fall out of the application scenarios of OceanBase Database.</p>
<p>We can build a lightweight real-time analytical database directly based on the real-time AP capabilities of OceanBase Database. This analytical database supports diverse data sources, real-time writes, batch writes, and some batch updates. After the data arrives at OceanBase Database, it will be processed in layers, first by the operational data store (ODS) layer, and then by the data warehouse detail (DWD), data warehouse summary (DWS), and application data service (ADS) layers in sequence.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959901104.png" alt="1713959902" class="img_ev3q"></p>
<p>The ODS layer stores raw data, and the real-time data warehouse usually uses columnstore tables. DWD and DWS process raw data in two layers, usually by using materialized views. Then, the ADS layer carries out serving. It uses row-based storage for simple point queries and table access by index primary key. However, when the data amount of each query is large, it uses columnar storage for serving.</p>
<p>In OceanBase Database V4.3.0, we can carry out both large queries on the data warehouse and serving at the same time in one system, providing users with better real-time capabilities. In addition, the enhanced compatibility with MySQL makes it easier to use for developers.</p>
<p>We aspire to forge the real-time AP system of OceanBase Database into an omnipotent real-time analytical database for sub-petabyte-scale scenarios.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959924580.png" alt="1713959925" class="img_ev3q"></p>
<p>First, it is suitable for scenarios with a data amount of 1 TB to 1 PB. It is designed for users with real-time demands, whereas users with pure offline analytics demands have many other options. OceanBase Database provides strong TP capabilities, supports real-time writing, and directly provides AP services through point queries. OceanBase Database also has sound MySQL compatibility and good distributed capabilities for high availability. Moreover, it also offers high stability, with basically no bugs.</p>
<p>Second, OceanBase Database provides exceptional real-time AP capabilities, and realizes columnar storage and vectorization. Notably, it has topped both the ClickBench and TPC-H benchmark rankings. I believe that the combination of these two capabilities will make OceanBase Database an excellent choice of real-time analytical database for sub-petabyte-scale scenarios.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="iii-whats-next"><strong>iii. What's next</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#iii-whats-next" class="hash-link" aria-label="iii-whats-next的直接链接" title="iii-whats-next的直接链接">​</a></h3>
<p>We have a lot to do next. In the first quarter, we released OceanBase Database V4.3.0 in this developers conference. This version enhances the capabilities of columnar storage and the vectorized engine. In the months to come, we will continue to tap the potentials of OceanBase All-in-one, to improve its integration with and support for multi-model, search, and AI capabilities.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713959950394.png" alt="1713959951" class="img_ev3q"></p>
<p>In the second quarter, we will develop full-text indexing and JSON multi-value indexing capabilities and strengthen database search capabilities. In the third quarter, we will further support vector database capabilities by using components. With support for vector databases, you can directly develop your own large model applications based on OceanBase Database. In the fourth quarter, we will implement storage-compute separation capabilities based on Amazon Simple Storage Service (S3).</p>
<p>OceanBase Database has many users in public clouds. As we all know, storage and computing are separated in a public cloud. However, currently, OceanBase Database implements storage-compute separation based on expensive cloud disks of public clouds. With the object storage capability based on S3, we can greatly improve the cost performance of OceanBase Database.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="v-improved-ease-of-use"><strong>V. Improved ease of use</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#v-improved-ease-of-use" class="hash-link" aria-label="v-improved-ease-of-use的直接链接" title="v-improved-ease-of-use的直接链接">​</a></h2>
<p>Many developers speak to me highly of OceanBase Database in the following aspects:</p>
<p>The first is its powerful technological capabilities. We enjoy running benchmark tests in our launch events. Sometimes we run TPC-C benchmark, sometimes TPC-H or ClickBench, and other times ClickHouse. It's a bit like having your excellent child competing for top universities and showing off their achievements in your neighborhood. This is the core technological capabilities of OceanBase Database.</p>
<p>The second is that OceanBase Database realizes high O&amp;M efficiency for proficient developers or DBAs. One Alipay DBA can operate and maintain thousands of servers. However, you can fully explore the powerful features of OceanBase Database only after you gain profound knowledge about it.</p>
<p>Therefore, we have made a lot of effort to make OceanBase Database easier to use and get started with.</p>
<p>Next, let's take a look at some video clips from our users and developers concerning the ease of use of OceanBase Database.</p>
<p>We would like to extend our gratitude to Chunlei, Guangming, and Baishan. Thank you for your praise and encouragement. Your constructive comments are also highly appreciated. We will spare no efforts to meet your high expectations for OceanBase Database.</p>
<p>Speaking of ease of use, I often think of the technical books I have read. Such books are often titled something like "From Beginners to Experts", "Seven Days to Learn xxx", and "Learn a programming language in 21 days". That is why I named this presentation "OceanBase – From Beginners to Experts"</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960010018.png" alt="1713960011" class="img_ev3q"></p>
<p>The ease-of-use feature is helpful for the following two types of users:</p>
<p>The first type is beginners. These users are more concerned about how to quickly deploy OceanBase Database, whether a GUI or CLI tool is available for quickly performing benchmark tests and basic demo tests, and whether reference documents are available.</p>
<p>The other type is users who have been using OceanBase Database for a while. For these users, all their requirements can be summarized into one question: What do I do when something goes wrong? This seemingly simple question is hard to answer. A situation going wrong may be caused by various exceptions, such as common exceptions like server failures, network failures, and disk failures, or underlying issues like system suspension or jitter caused by unknown reasons. Therefore, experienced users tend to look for answers in the debug logs of OceanBase Database.</p>
<p>In the previous videos, we can see some of the most basic GUI-based features of OceanBase Database. OCP is quite a standout in the industry. Guangming even said that OCP is the best ecological tool that he has ever used throughout his career. I'm thrilled to hear that. The credit goes to our OCP team.</p>
<p>However, OceanBase Database still entails some drawbacks. For example, the quickstart tutorial needs to be optimized. Many users may find it difficult to come by the relevant documents when they install and deploy OceanBase Database. In addition, no effective tools are available for diagnosing some underlying issues such as jitters and system suspension. The debug logs of OceanBase Database are also inferior to those of Oracle Database in terms of observability and ease of use.</p>
<p>In fact, we've invested considerable efforts to make relevant improvements last year, though some expectations are yet to be fulfilled.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="i-ease-to-learn-lowering-the-barrier-for-getting-started"><strong>i. Ease to learn: lowering the barrier for getting started</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#i-ease-to-learn-lowering-the-barrier-for-getting-started" class="hash-link" aria-label="i-ease-to-learn-lowering-the-barrier-for-getting-started的直接链接" title="i-ease-to-learn-lowering-the-barrier-for-getting-started的直接链接">​</a></h3>
<p>We have put in a great deal of work to lower the barrier for novice users from the following aspects:</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960041671.png" alt="1713960042" class="img_ev3q"></p>
<p>(1) Installation and deployment</p>
<p>Chunlei has mentioned some issues related to installation and deployment in the video earlier. Last year, we achieved quick installation and deployment of OceanBase Database within two minutes by using OBD. However, OBD offers limited features. Though boasting rich and powerful features, OCP supports the installation of OceanBase Database only in an environment that meets specific hardware conditions. Neither of the two tools can fully meet the needs of developers.</p>
<p>In light of this, we combine OBD and OCP. You can use OBD to install OceanBase Database and then use OCP to run the database, so that you can tap the rich management and control capabilities of OCP after installation. This resolves the installation and deployment issues mentioned by Chunlei.</p>
<p>(2) Performance tests</p>
<p>OceanBase Database is renowned for its performance. Yet many developers told me that they must become OceanBase experts to bring out its full potential.</p>
<p>Therefore, we provide parameter templates based on scenarios, enabling developers to build OceanBase databases that can yield remarkable benchmark test results.</p>
<p>(3) Documentation</p>
<p>OceanBase documentation is voluminous. There are only slightly over 1,900 topics for OceanBase Database V2.x, and over 2,900 for V3.x, but over 3,900 for V4.x. Over 1,000 topics are added in each version upgrade.</p>
<p>Despite the substantial efforts invested into documentation optimization last year, there is still much room for improvement. We hope to receive more insights about documentation and ease of use from more developers like Baishan.</p>
<p>We position OceanBase Database as a globally popular database. To achieve this goal, we must deliver product documentation that meets world-class standards.</p>
<p>Besides documentation, the following two features can also be useful for developers:</p>
<p>One is online experience. It would be more constructive if we can experience first-hand the features described in the documentation. Therefore, we provide the online experience feature, which provides an environment for you to experience online the OceanBase knowledge described in the articles. With the online experience feature, you can copy an operation and check the effect directly in the cloud environment. We also have a documentation area here. You are welcome to try it out.</p>
<p>The other is the knowledge base. We enhanced the OceanBase knowledge base last year and added more than 1000 cases. These cases are based on many years' experience of OceanBase and Ant Group DBAs in mining and solving problems in customer scenarios. We hope it can be helpful for developers.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ii-ease-of-diagnosis-improved-diagnostic-capabilities"><strong>ii. Ease of diagnosis: improved diagnostic capabilities</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#ii-ease-of-diagnosis-improved-diagnostic-capabilities" class="hash-link" aria-label="ii-ease-of-diagnosis-improved-diagnostic-capabilities的直接链接" title="ii-ease-of-diagnosis-improved-diagnostic-capabilities的直接链接">​</a></h3>
<p>We have also further improved the diagnostic capabilities of OceanBase Database.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960095220.png" alt="1713960096" class="img_ev3q"></p>
<p>OceanBase Database excels in handling simple exceptions based on OCP, such as some simple network failures, disk failures, and simple compaction issues. This is because OCP can detect the causes for such exceptions. However, as for deep-seated issues whose resolution requires profound understanding of OceanBase Database, OCP is found inadequate. Therefore, we provide the following tools as a complement:</p>
<p>(1) Active Session History (ASH): the perf-like tool of OceanBase Database</p>
<p>We all know the perf tool of Linux. When something goes wrong, we turn to perf to find the cause.</p>
<p>Last year, we developed a tool called ASH, an OceanBase version of perf. You can locate causes of issues by referring to ASH reports. Seemingly easy, developing ASH was actually very challenging for us.</p>
<p>We've set up a dedicated performance diagnostics team made up of many senior OceanBase kernel developers. They spent over a year developing a time model that ensures time accuracy for all backend tasks, locks, wait events, queue entry, queue exit, and so on.</p>
<p>(2) OceanBase Autonomy Service (OAS): for root cause analysis</p>
<p>OAS incorporates rules that are formulated based on years of customer service experience of Ant Group and OceanBase. After you locate the issue based on ASH reports, you can use OAS to identify the root cause.</p>
<p>(3) <code>alert.log</code></p>
<p>OceanBase Database has long been criticized for its debug logs, which contain excess content, making it difficult for developers to find useful information.</p>
<p>This year, we launched the <code>alert.log</code> file based on suggestions from developers. This file is an extract from the debug logs, and records the common system events that occur during the operation of OceanBase Database. Developers can resolve 80% of the issues that they encounter by referring to <code>alert.log</code>, without the need to dig through the long-winded <code>observer.log</code> file.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="iii-on-demand-use-of-serverless-instances-with-a-1-month-free-trial"><strong>iii. On-demand use of serverless instances, with a 1-month free trial</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#iii-on-demand-use-of-serverless-instances-with-a-1-month-free-trial" class="hash-link" aria-label="iii-on-demand-use-of-serverless-instances-with-a-1-month-free-trial的直接链接" title="iii-on-demand-use-of-serverless-instances-with-a-1-month-free-trial的直接链接">​</a></h3>
<p>The concept of "serverless" is intrinsically linked with cloud native. Compared with common instances, serverless instances support more flexible scaling methods and full on-demand use, making them a cost-effective choice for developers.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960146102.png" alt="1713960147" class="img_ev3q"></p>
<p>OceanBase Database serverless instances are now supported in ApsaraDB for OceanBase. A one-month free trial is provided for serverless instances with a specification of 1C4G on Alibaba Cloud and Huawei Cloud. We have a free trial demo here in the exhibition area. I was told that it is so popular that the quota is used up. Don't worry. We are applying for more quota.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="vi-a-more-open-technology-ecosystem"><strong>VI. A more open technology ecosystem</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#vi-a-more-open-technology-ecosystem" class="hash-link" aria-label="vi-a-more-open-technology-ecosystem的直��接链接" title="vi-a-more-open-technology-ecosystem的直接链接">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="i-smooth-integration-of-binlog-service-with-over-20-downstream-services"><strong>i. Smooth integration of binlog service with over 20 downstream services</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#i-smooth-integration-of-binlog-service-with-over-20-downstream-services" class="hash-link" aria-label="i-smooth-integration-of-binlog-service-with-over-20-downstream-services的直接链接" title="i-smooth-integration-of-binlog-service-with-over-20-downstream-services的直接链接">​</a></h3>
<p>Last year, we also developed a useful feature, a MySQL-compatible binlog service, which has been integrated with more than 20 downstream services, including some MySQL subscription tools and binlog-based cloud services.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960190610.png" alt="1713960191" class="img_ev3q"></p>
<p>I'd like to share some untold stories behind this feature. The OceanBase product R&amp;D department will present a "Breakthrough Award" at the end of each year. It's the top award in our department. The final candidates of the award last year were the columnar storage project team and the binlog project team.</p>
<p>Well, as it turned out, the binlog project team won the award. I am glad that our department leaders voted for the binlog project team that is dedicated to improve ease of use, instead of the columnar storage project team that is set out to enhance core product capabilities. This is a testament that our production and research team has realized that improved ease of use means more to developers than modest performance enhancement.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ii-oceanbase-landscape-from-basic-ecological-adaptation-to-open-technology-ecosystem"><strong>ii. OceanBase landscape: from basic ecological adaptation to open technology ecosystem</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#ii-oceanbase-landscape-from-basic-ecological-adaptation-to-open-technology-ecosystem" class="hash-link" aria-label="ii-oceanbase-landscape-from-basic-ecological-adaptation-to-open-technology-ecosystem的直接链接" title="ii-oceanbase-landscape-from-basic-ecological-adaptation-to-open-technology-ecosystem的直接链接">​</a></h3>
<p>This is the landscape of OceanBase ecological tools.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960205995.png" alt="1713960206" class="img_ev3q"></p>
<p>OceanBase ecological tools have gone through two main stages in their development.</p>
<p>The first stage is ecological adaptation, including the adaption of ecological tools to the OceanBase Database kernel and adaption of OceanBase Database as a database ecosystem to other database ecosystems, such as Kubernetes and big data.</p>
<p>The second stage is joint construction of the ecosystem based on open APIs. At present, more than 750 mainstream products have joined the OceanBase ecosystem, and joined forces with OceanBase to build the open ecosystem.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="iii-sustained-efforts-to-lower-entry-barriers-for-developers-based-on-open-source"><strong>iii. Sustained efforts to lower entry barriers for developers based on open source</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#iii-sustained-efforts-to-lower-entry-barriers-for-developers-based-on-open-source" class="hash-link" aria-label="iii-sustained-efforts-to-lower-entry-barriers-for-developers-based-on-open-source的直接链接" title="iii-sustained-efforts-to-lower-entry-barriers-for-developers-based-on-open-source的直接链接">​</a></h3>
<p>Finally, I'd like talk about the open source community of OceanBase.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960234174.png" alt="1713960235" class="img_ev3q"></p>
<p>The OceanBase open source community was originally positioned as a "responsive" community where user feedbacks will be responded in a timely manner. Now we have upgrade the community to be an interactive one.</p>
<p>I've always believed that open source is not just about opening up and sharing some source code of products or technologies, but more about the open source community being a bridge that brings together people interested in a certain product. This is one of the key insights I gained last year.</p>
<p>Gladly, our user organization OUG finally went on track last year. We've held a series of OUG city visit and enterprise visit events in concert with a bunch of enterprises such as 58, Zhihu, and Vivo in the open source community.</p>
<p>Many developers have shared their thoughts in the open source community. More than 118 developers opened their blog accounts and published more than 1,000 technical blogs. The community also boasts an impressive collection of co-developed open source projects, including six warehouses containing over 50,000 code lines built jointly by OceanBase and other companies or developers.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="vii-huge-kudos-to-developers-who-built-a-bridge-for-communication-with-inspired-initiatives"><strong>VII. Huge kudos to developers, who built a bridge for communication with inspired initiatives</strong><a href="https://oceanbase.github.io/zh-Hans/blog/all-in-one#vii-huge-kudos-to-developers-who-built-a-bridge-for-communication-with-inspired-initiatives" class="hash-link" aria-label="vii-huge-kudos-to-developers-who-built-a-bridge-for-communication-with-inspired-initiatives的直接链接" title="vii-huge-kudos-to-developers-who-built-a-bridge-for-communication-with-inspired-initiatives的直接链接">​</a></h2>
<p>Our special thanks go to the 108 developers entitled "Star of the Month". Your active participation has made our community a better place to learn about OceanBase. We also highly appreciate the efforts of the application developers and 315 OceanBase contributors for jointly building the OceanBase ecosystem and making the open source endeavor more fruitful.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960304441.png" alt="1713960305" class="img_ev3q"></p>
<p>Last but not least, I'd like to share with you a project case from the OceanBase community. It is an initiative made by a developer to build a vector engine plug-in to integrate SQL and AI in OceanBase Database.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713960316156.png" alt="1713960317" class="img_ev3q"></p>
<p>Though OceanBase Database currently does not support vector databases, he still spent months in writing a vector engine plug-in based on the open source code of OceanBase Database. He also built a personal knowledge base based on the vector engine plug-in that he wrote. This knowledge base supports natural language retrieval.</p>
<p>This initiative is fascinating. We look forward to more initiatives like this coming up in the OceanBase community. We sincerely hope that our open source community will become a warm haven where people can have fun and make friends.</p>
<p>That's all for my sharing today. Look forward to seeing you again at the 2024 Release Conference.</p>]]></content:encoded>
            <category>Release</category>
            <category>OceanBase</category>
        </item>
        <item>
            <title><![CDATA[OceanBase provides users with sufficiently flexible and simple I/O resource isolation experiences.]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/io-isolation</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/io-isolation</guid>
            <pubDate>Mon, 03 Jun 2024 13:23:07 GMT</pubDate>
            <description><![CDATA[Sun Jianyun, an OceanBase technical expert]]></description>
            <content:encoded><![CDATA[<p><strong>Sun Jianyun, an OceanBase technical expert</strong></p>
<p>He used to be a member of the TPC-C project and the technical support team for Double 11 shopping festivals, and is now engaged in the design, research, and development of I/O scheduling, DDL capabilities, and other work related to storage engines.</p>
<p>In <a href="https://open.oceanbase.com/blog/10900412" target="_blank" rel="noopener noreferrer">Why resource isolation matters to HTAP? </a>, we talked about why hybrid transaction/analytical processing (HTAP) relies on resource isolation and how to implement it. Resource isolation is a capability. Many scenarios can be derived from it, such as HTAP, multitenancy, and pay-as-you-go. Based on resource isolation and cloud-based resource pools, all kinds of resources can be allocated on demand. Isolation of resources such as CPU and memory is already supported in OceanBase Database V4.0. OceanBase Database V4.1 supports enhanced disk I/O isolation and provides users a simple and flexible way to use this feature.</p>
<p>We believe that disk I/O isolation is an essential part of resource isolation. Disk I/O isolation enhances and completes the resource control capabilities for users. This article describes some thoughts of the OceanBase team on disk I/O isolation, how it is configured in OceanBase Database, and the disk I/O isolation performance testing of OceanBase Database V4.1.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="i-why-is-disk-io-isolation-necessary">I. Why is disk I/O isolation necessary?<a href="https://oceanbase.github.io/zh-Hans/blog/io-isolation#i-why-is-disk-io-isolation-necessary" class="hash-link" aria-label="I. Why is disk I/O isolation necessary?的直接链接" title="I. Why is disk I/O isolation necessary?的直接链接">​</a></h2>
<p>Some may ask, "Is resource isolation, especially disk I/O isolation, really necessary?" Why not directly divide the loads among different servers? For example, transaction processing (TP) and analytical processing (AP) loads can be routed to different replicas on different servers, and different tenants can be deployed on different servers to implement physical isolation. As I see it, that is truly a simple and convenient solution. However, it has many limitations, and cost is the biggest concern. Assume that a game company has two tenants A and B, where tenant A processes services outside China and tenant B processes services inside China. The load peaks of one tenant coincide with the load troughs of the other and vice versa due to the time zone difference. Although each of them can exclusively occupy separate server resources, half of the resources are wasted.</p>
<p>For disk I/O resources, loads whose data is tightly coupled cannot be simply divided among different servers. For example, operations such as backup, migration, and reorganization in a database strongly depend on intensive data reads and writes. Without disk I/O isolation, these tasks can affect the service throughput and response time. Actually, it is difficult to divide TP and AP loads on different servers as desired. TP and AP loads cannot be clearly demarcated sometimes. Even loads of the same type, such as TP loads, have different priorities based on services. What can we do in this situation?</p>
<p>Disk I/O is a type of flexible resources, and loads can contend for disk I/O resources. Resources such as memory are rigid and described as scalars. A memory block occupied by Load A cannot be simultaneously allocated to Load B. Disk I/O is a type of flexible resources and described as the processing capability within a unit time. Loads A and B can read data from and write data to the disk at the same time. Rigid resources can be clearly isolated like cutting a cake. However, for flexible resources, contention between loads must be considered. Assume that you have two fields A and B irrigated by the same river. When the water that flows to Field A is reduced, the water that flows to Field B can be increased.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ii-what-is-good-disk-io-isolation">II. What is good disk I/O isolation?<a href="https://oceanbase.github.io/zh-Hans/blog/io-isolation#ii-what-is-good-disk-io-isolation" class="hash-link" aria-label="II. What is good disk I/O isolation?的直接链接" title="II. What is good disk I/O isolation?的直接链接">​</a></h2>
<p>To answer this question, we need to figure out customers' expectations of disk I/O isolation, which vary from one customer to another.</p>
<ul>
<li>Some customers want to implement exclusive resource usage through I/O isolation, such as an exclusive disk bandwidth of 200 Mbit/s.</li>
<li>Some customers want to limit the resource usage of some loads to specified thresholds through disk I/O isolation.</li>
<li>Others only want to allocate resources by weight when resources are insufficient. Resource isolation is not a concern when resources are sufficient.</li>
</ul>
<p>In the technical field of resource isolation, the preceding three types of requirements correspond to three isolation semantics: reservation, limitation, and proportion. They are also what disk I/O isolation is supposed to implement in OceanBase Database.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="iii-how-do-we-configure-disk-io-isolation-in-oceanbase-database">III. How do we configure disk I/O isolation in OceanBase Database?<a href="https://oceanbase.github.io/zh-Hans/blog/io-isolation#iii-how-do-we-configure-disk-io-isolation-in-oceanbase-database" class="hash-link" aria-label="III. How do we configure disk I/O isolation in OceanBase Database?的直接链接" title="III. How do we configure disk I/O isolation in OceanBase Database?的直接链接">​</a></h2>
<p>OceanBase Database allows you to configure disk I/O isolation between tenants or between loads in a tenant.</p>
<p>The former is quite easy. For input/output operations per second (IOPS), you can specify the <code>MIN_IOPS</code>, <code>MAX_IOPS</code>, and <code>IOPS_WEIGHT</code> parameters for a tenant in the unit config to meet the foregoing three types of isolation requirements. Here is an example.</p>
<div class="language-SQL language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">alter</span><span class="token plain"> resource unit </span><span class="token keyword" style="color:#00009f">set</span><span class="token plain"> tp_unit min_iops</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">20000</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> max_iops</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">40000</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> iops_weight</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">500</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Then, how to configure disk I/O isolation between loads in a tenant? OceanBase Database extends the ResourceManager package of Oracle to adapt to the use habits of users.</p>
<p>The following example shows you how to use ResourceManager to isolate the disk I/O resources for TP and AP loads.</p>
<ul>
<li>First, create a resource management plan named <code>htap_plan</code> and two resource consumer groups named <code>tp_group</code> and <code>ap_group</code>.</li>
<li>Second, bind <code>tp_group</code> and <code>ap_group</code> to <code>htap_plan</code>. Allocate more resources to <code>tp_group</code> and fewer resources to <code>ap_group</code>. The value of each of <code>MIN_IOPS</code>, <code>MAX_IOPS</code>, and <code>WEIGHT_IOPS</code> is a resource percentage of the unit config of the tenant.</li>
<li>Third, set the mapping rule between the loads and resource consumer groups. In this example, loads are mapped to consumer groups by username. For example, all loads of the <code>trade</code> user use resources of the <code>tp_group</code> resource consumer group.</li>
</ul>
<div class="language-SQL language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Create a resource management plan</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">BEGIN</span><span class="token plain"> DBMS_RESOURCE_MANAGER</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">CREATE_PLAN</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">PLAN</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'htap_plan'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">END</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Create resource consumer groups</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">BEGIN</span><span class="token plain"> DBMS_RESOURCE_MANAGER</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">CREATE_CONSUMER_GROUP</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  CONSUMER_GROUP </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'tp_group'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">COMMENT</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'resource group for oltp applications'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">END</span><span class="token punctuation" style="color:#393A34">;</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">BEGIN</span><span class="token plain"> DBMS_RESOURCE_MANAGER</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">CREATE_CONSUMER_GROUP</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  CONSUMER_GROUP </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'ap_group'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">COMMENT</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'resource group for olap applications'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">END</span><span class="token punctuation" style="color:#393A34">;</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Allocate resources</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">BEGIN</span><span class="token plain"> DBMS_RESOURCE_MANAGER</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">CREATE_PLAN_DIRECTIVE</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">PLAN</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'htap_plan'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  GROUP_OR_SUBPLAN </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'tp_group'</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">COMMENT</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'more resource for tp_group'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MGMT_P1 </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">100</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MIN_IOPS </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">60</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MIX_IOPS </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">100</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  WEIGHT_IOPS </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">100</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">END</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">BEGIN</span><span class="token plain"> DBMS_RESOURCE_MANAGER</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">CREATE_PLAN_DIRECTIVE</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">PLAN</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'htap_plan'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  GROUP_OR_SUBPLAN </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'ap_group'</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">COMMENT</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'less resource for ap_group'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MGMT_P1 </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">20</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MIN_IOPS </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MIX_IOPS </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">80</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  WEIGHT_IOPS </span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">20</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">END</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Map loads to resource consumer groups</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">BEGIN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">DBMS_RESOURCE_MANAGER</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">SET_CONSUMER_GROUP_MAPPING</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">'FUNCTION'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'CAOPACTION_HIGH'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'background_group'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">END</span><span class="token punctuation" style="color:#393A34">;</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Map specific SQL statements to a resource consumer group</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">BEGIN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  DBMS_RESOURCE_MANAGER</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">SET_CONSUMER_GROUP_MAPPING</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">'COLUMN'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'test.t1.c1 = 3'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'big1_group'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">END</span><span class="token punctuation" style="color:#393A34">;</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">BEGIN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  DBMS_RESOURCE_MANAGER</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">SET_CONSUMER_GROUP_MAPPING</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">'USER'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'trade'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'tp_group'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">END</span><span class="token punctuation" style="color:#393A34">;</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">BEGIN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  DBMS_RESOURCE_MANAGER</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">SET_CONSUMER_GROUP_MAPPING</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">'USER'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'analysis'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'ap_group'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">END</span><span class="token punctuation" style="color:#393A34">;</span><span class="token operator" style="color:#393A34">/</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Mapping rules for resource consumer groups also support function names and column names. In function name-based mapping, the resource usage of backend tasks can be controlled by using ResourceManager. In column name-based mapping, resource isolation can be refined to the SQL statement level. Here is an example.</p>
<div class="language-SQL language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Map backend tasks to a resource consumer group</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">BEGIN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  DBMS_RESOURCE_MANAGER</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">SET_CONSUMER_GROUP_MAPPING</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">'FUNCTION'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'CAOPACTION_HIGH'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'background_group'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">END</span><span class="token punctuation" style="color:#393A34">;</span><span class="token operator" style="color:#393A34">/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Map specific SQL statements to a resource consumer group</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">BEGIN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  DBMS_RESOURCE_MANAGER</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">SET_CONSUMER_GROUP_MAPPING</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">'COLUMN'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'test.t1.c1 = 3'</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">'big1_group'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">END</span><span class="token punctuation" style="color:#393A34">;</span><span class="token operator" style="color:#393A34">/</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="iv-disk-io-isolation-performance-testing-of-oceanbase-database-v4x">IV. Disk I/O isolation performance testing of OceanBase Database V4.x<a href="https://oceanbase.github.io/zh-Hans/blog/io-isolation#iv-disk-io-isolation-performance-testing-of-oceanbase-database-v4x" class="hash-link" aria-label="IV. Disk I/O isolation performance testing of OceanBase Database V4.x的直接链接" title="IV. Disk I/O isolation performance testing of OceanBase Database V4.x的直接链接">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="verify-the-disk-io-isolation-capability"><strong>Verify the disk I/O isolation capability</strong><a href="https://oceanbase.github.io/zh-Hans/blog/io-isolation#verify-the-disk-io-isolation-capability" class="hash-link" aria-label="verify-the-disk-io-isolation-capability的直接链接" title="verify-the-disk-io-isolation-capability的直接链接">​</a></h3>
<p>Create four tenants for a simulation test. Each tenant starts 64 threads to send I/O requests that perform 16 KB random reads. The loads of tenants 1, 2, and 4 last for 20 seconds, and the load of tenant 3 begins from the 10th second and lasts for 10 seconds. In this test, the maximum IOPS is about 60,000. Without limitations, any tenant can use up the disk resources.</p>
<p>We first verified the disk I/O isolation between tenants. Table 1 describes the resource configurations of the tenants and Figure 1 shows the test results of the tenants.</p>
<ul>
<li>When the disk resources are used up, the newly joined tenant 3 still has an IOPS of 10,000, which is reserved by using the <code>MIN_IOPS</code> parameter.</li>
<li>The IOPS of tenant 4 does not exceed 5,000 because its maximum IOPS is limited by using the <code>MAX_IOPS</code> parameter.</li>
<li>Regardless of the load changes, the IOPS ratio between tenant 1 and tenant 2 is always 2:1 as defined.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1683280561708.png" alt="1683280561" class="img_ev3q"><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1683280562322.png" alt="1683280562" class="img_ev3q"></p>
<p>Then, we verified the disk I/O isolation between loads in a tenant. Set four types of loads in tenant 2. Table 2 describes the resource configurations of the loads. Figure 2 shows the test results.</p>
<ul>
<li>The IOPS of Load B remains about 2,000, even if its weight is 0. This is because 97% of the minimum IOPS resources of the tenant is reserved for Load B by using the <code>MIN_PERCENT</code> parameter.</li>
<li>The IOPS of Load A remains about 1,000. This is because the <code>MAX_PERCENT</code> parameter is set to <code>1</code> for Load A. In this way, Load A can use only 1% of the maximum resources of the tenant.</li>
<li>The IOPS ratio between Load C and Load D is always 2:1, which conforms to their weight ratio of 50:25.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1683280606307.png" alt="1683280606" class="img_ev3q"><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-05/1683280606875.png" alt="1683280606" class="img_ev3q"></p>
<p>The preceding tests show that OceanBase Database supports disk I/O isolation between tenants and between loads in a tenant, and meets the three isolation semantics of reservation, limitation, and proportion.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="adjust-disk-io-isolation-configurations-in-real-time"><strong>Adjust disk I/O isolation configurations in real time</strong><a href="https://oceanbase.github.io/zh-Hans/blog/io-isolation#adjust-disk-io-isolation-configurations-in-real-time" class="hash-link" aria-label="adjust-disk-io-isolation-configurations-in-real-time的直接链接" title="adjust-disk-io-isolation-configurations-in-real-time的直接链接">​</a></h3>
<p>Some may have noticed that the disk I/O isolation configurations remain unchanged in the preceding tests. Does OceanBase Database support real-time adjustment of the isolation configurations? The answer is "Yes". The following test will prove it.</p>
<p>Prepare a large table and perform a full-table scan with a parallel query. During the scan, change the value of the <code>MAX_IOPS</code> parameter for the tenant repeatedly as the administrator. The video shows that the IOPS monitored by the operating system changes constantly.</p>
<p>You may have noticed that the IOPS monitored by the operating system is always lower than the value specified by the administrator. This is because OceanBase Database normalizes the overhead of I/O requests.</p>
<p>For example, the overhead of 64 KB random reads is different from that of 4 KB random reads. The baseline IOPS overhead specified in the unit config of the tenant is 16 KB random reads. However, the actual size of I/O requests is about 20 KB. After overhead calculation, the IOPS monitored by the operating system is different. For more information, see the related code of ob_io_manager.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="v-afterword">V. Afterword<a href="https://oceanbase.github.io/zh-Hans/blog/io-isolation#v-afterword" class="hash-link" aria-label="V. Afterword的直接链接" title="V. Afterword的直接链接">​</a></h2>
<p>The resource isolation capability of OceanBase Database V4.x allows you to flexibly control the resources allocated to different loads. We will make every effort to improve this capability to address user concerns, such as the unit config and number of resource units of the tenant. OceanBase Database is devoted to providing a better resource isolation capability and user experience. When the business traffic changes, OceanBase Database can automatically allocate the required resources, like a standalone database with unlimited resources. It must be a long haul to reach that goal, but we are resolved and ready to push through all the challenges.</p>
<p>Finally, feel free to share with us your comments on disk I/O isolation.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OceanBase 4.0 interpretation: Reduce the threshold of distributed database use, talk about our thinking on small specifications]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/miniaturization</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/miniaturization</guid>
            <pubDate>Mon, 03 Jun 2024 13:23:07 GMT</pubDate>
            <description><![CDATA[Author | Zhao Yuzhong, a senior technical expert of OceanBase. He joined Alipay in 2010 to help with the R&D of the distributed transaction framework, and has engaged in the R&D of storage engines as an OceanBaser since 2013.]]></description>
            <content:encoded><![CDATA[<blockquote>
<p><strong>Author | Zhao Yuzhong, a senior technical expert of OceanBase.</strong> He joined Alipay in 2010 to help with the R&amp;D of the distributed transaction framework, and has engaged in the R&amp;D of storage engines as an OceanBaser since 2013.</p>
</blockquote>
<p>With the emergence of more scenarios and the growth of data volume in recent years, distributed databases have rapidly spread across a variety of sectors, providing great solutions for data-intensive and high-concurrency applications with their technical capabilities such as data consistency, high availability, and elastic scaling. A distributed database is often deployed on multiple servers to ensure high availability and performance. Therefore, to handle small-scale simple scenarios in the early days of their business, users tend to deploy a centralized database that costs less and exhibits higher performance if small specifications. The problem is, sooner or later, the centralized database will be bottlenecked as the business size grows, and adjustments or restructuring of the database architecture by then can be extremely challenging and costly.</p>
<p>OceanBase Database Community Edition V4.0 was released at the Apsara Conference 2022. It is the industry's first MySQL-compatible integrated database that supports both standalone and distributed deployment modes. This version provides many much-expected capabilities, such as enhanced online analytical processing (OLAP) capabilities. Featuring an integrated architecture, it can be deployed in standalone mode with a few clicks and can stably run in a production system with small hardware specifications, such as four CPU cores and 16 GB of memory (4C16G). This reduces the deployment costs and improves its usability. We hope that the dual technical advantages of the integrated architecture can bring perpetual benefits for database users.</p>
<p>According to their feedback, users are highly interested in the integrated architecture of OceanBase Database Community Edition V4.0 and its support for small-specification deployment. We believe that small-specification deployment is not only about providing all necessary features in standalone mode. More importantly, it delivers higher performance with the same hardware configuration. In this article, we will, from the following three perspectives, share our thoughts on small-sized distributed databases, and our innovative ideas and solutions about the integrated architecture that supports both standalone and distributed deployment:</p>
<ul>
<li>Reasons for choosing a small-specification distributed database</li>
<li>Key techniques for small-specification deployment</li>
<li>Performance of OceanBase databases with small specifications</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="i-why-a-distributed-database-with-small-specifications">I. Why a distributed database with small specifications?<a href="https://oceanbase.github.io/zh-Hans/blog/miniaturization#i-why-a-distributed-database-with-small-specifications" class="hash-link" aria-label="I. Why a distributed database with small specifications?的直接链接" title="I. Why a distributed database with small specifications?的直接链接">​</a></h2>
<p>Over the past decade or so since its founding in 2010, OceanBase has broken the world records in TPC-C and TPC-H tests, empowered the Double 11 shopping festival every year, and ensured that every transaction was safely and efficiently executed. Pushing through all kinds of challenges, OceanBase Database, as a fully self-developed native distributed database, has proved its scalability and stability. From OceanBase Database V2.2 that topped the TPC-C ranking for the first time with 203 Elastic Compute Service (ECS) servers, to a later version that took the crown again with 1,554 ECS servers, the performance of OceanBase Database rises linearly with the number of servers.</p>
<p>On the other hand, as OceanBase Database caught the attention from industries other than the financial sector, we realized that not all users were faced by the amount of data comparable to the Double 11. In fact, standalone databases are just enough to tick all the boxes of many users in the early days of their business, when the data volume is rather small. Therefore, it is a great help to provide minimal database specifications for users to begin with. In this way, users are able to break in at very low costs. Also, with the great scalability of OceanBase Database, users can flexibly scale out their database systems later to take care of the increasing data volume and performance requirements.</p>
<p><strong>▋ From small to large: Basic database requirements in a business that grows</strong></p>
<p>The latest OceanBase Database V4.0 supports a minimum deployment specification of 4C8G. What does 4C8G mean? It's just a typical configuration of a nice laptop. In other words, OceanBase Database V4.0 can be deployed and stably run on a personal computer.</p>
<p>As user business grows, OceanBase Database V4.0 can be scaled out to support changing needs over the entire lifecycle of the business. OceanBase Database V4.0 helps users find better solutions to cost reduction, efficiency improvement, and business innovation.</p>
<ul>
<li>In its early days, user business handles small amount of data and has few requirements on disaster recovery. The user can deploy and run OceanBase Database V4.0 on a single server and perform cold backup regularly to protect its data system from possible disasters.</li>
<li>As its business grows, the user can vertically scale up the specifications of the existing server. To meet its requirements on disaster recovery, the user can add another server to build a primary/standby architecture, which provides the online disaster recovery capability. (Manual intervention is still required during disaster recovery due to the limits of the primary/standby architecture.)</li>
<li>When its business expands to certain size and data becomes more important, the user can simply upgrade to the three-replica architecture, which ensures high availability with three servers and supports automatic disaster recovery. When a server fails, the three-replica architecture of OceanBase Database V4.0 guarantees business recovery in 8s with zero data loss. In other words, the recovery time objective (RTO) is less than 8s and the recovery point objective (RPO) is 0.</li>
<li>When user business experiences even greater growth and each server has been upgraded to the highest configurations, the user has to deal with this "happy trouble" as Taobao and Alipay did. In this case, the transparent distributed scalability of OceanBase Database allows the user to scale its cluster out from 3 to 6, 9 or even thousands of servers.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2024/06/04/gLRmXzs5VMCBT1A.webp" alt="image.png" class="img_ev3q"></p>
<p>Figure 1 Deployment evolution: OceanBase Database vs conventional databases</p>
<p><strong>▋ Smooth transitions that ensure linear performance improvement</strong></p>
<p>The integrated architecture of OceanBase Database supports smooth transition from standalone to distributed multi-cluster deployment mode, keeping the performance improvement at a linear speed.</p>
<p>Thanks to the good vertical scalability of OceanBase Database, the configuration upgrade of the server in standalone mode usually achieves linear performance improvement. When a user scales a distributed cluster from 3 to 6 servers, for example, distributed transactions are often introduced, which, in most cases, results in performance loss. However, OceanBase Database reduces the probability of distributed transactions through a variety of mechanisms, such as the TableGroup mechanism that binds multiple tables together, and the well-designed load balancing strategies.</p>
<p>The good distributed scalability of OceanBase Database also helps maintain linear performance improvement as the number of servers increases. For example, in the TPC-C test, which involves about 10% of distributed transactions, the performance improvement of OceanBase Database remained linear as more nodes were added to the cluster.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/f419efdd-d5b7-4a33-9afe-c7e0107c71f5/image/2023-01-11/d6979385-99d3-4ed9-a378-d2a8962e3342.png" alt="" class="img_ev3q"></p>
<p>Figure 2 Performance of OceanBase Database with different number of nodes in the TPC-C test</p>
<p>More importantly, all operations performed in scaling from a standalone OceanBase database to an OceanBase cluster of thousands of nodes are transparent to the business. Users do not need to modify the code of their upper-level business applications, or manually migrate their operation data. If you use OceanBase Cloud, you can perform backup, scaling, and O&amp;M operations all on the same platform, which is quite convenient.</p>
<p>From the first day of the development of OceanBase Database V4.0, we have been thinking about how to run a distributed database on small-specification hardware, yet delivering high performance, so that users benefit from cost-effective high availability in their respective scenarios. OceanBase Database V4.0 not only provides all necessary features in standalone mode, and also delivers higher performance with the same hardware configuration.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ii-key-techniques-for-small-specification-deployment">II. Key techniques for small-specification deployment<a href="https://oceanbase.github.io/zh-Hans/blog/miniaturization#ii-key-techniques-for-small-specification-deployment" class="hash-link" aria-label="II. Key techniques for small-specification deployment的直接链接" title="II. Key techniques for small-specification deployment的直接链接">​</a></h2>
<p>In the fundamental software sector, it is very hard to make a database system "large" because the system will be increasingly vulnerable to failures as more nodes are added to it. In our second TPC-C test, for example, we built an OceanBase cluster of 1,554 Elastic Compute Service (ECS) servers. In such a cluster, the frequency of a single-server failure is about once a day or every other day. The point is we have to make the product sufficiently stable and highly available to keep such a jumbo-sized cluster up and running.</p>
<p>It is equally hard to make a database system "small" because it requires getting down to every detail, much like using a microscope to arrange the usage of every slice of resource. Not only that, some proper designs or configurations in a large system may be totally unacceptable in a smaller one. What's more challenging is that we must make the system suitable for both large and small hardware specifications. This requires us to weigh up between large and small specifications when designing the database system, so as to minimize the additional overhead of a distributed architecture while allowing the database system to make adaptive response according to hardware specifications in many scenarios. Now, let's talk about the technical solution of OceanBase Database V4.0 by taking the usage of CPU and memory, the two major challenges, as an example.</p>
<p><strong>▋ Reducing CPU utilization through dynamic control of log streams</strong></p>
<p>To build a small database, OceanBase Database V4.0 needs to control the CPU utilization in the first place. In versions earlier than V4.0, OceanBase Database would generate a Paxos log stream for each partition of a data table to ensure the data consistency among multiple replicas based on the Paxos protocol. This is a very flexible design because Paxos groups are based on partitions, which means that partitions can be migrated between servers. However, this design puts heavy workload on the CPU because each Paxos log stream consumes overhead for leader selection, heartbeat, and log synchronization. Such additional overhead occupies a moderate percentage of the CPU resource if servers have large specifications, or the number of partitions is small, but causes an unbearable burden for small-specification servers.</p>
<p>How do we solve that issue in OceanBase Database V4.0? We go straight-forward and reduce the number of Paxos log streams. If we can reduce the number of Paxos log streams to the same as that of servers, the overhead for Paxos log streams is roughly equal to that for logs in a conventional database in the primary/standby mode.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/a248fea2-2640-432c-8c0e-68f394e9611b/image/2023-01-11/5830c47b-96eb-4303-9398-8f1b080610a4.png" alt="" class="img_ev3q"></p>
<p>Figure 3 Dynamic log streams of a cluster based on OceanBase Database V4.0</p>
<p>OceanBase Database V4.0 generates a Paxos log stream for multiple data table partitions and dynamically controls the log streams. As shown in the figure above, the database cluster consists of three zones, and each zone has two servers deployed. Assume that two resource units are configured for a tenant. In this case, two Paxos log streams are generated for the tenant, with one containing P1, P2, P3, and P4 partitions and the other containing P5 and P6 partitions.</p>
<ul>
<li>
<p>When the two servers are not load-balanced, the load balancing module of OceanBase Database migrates the partitions between the Paxos log streams.</p>
</li>
<li>
<p>To scale out the cluster, a user can split one Paxos log stream into multiple Paxos log streams and migrate them as a whole.</p>
</li>
<li>
<p>To scale in the cluster, the user can migrate multiple Paxos log streams and merge the streams.</p>
</li>
</ul>
<p>With dynamic log stream control, OceanBase Database V4.0 greatly reduces the CPU overhead of the distributed architecture and guarantees high availability and flexible scaling.</p>
<p><strong>▋ Achieving high concurrency with a small memory space through dynamic metadata loading</strong></p>
<p>The second challenge that OceanBase Database V4.0 needs to take in building a small database is to optimize memory usage. For the sake of performance, OceanBase Database of versions earlier than V4.0 stored some metadata in memory. The memory usage of this portion of metadata was not high if the total memory size was large, but unacceptable for a small-specification server. To support ultimate performance at small specifications, we have achieved dynamic loading of all metadata in OceanBase Database V4.0.</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2024/06/04/q4N9U5Vz2GAQHTM.png" alt="5D9183AF-7D02-40D3-8EAE-2173F2025126.png" class="img_ev3q"></p>
<p>Figure 4 SSTable hierarchical storage</p>
<p>As shown in the figure above, we store an SSTable in a hierarchical structure. To be specific, we store the microblocks of the SSTable in partitions and maintain only the handle of the partitions in memory. The requested data is dynamically loaded by using KVCache only when the partitions need to be accessed. In this way, OceanBase Database V4.0 is capable of processing highly concurrent requests for massive amount of data with a small memory size.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="iii-performance-of-databases-with-small-specifications">III. Performance of databases with small specifications<a href="https://oceanbase.github.io/zh-Hans/blog/miniaturization#iii-performance-of-databases-with-small-specifications" class="hash-link" aria-label="III. Performance of databases with small specifications的直接链接" title="III. Performance of databases with small specifications的直接链接">​</a></h2>
<p>To test the actual performance of OceanBase Database with small specifications, we deployed OceanBase Database Community Edition V4.0 in 1:1:1 mode based on three 4C16G servers and compared its performance with that of RDS for MySQL 8.0, which was also deployed on 4C16G servers. The comparison was performed by using Sysbench and the results show that OceanBase Database Community Edition V4.0 outperforms RDS for MySQL 8.0 in most data processing scenarios. In particular, under the same hardware specifications, OceanBase Database Community Edition V4.0 handles a throughput 1.9 times that of RDS for MySQL 8.0 in INSERT and UPDATE operations.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/22580914-c626-4968-b2ec-14b36e73a7dc/image/2023-01-11/74429483-aee7-4cb6-9cdf-0f5c43646d01.png" alt="" class="img_ev3q"></p>
<p>Figure 5 Throughput performance test results of OceanBase Database Community Edition V4.0 and RDS for MySQL 8.0 on Sysbench (4C16G)</p>
<p>We also compared the two at specifications of 8C32G, 16C64G, and 32C128G, which are most popular among users. As the server specifications increase, the performance gap widens between OceanBase Database Community Edition V4.0 and RDS for MySQL 8.0. At 32C128G specifications, OceanBase Database Community Edition V4.0 achieves a throughput 4.3 times that of RDS for MySQL 8.0 with 75% less response time.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/9a3fa66e-7716-48e3-bcf2-82735c4726c9/image/2023-01-11/b4324d1e-6bd1-48cb-9ccc-c100cca6aae6.png" alt="" class="img_ev3q"></p>
<p>Figure 6 Throughput performance test results of OceanBase Database Community Edition V4.0 and RDS for MySQL 8.0 on Sysbench</p>
<p><img decoding="async" loading="lazy" src="https://s2.loli.net/2024/06/04/vLwU6FWcVfNKy7M.png" alt="lQLPJyDF79Hqr-3NBTjNDHCwRri-CRupihMGSPeKlXwaAA_3184_1336.png" class="img_ev3q"></p>
<p>Table 1 Performance (throughput and response time) test results of OceanBase Database Community Edition V4.0 and RDS for MySQL 8.0 on Sysbench</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="afterword">Afterword<a href="https://oceanbase.github.io/zh-Hans/blog/miniaturization#afterword" class="hash-link" aria-label="Afterword的直接链接" title="Afterword的直接链接">​</a></h2>
<p>OceanBase Database has achieved ultimate performance in the TPC-C test with a massive cluster of more than a thousand servers, and ultimate resource usage in standalone performance tests at small specifications, such as 4C16G. What's behind those achievements is our unshakable faith in our mission to make data management and use easier. Streamlining services for customers with every effort is the motto of every OceanBase engineer. Growing fast, OceanBase Database is not yet perfect. We still have a lot to do to optimize its performance with higher specifications and save more resources in a database with even smaller specifications. OceanBase Database Community Edition V4.0 is now available and we are looking forward to working with all users to build a general database system that is easier to use.</p>
<p>Welcome to <a href="https://open.oceanbase.com/blog" target="_blank" rel="noopener noreferrer">OceanBase Community</a>. We will keep generating useful content, and pursue excellence together with tens of millions of developers.</p>
<p>🔍Join us on DingTalk (Group ID: 33254054), or scan the QR code below to contact OceanBase Technical Support. We are ready to answer all questions about our products.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/f4d95b17-3494-4004-8295-09ab4e649b68/image/2022-08-29/00ff7894-c260-446d-939d-f98aa6648760.png" alt="" class="img_ev3q"></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OceanBase 4.3 - Milestone release for real-time AP analysis]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/ob-430</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/ob-430</guid>
            <pubDate>Mon, 03 Jun 2024 13:23:07 GMT</pubDate>
            <description><![CDATA[1713848983]]></description>
            <content:encoded><![CDATA[<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713848982839.png" alt="1713848983" class="img_ev3q"></p>
<p>In early 2023, OceanBase Database V4.1 was released. It is the first milestone version of the V4.x series and supports an integrated architecture for standalone and distributed modes. Such integrated architecture reduces the recovery time objective (RTO), a database reliability indicator, to less than 8 seconds, ensuring rapid database recovery from an unexpected failure. Unlike the V3.x series, the new version does not limit the number of partitions, providing higher capacity for processing large transactions. Core features such as the arbitration replica are supported to cut costs.</p>
<p>In September 2023, OceanBase Database V4.2.1 was released. As the first long-term supported (LTS) version of the V4.x series, it augments all core features of the V3.x series, and demonstrates improved performance in many aspects such as stability, scalability, support for small specifications, and ease of diagnostics. Six months after its release, hundreds of customers have deployed this LTS version in their production environments for stable operations.</p>
<p>To meet higher expectations on ease-of-use and capabilities of tackling miscellaneous workloads, we have released OceanBase Database V4.3.0, which is rigorously implemented on top of open design after thorough research.</p>
<p>OceanBase Database V4.3.0 sets a significant milestone on our roadmap to achieve real-time analytical processing (AP). This version provides a columnar engine based on the log-structured merge-tree (LSM-tree) architecture, which implements hybrid columnar and row-based storage. The database also introduces a new vectorized engine based on column data format descriptions and a cost model based on columnar storage. This way, wide tables can be effectively processed and the query performance in AP scenarios is significantly improved without affecting transactional processing (TP) business scenarios. Overall, the new OceanBase Database version is well-suited for mixed workload scenarios involving complex analytics, real-time reporting, real-time data warehousing, and online transactions. The materialized view feature is provided. Query results are pre-calculated and stored in materialized views to improve real-time query performance, and support rapid report generation and data analysis. The kernel in the new version also extends online DDL and adds support for tenant cloning. It has optimized performance and system resource usage, and provides better system usability. In a test with the same hardware configurations, the performance of OceanBase Database V4.3.0 in wide-table queries is comparable with mainstream columnstore databases in the industry.</p>
<p>Now, let's take a closer look at key updates of OceanBase Database V4.3.0:</p>
<p>○ TP and AP integration</p>
<p>○ High-performance kernel</p>
<p>○ Higher computing performance</p>
<p>○ Ease-of-use enhancements</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="i-tp-and-ap-integration"><strong>I. TP and AP integration</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#i-tp-and-ap-integration" class="hash-link" aria-label="i-tp-and-ap-integration的直接链接" title="i-tp-and-ap-integration的直接链接">​</a></h2>
<p>In addition to features of V4.2, such as highly concurrent real-time row updates, and point queries of the primary key indexes, OceanBase Database V4.3.0 introduces more AP services. Its scalable distributed architecture also supports high availability, strong consistency, and geo-disaster recovery. The new version provides a columnar engine and enhances vectorized execution, parallel computing, and distributed plan optimization. This way, the database supports both TP and AP business.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-integrated-columnar-and-row-based-storage"><strong>(1) Integrated columnar and row-based storage</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#1-integrated-columnar-and-row-based-storage" class="hash-link" aria-label="1-integrated-columnar-and-row-based-storage的直接链接" title="1-integrated-columnar-and-row-based-storage的直接链接">​</a></h3>
<p>Columnar storage is one of the key capabilities of AP databases in complex large-scale data analysis and ad hoc queries of massive data. Columnar storage is a way to organize data files. Different from row-based storage, columnar storage physically arranges data in a table by column. When data is stored by column, the system can scan only the columns involved in the query and calculation, instead of scanning the entire row. This way, the consumption of resources such as I/O and memory is reduced and the calculation is accelerated. Moreover, columnar storage naturally provides better data compression conditions, making it easier to achieve higher compression ratios, thereby reducing storage space and network transmission bandwidth.</p>
<p>However, columnar engines generally assume limited random updates and attempt to ensure that data in columnar storage is static. When a large amount of data is updated randomly, system performance will inevitably degrade. The LSM-tree architecture of OceanBase Database can process baseline data and incremental data separately, and therefore can solve the performance issue. Therefore, OceanBase Database V4.3.0 supports the columnar engine based on the current architecture, implementing integrated columnar and row-based data storage on an OBServer node with only one set of code and one architecture, and ensuring the performance of both TP and AP queries.</p>
<p>To help users with AP requirements smoothly use the new version, OceanBase Database has adapted and optimized several modules, including the optimizer, executor, DDL, and transaction processing, for the columnar engine. These optimizations introduce a new cost model and vectorized engine based on columnar storage, enhancements to the query pushdown feature, and features like skip index, a new column-based encoding algorithm, and adaptive compactions.</p>
<p>To make AP queries easy, we recommend that you run the following command in a MySQL or Oracle tenant of OceanBase Database to create a columnstore table by default:</p>
<div class="language-SQL language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">alter</span><span class="token plain"> system </span><span class="token keyword" style="color:#00009f">set</span><span class="token plain"> default_table_store_format </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"column"</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>You can flexibly create a business table as a rowstore table, columnstore table, or hybrid rowstore-columnstore table based on the load type. You can also create a columnstore index for a rowstore table.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713849285226.png" alt="1713849286" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713849296536.png" alt="1713849297" class="img_ev3q"></p>
<p>The optimizer determines, based on estimated costs, whether to scan a hybrid rowstore-columnstore table by row or by column.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713849310310.png" alt="1713849311" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-new-vectorized-engine"><strong>(2) New vectorized engine</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#2-new-vectorized-engine" class="hash-link" aria-label="2-new-vectorized-engine的直接链接" title="2-new-vectorized-engine的直接链接">​</a></h3>
<p>Earlier versions of OceanBase Database have implemented a vectorized engine based on uniform data format descriptions, offering performance significantly better than that of non-vectorized engines. However, the engine still has some performance deficiencies in deep AP scenarios. OceanBase Database V4.3.0 implements the vectorized engine 2.0, which is based on column data format descriptions, avoiding the memory usage, serialization, and read/write access overhead caused by ObDatum maintenance. Based on the reconstruction of data format descriptions, the new vectorized engine also reimplements more than 10 commonly used operators such as HashJoin, AGGR, HashGroupBy, and Exchange (DTL Shuffle), as well as over 20 MySQL expressions including relational operations, logical operations, and arithmetic operations. Subsequent V4.3.x versions will further improve and implement other operators and expressions based on the new vectorized engine to achieve better performance.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-materialized-views"><strong>(3) Materialized views</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#3-materialized-views" class="hash-link" aria-label="3-materialized-views的直接链接" title="3-materialized-views的直接链接">​</a></h3>
<p>OceanBase Database V4.3.0 introduces materialized views. Materialized views are a key feature for AP business scenarios. By precomputing and storing the query results of views, real-time calculations are reduced to improve query performance and simplify complex query logic. Materialized views are commonly used for rapid report generation and data analysis scenarios.</p>
<p>Materialized views need to store query result sets to optimize the query performance. Due to data dependency between a materialized view and its base tables, data in the materialized view must be refreshed accordingly when data in any base tables changes. Therefore, the materialized view refresh mechanism is also introduced in the new version, including complete refresh and incremental refresh strategies. Complete refresh is a relatively direct method. Each time a refresh operation is performed, the system re-executes the corresponding query statement of a materialized view to recalculate and overwrite the original result set. This method is applicable to scenarios with a small amount of data. Incremental refresh, by contrast, only deals with data that has been changed since the last refresh. To achieve accurate incremental refresh, OceanBase Database implements a materialized view log feature that is similar to Oracle Materialized View Log (MLOG). The feature tracks incremental data updates in base tables and records the updates in logs. This ensures that materialized views can be refreshed incrementally in a short period. Incremental refresh is particularly useful in business scenarios with large data volume and frequent data changes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ii-high-performance-kernel"><strong>II. High-performance kernel</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#ii-high-performance-kernel" class="hash-link" aria-label="ii-high-performance-kernel的直接链接" title="ii-high-performance-kernel的直接链接">​</a></h2>
<p>The kernel in the new version has enhanced the cost model, added support for tenant cloning, extended online DDL, and Amazon Simple Storage Service (S3) as the backup and restore media, restructured the session management module, and optimized log stream state machine and system resource usage, to improve database performance and stability in handling key business workloads.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-enhanced-row-estimation-system"><strong>(1) Enhanced row estimation system</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#1-enhanced-row-estimation-system" class="hash-link" aria-label="1-enhanced-row-estimation-system的直接链接" title="1-enhanced-row-estimation-system的直接链接">​</a></h3>
<p>As the OceanBase Database version evolves, more cost estimation methods are available for the optimizer. For row estimation of each operator, a variety of algorithms, such as row estimation based on the storage layer, row estimation based on statistics, dynamic sampling, and default statistics, are supported. However, there are no clear strategies and complete control methods for using row estimation. OceanBase Database V4.3.0 reconstructs the row estimation system. Specifically, it prioritizes row estimation strategies based on scenarios and provides methods such as hints and system variables for you to manually intervene the selection of a row estimation strategy. This version also enhances the predicate selectivity and number of distinct values (NDV) calculation framework to improve the accuracy of cost estimation by the optimizer.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-enhanced-statistics"><strong>(2) Enhanced statistics</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#2-enhanced-statistics" class="hash-link" aria-label="2-enhanced-statistics的直接链接" title="2-enhanced-statistics的直接链接">​</a></h3>
<p>OceanBase Database V4.3.0 improves the statistics feature, statistics collection performance, and the compatibility and usability of statistics. Specifically, this version reconstructs the offline statistics collection process to improve the collection efficiency, optimizes the statistics collection strategies to automatically collect information about index histograms by default and collect statistics in a deductive manner, and ensures transaction consistency for online statistics collection. It is compatible with the <code>DBMS_STATS.COPY_TABLE_STATS</code> procedure of Oracle for statistics copying, and is also compatible with the <code>ANALYZE TABLE</code> statement of MySQL. It provides a command to cancel statistics collection, enriches the monitoring on the statistics collection progress, and enhances maintenance usability. It also supports the parallel deletion of statistics.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-adaptive-cost-model"><strong>(3) Adaptive cost model</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#3-adaptive-cost-model" class="hash-link" aria-label="3-adaptive-cost-model的直接链接" title="3-adaptive-cost-model的直接链接">​</a></h3>
<p>In earlier versions of OceanBase Database, the cost model uses constant parameters measured by internal machines to represent hardware system statistics, and describes the execution overhead of each operator by using a series of formulas and constant parameters. However, in real business scenarios, different hardware environments may have different CPU clock frequencies, sequential or random read speeds, and NIC bandwidths, thereby resulting in cost estimation deviations. The optimizer cannot generate optimal plans in different business environments because of these deviations. The new version implements the cost model in an optimized way to support the <code>DBMS_STATS</code> package for collecting or setting system statistics coefficients, thus adapting the cost model to hardware. It also provides the <code>DBA_OB_AUX_STATISTICS</code> view to display the system statistics coefficients of the current tenant.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-fixed-session-variables-for-function-based-indexes"><strong>(4) Fixed session variables for function-based indexes</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#4-fixed-session-variables-for-function-based-indexes" class="hash-link" aria-label="4-fixed-session-variables-for-function-based-indexes的直接链接" title="4-fixed-session-variables-for-function-based-indexes的直接链接">​</a></h3>
<p>When a function-based index is created on a table, a hidden virtual generated column is added to the table and defined as the index key of the function-based index. The values of the virtual generated column are stored in the index table. The results of some built-in system functions are affected by session variables. The calculation result of a function varies based on the values of session variables, even if the input arguments are the same. When a function-based index or generated column is created in this version, session variables on which the function-based index or generated column depends are fixed in the column schema to improve stability. When values of the indexed column or generated column are calculated, fixed session variable values are used. Therefore, the calculation result is not affected by variable values in the current session. OceanBase Database V4.3.0 supports fixed values of the system variables <code>timezone_info</code>, <code>nls_format</code>, <code>nls_collation</code>, and <code>sql_mode</code>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5-online-ddl-expansion-in-mysql-mode"><strong>(5) Online DDL expansion in MySQL mode</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#5-online-ddl-expansion-in-mysql-mode" class="hash-link" aria-label="5-online-ddl-expansion-in-mysql-mode的直接链接" title="5-online-ddl-expansion-in-mysql-mode的直接链接">​</a></h3>
<p>OceanBase Database V4.3.0 supports more online DDL scenarios for column type changes, including:</p>
<p>○ Conversion of integer types: Online DDL operations, instead of offline DDL operations, are performed to change the data type of a primary key column, index column, generated column, column on which a generated column depends, or column with a <code>UNIQUE</code> or <code>CHECK</code> constraint to an integer type with a larger value range.</p>
<p>○ Conversion of the DECIMAL data type: For columns that support the DECIMAL data type, online DDL operations are performed to increase the precision within any of the [1,9], [10,18], [19,38], and [39,76] ranges without changing the scale.</p>
<p>○ Conversion of the BIT or CHAR data type: For columns that support the BIT or CHAR data type, online DDL operations are performed to increase the width.</p>
<p>○ Conversion of the VARCHAR or VARBINARY data type: For columns that support the VARCHAR or VARBINARY data type, online DDL operations are performed to increase the width.</p>
<p>○ Conversion of the LOB data type: To change the data type of a column that supports LOB data types to a LOB data type with a larger value range, offline DDL operations are performed for columns of the TINYTEXT or TINYBLOB data type, and online DDL operations are performed for columns of other data types.</p>
<p>○ Conversion between the TINYTEXT and VARCHAR data types: For columns that support the TINYTEXT data type, online DDL operations are performed to change the VARCHAR(x) data type to the TINYTEXT data type if <code>x &lt;= 255</code>, and offline DDL operations are performed if otherwise. For columns that support the VARCHAR data type, online DDL operations are performed to change the TINYTEXT data type to the VARCHAR(x) data type if <code>x &gt;= 255</code>, and offline DDL operations are performed if otherwise.</p>
<p>○ Conversion between the TINYBLOB and VARBINARY data types: For columns that support the TINYBLOB data type, online DDL operations are performed to change the VARBINARY(x) data type to the TINYBLOB data type if <code>x &lt;= 255</code>, and offline DDL operations are performed if otherwise. For columns that support the VARBINARY data type, online DDL operations are performed to change the TINYBLOB data type to the VARBINARY(x) data type if <code>x &gt;= 255</code>, and offline DDL operations are performed if otherwise.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="6-globally-unique-client-session-id"><strong>(6) Globally unique client session ID</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#6-globally-unique-client-session-id" class="hash-link" aria-label="6-globally-unique-client-session-id的直接链接" title="6-globally-unique-client-session-id的直接链接">​</a></h3>
<p>Prior to OceanBase Database V4.3.0 and OceanBase Database Proxy (ODP) V4.2.3, when the client executes <code>SHOW PROCESSLIST</code> through ODP, the client session ID in ODP is returned. However, when the client queries the session ID by using an expression such as <code>connection_id</code> or from a system view, the session ID on the server is returned. A client session ID corresponds to multiple server session IDs. This causes confusion in session information queries and makes user session management difficult. In the new version, the client session ID generation and maintenance process is reconstructed. When the version of OceanBase Database is not earlier than V4.3.0 and the version of ODP is not earlier than V4.2.3, the session IDs returned by various channels, such as the <code>SHOW PROCESSLIST</code> command, the <code>information_schema.PROCESSLIST</code> and <code>GV$OB_PROCESSLIST</code> views, and the <code>connection_id</code>, <code>userenv('sid')</code>, <code>userenv('sessionid')</code>, <code>sys_context('userenv','sid')</code>, and <code>sys_context('userenv','sessionid')</code> expressions, are all client session IDs. You can specify a client session ID in the SQL or PL command <code>KILL</code> to terminate the corresponding session. If the preceding version requirements for OceanBase Database and ODP are not met, the handling method in earlier versions is used.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="7-improvement-of-the-log-stream-state-machine"><strong>(7) Improvement of the log stream state machine</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#7-improvement-of-the-log-stream-state-machine" class="hash-link" aria-label="7-improvement-of-the-log-stream-state-machine的直接链接" title="7-improvement-of-the-log-stream-state-machine的直接链接">​</a></h3>
<p>In OceanBase Database V4.3.0, the log stream status is split into the in-memory status and persistent status. The persistent status indicates the life cycle of a log stream. After the OBServer node where a log stream resides breaks down and then restarts, the system determines whether the log stream should exist and what the in-memory status of the log stream should be based on the persistent status of the log stream. The in-memory status indicates the runtime status of a log stream, representing the overall status of the log stream and the status of key submodules. Based on the explicit status and status sequence of the log stream, underlying modules can determine which operations are safe to the log stream and whether the log stream has gone through a status change of the ABA type. For backup and restore or migration processes, the working status of a log stream is optimized after the OBServer node where the log stream resides restarts. This feature improves the stability of log stream-related features and enhances the concurrency control on log streams.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="8-tenant-cloning"><strong>(8) Tenant cloning</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#8-tenant-cloning" class="hash-link" aria-label="8-tenant-cloning的直接链接" title="8-tenant-cloning的直接链接">​</a></h3>
<p>OceanBase Database V4.3.0 supports tenant cloning. You can quickly clone a specified tenant by executing an SQL statement in the sys tenant. After a tenant cloning job is completed, the created new tenant is a standby tenant. You can convert the standby tenant into the primary tenant to provide services. The new tenant and the source tenant share physical macroblocks in the initial state, but new data changes and resource usage are isolated between the tenants. You can clone an online tenant for temporary data analysis with high resource consumption or other high-risk operations to avoid risking the online tenant. In addition, you can also clone a tenant for disaster recovery. When irrecoverable misoperations are performed in the source tenant, you can use the new tenant for data rollback.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="9-support-for-s3-as-the-backup-and-restore-media"><strong>(9) Support for S3 as the backup and restore media</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#9-support-for-s3-as-the-backup-and-restore-media" class="hash-link" aria-label="9-support-for-s3-as-the-backup-and-restore-media的直接链接" title="9-support-for-s3-as-the-backup-and-restore-media的直接链接">​</a></h3>
<p>Earlier versions of OceanBase Database support two types of storage media for backup and restore: file storage (NFS) and object storage such as Alibaba Cloud Object Storage Service (OSS) and Tencent Cloud Object Storage (COS). The new version supports Amazon Simple Storage Service (S3) and S3-compatible object storage like Huawei Cloud Object Storage Service (OBS) and Google Cloud Storage (GCS) as the log archive and data backup destination. You can also use backup data on S3 and S3-compatible object storage for physical restore.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="10-proactive-broadcastrefresh-of-tablet-locations"><strong>(10) Proactive broadcast/refresh of tablet locations</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#10-proactive-broadcastrefresh-of-tablet-locations" class="hash-link" aria-label="10-proactive-broadcastrefresh-of-tablet-locations的直接链接" title="10-proactive-broadcastrefresh-of-tablet-locations的直接链接">​</a></h3>
<p>In earlier versions, OceanBase Database provides the periodic location cache refresh mechanism to ensure that the location information of log streams is updated in real time and consistent. However, tablet location information can only be passively refreshed. Changes in the mappings between tablets and log streams can trigger SQL retries and read/write errors with a certain probability. OceanBase Database V4.3.0 supports proactive broadcast of tablet locations to reduce SQL retries and read/write errors caused by changes in mappings after transfer. It also supports proactive refresh to avoid unrecoverable read/write errors.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="11-migration-of-active-transactions-during-tablet-transfer"><strong>(11) Migration of active transactions during tablet transfer</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#11-migration-of-active-transactions-during-tablet-transfer" class="hash-link" aria-label="11-migration-of-active-transactions-during-tablet-transfer的直接链接" title="11-migration-of-active-transactions-during-tablet-transfer的直接链接">​</a></h3>
<p>In the design of standalone log streams, data is in the unit of tablets, while logs are in the unit of log streams. Multiple tablets are aggregated into one log stream, saving the high cost of two-phase commit of transactions within a single log stream. To balance data and traffic among different log streams, tablets can be flexibly transferred between log streams. However, during the tablet transfer process, active transactions may still be handling the data, and even a simple operation may damage the atomicity, consistency, isolation, and durability (ACID) of the transactions. For example, if active transaction data on the transfer source cannot be completely migrated to the transfer destination during concurrent transaction execution, the atomicity of the transactions cannot be guaranteed. In earlier versions, active transactions were killed during the transfer to avoid transaction problems. This mechanism affects the normal execution of transactions to some extent. To solve this problem, the new version supports the migration of active transactions during tablet transfer, which enables concurrent execution of active transactions and ensures that no abnormal rollbacks or consistency issues occur in concurrent transactions due to the transfer.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="12-memory-throttling-mechanism"><strong>(12) Memory throttling mechanism</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#12-memory-throttling-mechanism" class="hash-link" aria-label="12-memory-throttling-mechanism的直接链接" title="12-memory-throttling-mechanism的直接链接">​</a></h3>
<p>Prior to OceanBase Database V4.x, only a few modules release memory based on freezes and minor compactions, and the MemTable is the largest part among them. Therefore, in earlier versions, an upper limit is set for memory usage of the MemTable, enabling it to run as smoothly as possible within the memory usage limit and avoiding writing failures caused by sudden memory exhaustion. In OceanBase Database V4.x, more modules that release memory based on freezes and minor compactions are introduced, such as the transaction data module. The new version provides more refined means to control the memory usage of various modules and supports the memory upper limit control of TxData and metadata service (MDS) modules. The two modules share memory space with the MemTable. When the sum of the memory usage of the three modules reaches <code>Tenant memory × _tx_share_memory_limit_percentage% × writing_throttling_trigger_percentage%</code>, overall memory throttling is triggered for the three modules. The new version also supports freezes and minor compactions of the transaction data table by time to reduce the memory usage of the transaction data module. By default, the transaction data table is frozen once every 1,800 seconds.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="13-optimization-of-ddl-temporary-result-space"><strong>(13) Optimization of DDL temporary result space</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#13-optimization-of-ddl-temporary-result-space" class="hash-link" aria-label="13-optimization-of-ddl-temporary-result-space的直接链接" title="13-optimization-of-ddl-temporary-result-space的直接链接">​</a></h3>
<p>During the DDL operations, many processes may store temporary results in materialized structures. Here are two typical scenarios: (1) During index creation, the system scans data in the base data table and sorts and inserts the obtained data to the index table. If the memory is insufficient during the sorting process, current data in the memory space will be temporarily stored in materialized structures to release the memory space for subsequent scanning. Data in the materialized structures is then merged and sorted. (2) In the columnar storage bypass import scenario, the system first temporarily stores the data to be inserted into each column group in materialized structures, and then obtains data from the materialized structures for insertion. These materialized structures can be used in the <code>SORT</code> operator to store intermediate data required for external sorting. When the system inserts data into column groups, the data can be cached in materialized structures, avoiding additional overhead caused by repeated table scanning. As a result, the temporary files occupy considerable disk space. The new version eliminates unnecessary redundant structures to simplify the data flow, and supports encoding and compression of temporary results for storage on disks. This greatly reduces the disk space occupied by temporary files.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="iii-higher-computing-performance"><strong>III. Higher computing performance</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#iii-higher-computing-performance" class="hash-link" aria-label="iii-higher-computing-performance的直接链接" title="iii-higher-computing-performance的直接链接">​</a></h2>
<p>The online analytical processing (OLAP) capabilities are significantly enhanced in the new version, achieving a performance boost in TPC-H 1TB and TPC-DS 1TB tests. The new version also optimizes PDML, read and write operations in OBKV, bypass import performance of LOB data, and node restart performance.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-increased-performance-in-the-tpc-h-1tb-test"><strong>(1) Increased performance in the TPC-H 1TB test</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#1-increased-performance-in-the-tpc-h-1tb-test" class="hash-link" aria-label="1-increased-performance-in-the-tpc-h-1tb-test的直接链接" title="1-increased-performance-in-the-tpc-h-1tb-test的直接链接">​</a></h3>
<p>The following figure shows the performance of a tenant with 80 CPU cores and 500 GB of memory of different OceanBase Database versions in the TPC-H 1TB test. Overall, the performance of V4.3.0 is about 25% higher than that of V4.2.0.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713849771931.png" alt="1713849772" class="img_ev3q"></p>
<p>Figure 1: Performance of V4.3.0 and V4.2.0 in the TPC-H 1TB test</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-increased-performance-in-tpc-ds-1tb-test"><strong>(2) Increased performance in TPC-DS 1TB test</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#2-increased-performance-in-tpc-ds-1tb-test" class="hash-link" aria-label="2-increased-performance-in-tpc-ds-1tb-test的直接链接" title="2-increased-performance-in-tpc-ds-1tb-test的直接链接">​</a></h3>
<p>The following figure shows the performance of a tenant with 80 CPU cores and 500 GB of memory of different OceanBase Database versions in the TPC-DS 1TB test. Overall, the performance of V4.3.0 is about 111% higher than that of V4.2.0.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-04/1713849828408.png" alt="1713849829" class="img_ev3q"></p>
<p>Figure 2: Performance of V4.3.0 and V4.2.0 in the TPC-DS 1TB test</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-obkv-performance-optimization"><strong>(3) OBKV performance optimization</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#3-obkv-performance-optimization" class="hash-link" aria-label="3-obkv-performance-optimization的直接链接" title="3-obkv-performance-optimization的直接链接">​</a></h3>
<p>Compared with those in V4.2.1, the OBKV single-row read-write performance is improved by about 70%, and the batch read-write performance is improved by 80% to 220%.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-pdml-transaction-optimization"><strong>(4) PDML transaction optimization</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#4-pdml-transaction-optimization" class="hash-link" aria-label="4-pdml-transaction-optimization的直接链接" title="4-pdml-transaction-optimization的直接链接">​</a></h3>
<p>The new version implements optimizations at the transaction layer by supporting parallel commit, log replay, and partition-level rollbacks within transaction participants. Compared with earlier V4.x versions, the new version significantly improves the PDML execution performance and scalability in high concurrency scenarios.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5-io-usage-optimization-for-loading-tablet-metadata"><strong>(5) I/O usage optimization for loading tablet metadata</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#5-io-usage-optimization-for-loading-tablet-metadata" class="hash-link" aria-label="5-io-usage-optimization-for-loading-tablet-metadata的直接链接" title="5-io-usage-optimization-for-loading-tablet-metadata的直接链接">​</a></h3>
<p>OceanBase Database V4.x supports millions of partitions on a single machine. As the memory may fail to hold the metadata of millions of tablets, OceanBase Database V4.x supports on-demand loading of tablet metadata. OceanBase Database supports on-demand loading of metadata at the partition level and the subclass level within partitions. In a partition, metadata is split into multiple subclasses for hierarchical storage. In scenarios where background tasks require deeper metadata, the data read consumes more I/O resources. These I/O overheads are not a problem for local SSD disks, but may affect system performance when HDD disks or cloud disks are used. OceanBase Database V4.3.0 aggregates frequently accessed metadata in storage, and only one I/O operation is required to access the metadata. This greatly reduces the I/O overhead in zero load scenarios and avoids the impact on foreground query performance caused by background task I/O overhead. In addition, the metadata loading process during the restart of an OBServer node is optimized. Tablet metadata is loaded in batches at the granularity of macroblocks, greatly reducing discrete I/O reads and speeding up the restart by several or even dozens of times.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="iv-ease-of-use-enhancements"><strong>IV. Ease-of-use enhancements</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#iv-ease-of-use-enhancements" class="hash-link" aria-label="iv-ease-of-use-enhancements的直接链接" title="iv-ease-of-use-enhancements的直接链接">​</a></h2>
<p>The new version provides the index use monitoring feature to help you identify and delete invalid indexes, and allows you to import a small amount of local data from the client. Features such as LOB INROW threshold configuration, Remote Procedure Call (RPC) authentication certificate management, and parameter resetting are also provided to improve system usability.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-index-use-monitoring"><strong>(1) Index use monitoring</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#1-index-use-monitoring" class="hash-link" aria-label="1-index-use-monitoring的直接链接" title="1-index-use-monitoring的直接链接">​</a></h3>
<p>We usually create indexes to improve the query performance of the database. However, more and more indexes are created as data tables are used in more business scenarios by more operators. Unused indexes are a waste of storage space and increase the overhead of DML operations. In this case, you need to drop useless indexes to alleviate burden on the system. However, you can hardly identify all useless indexes by manual efforts. Therefore, OceanBase Database V4.3.0 provides the index monitoring feature. After you enable this feature and set the sampling method, the index usage information that meets the rules is recorded in the memory of a user tenant and refreshed to the internal table once every 15 minutes. You can then query the DBA_INDEX_USAGE view to find out whether an index is referenced and drop useless indexes to release space.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-local-import-from-the-client"><strong>(2) Local import from the client</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#2-local-import-from-the-client" class="hash-link" aria-label="2-local-import-from-the-client的直接链接" title="2-local-import-from-the-client的直接链接">​</a></h3>
<p>OceanBase Database V4.3.0 supports the <code>LOAD DATA LOCAL INFILE</code> statement for local import from the client. You can use the feature to import local files through streaming file processing. Based on this feature, developers can import local files for testing without uploading files to the server or object storage, improving the efficiency of importing a small amount of data.</p>
<p>Note: To import local data from the client, make sure that:</p>
<p>a. The version of OceanBase Client (OBClient) is V2.2.4 or later.</p>
<p>b. The version of ODP is V3.2.4 or later. If you directly connect to an OBServer node, ignore this requirement.</p>
<p>c. The version of OceanBase Connector/J is V2.4.8 or later if you use Java and OceanBase Connector/J.</p>
<p>You can directly use a MySQL client or a native MariaDB client of any version.</p>
<p>The <code>SECURE_FILE_PRIV</code> variable is used to specify the server paths that can be accessed by the OBServer node. This variable does not affect local import from a client, and therefore does not need to be specified for local import.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-lob-inrow-threshold-configuration"><strong>(3) LOB INROW threshold configuration</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#3-lob-inrow-threshold-configuration" class="hash-link" aria-label="3-lob-inrow-threshold-configuration的直接链接" title="3-lob-inrow-threshold-configuration的直接链接">​</a></h3>
<p>By default, LOB data of a size less than or equal to 4 KB is stored in INROW mode, and LOB data of a size greater than 4 KB is stored in the LOB auxiliary table. In some scenarios, INROW storage provides higher performance than auxiliary table-based storage. Therefore, this version supports dynamic configuration of the LOB storage mode. You can adjust the INROW threshold based on your business needs, provided that the threshold does not exceed the limit for INROW storage.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-rpc-authentication-certificate-management"><strong>(4) RPC authentication certificate management</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#4-rpc-authentication-certificate-management" class="hash-link" aria-label="4-rpc-authentication-certificate-management的直接链接" title="4-rpc-authentication-certificate-management的直接链接">​</a></h3>
<p>When RPC authentication is enabled for a cluster, for an access request from a client, such as the arbitration service, primary/standby database, or OceanBase Change Data Capture (CDC), you need to place the root CA certificate of the client in the deployment directory of each OBServer node in the cluster, and then perform related configurations. This whole process is complicated. OceanBase Database V4.3.0 supports the internal certificate management feature. You can use the <code>DBMS_TRUSTED_CERTIFICATE_MANAGER</code> system package provided in the sys tenant to add, delete, and modify root CA certificates trusted by an OceanBase cluster. The DBA_OB_TRUSTED_ROOT_CERTIFICATE view is also provided in the sys tenant to display the list of client root CA certificates added to OBServer nodes in the cluster and the certificate expiration time.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5-parameter-resetting"><strong>(5) Parameter resetting</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#5-parameter-resetting" class="hash-link" aria-label="5-parameter-resetting的直接链接" title="5-parameter-resetting的直接链接">​</a></h3>
<p>In earlier versions, if you want to reset a parameter to the default value, you need to query the default value of the parameter first, and then manually set the parameter to the default value. The new version provides the <code>ALTER SYSTEM [RESET] parameter_name [SCOPE = {MEMORY | SPFILE | BOTH}] {TENANT [=] 'tenant_name'}</code> syntax for you to reset a parameter to the default value. The default value is obtained from the node that executes the statement. You can reset cluster-level parameters or parameters of a specified tenant in the sys tenant. You can also reset parameters for the current user tenant. On OBServer nodes, whether the <code>SCOPE</code> option is specified or not does not affect the implementation logic. For a parameter that takes effect statically, the default value is only stored on the disk but not updated to the memory. For a parameter that takes effect dynamically, the default value is stored on the disk and updated to the memory.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="v-afterword"><strong>V. Afterword</strong><a href="https://oceanbase.github.io/zh-Hans/blog/ob-430#v-afterword" class="hash-link" aria-label="v-afterword的直接链接" title="v-afterword的直接链接">​</a></h2>
<p>OceanBase Database V4.3.0 sets a significant milestone on our roadmap to achieve real-time AP. We will keep updating AP features of subsequent versions to overcome challenges in real-world business scenarios.</p>
<p>We would like to thank all our users and developers for their contributions to OceanBase Database V4.3.0. Their valuable suggestions are a powerful driving force that pushes OceanBase forward. We look forward to working with every user and developer in tackling critical workloads, developing modern data architectures, and building better and more user-friendly distributed databases.</p>
<p>You can visit <a href="https://www.oceanbase.com/product/oceanbase-database-rn/releaseNote" target="_blank" rel="noopener noreferrer"><strong>Release Notes</strong></a> to learn more about the new OceanBase Database V4.3.0.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why is resource isolation important for HTAP?]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/resource-isolation</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/resource-isolation</guid>
            <pubDate>Mon, 03 Jun 2024 13:23:07 GMT</pubDate>
            <description><![CDATA[About the author: Xi Huafeng, an OceanBase technical expert, has been dedicated to optimizing the high availability and scalability of databases for 11 years. He helped with the implementation of the Paxos protocol in OceanBase Database and was a member of the OceanBase TPC-C project team. He is now a member of the OceanBase system group, dedicated to building HTAP infrastructure and working on isolation of resources for AP and TP tasks.]]></description>
            <content:encoded><![CDATA[<blockquote>
<p>About the author: Xi Huafeng, an OceanBase technical expert, has been dedicated to optimizing the high availability and scalability of databases for 11 years. He helped with the implementation of the Paxos protocol in OceanBase Database and was a member of the OceanBase TPC-C project team. He is now a member of the OceanBase system group, dedicated to building HTAP infrastructure and working on isolation of resources for AP and TP tasks.</p>
</blockquote>
<p>In <a href="https://open.oceanbase.com/blog/10900410" target="_blank" rel="noopener noreferrer">Technical thoughts on the benefits of vectorized engines to HTAP</a>, we shared views of the OceanBase team on vectorized engines and introduced our ideas of dealing with complex queries by using a vectorized engine.</p>
<p>Hybrid transaction and analytical processing (HTAP) is supposed to run online transaction processing (OLTP) and online analytical processing (OLAP) in the same system for better performance. It helps with making business decisions in real time and facilitates innovation at lower operational costs. However, parallel running of OLTP and OLAP tends to cause resource contention because they use database resources, such as CPU, memory, and disk, in different ways. Minimizing such contention is the key to achieve HTAP and also the issue to resolve through resource isolation — the technology to be introduced in this article.</p>
<p>We believe that true HTAP requires complete resource isolation. A database must support logical isolation complementary to physical isolation to help users adjust resource allocation as needed. Resources for core business that requires ultimate independence can be physically isolated, while those for cost-sensitive long-tail business can be logically isolated. In this article, we will share thoughts of the OceanBase team on resource isolation, and explain why this technology is a must for HTAP and what we have done to tackle the challenges of making it work.</p>
<ul>
<li>Why is resource isolation necessary for HTAP?</li>
<li>How to implement resource isolation for HTAP?</li>
<li>What is achieved by resource isolation in OceanBase Database?</li>
</ul>
<p><strong>Why is resource isolation necessary for HTAP?</strong></p>
<p>Resource isolation lays the groundwork for HTAP</p>
<p>To show the importance of resource isolation, we can put a database beside an operating system. The two are both complex because of their openness of functionality and nature of delivering a higher price-performance ratio. The openness of functionality denotes uncontrollable workloads. For example, a user process or an SQL statement can be used to perform any operations in the system. As for the price-performance ratio, it is important because even a teeny-tiny saving of resources means a lot, given a massive user base. Among all the ways of driving up the cost performance, resource isolation is unarguably the most straightforward one.</p>
<p>After decades of development, modern operating systems are generally capable of supporting multiple users and Docker, a virtualized application container. Docker-based Kubernetes, for example, has become the de facto standard for business deployment. Databases, on the other hand, are also required to handle multi-tenancy and HTAP. Many companies separate their historical databases from online databases and perform OLAP in historical databases, which not only makes O&amp;M more complicated but also downgrades OLAP efficiency, making it impossible to achieve dynamic balance between OLTP and OLAP with limited hardware resources. As more database instances are being deployed, achieving such balance will only bring more benefits.</p>
<p>Resource isolation is a requirement naturally derived from the grouping of different workloads. For example, backup tasks at the background and SQL processing tasks at the foreground are grouped because they obviously have different requirements on timeliness. OLTP and OLAP also involve grouping because the two use resources in different ways. In other words, as long as a software system processes objects differently, it naturally classifies them into groups to ensure the quality of service (QoS), which gives rise to the need for resource isolation.</p>
<p>Resource isolation is critical to the operation stability of a database. There are two typical cases of resource isolation. First, you can reserve resources for important database tasks through resource isolation to prevent the database from being overloaded and crash. Second, users may sometimes hold business with different QoS requirements in the same database. For example, they hold real-time OLTP business and a small number of less import background tasks in the same database. If users agree to expose such information to the database so that the database can isolate resources for the business, the database will be able to run more stably.</p>
<p>A classic example of the second case is the isolation between OLAP and OLTP. To avoid interference between OLTP and OLAP, a conventional database tends to be built with more hardware resources so that each business is allocated with sufficient resources, which leads to inefficient resource utilization. To address this issue, we can consolidate multiple databases into one physical database to reduce the O&amp;M complexity and hardware costs.</p>
<p>Merging OLTP and OLAP databases into one HTAP database can be considered as a consolidation process. As aforementioned, operating systems have supported multi-user and Docker for long. Is it possible that databases also demand the sharing of physical resources as technology evolves? We believe that as technology evolves and databases grow larger, logical resource isolation will be applied in more scenarios. In the real world, many users run OLTP workloads in parallel with simple OLAP workloads in the same database. However, the performance may not be as expected due to the limited OLAP and resource isolation capabilities of the database.</p>
<p>For example, if the owner of an online store wants to know the best-sellers of a day, it's better to perform analysis in the online database. However, if the database does not support resource isolation, the analytical SQL queries may affect online transactions. To ensure the stability of online transactions, it is necessary to scale out the database by introducing more physical resources to keep the business stable. Even so, the analytical SQL queries must be strictly reviewed to prevent them from exhausting all resources.</p>
<p>Which is better: physical or logical isolation?</p>
<p>Resource isolation is not new. Conventionally, not sharing physical resources is taken as a physical isolation solution. In a database that adopts physical isolation, row-store-based replicas are used for OLTP and column-store-based replicas are used for OLAP under different tenants or within the same tenant. Physical resources for OLAP and OLTP are isolated. If cost is not a consideration, physical isolation is no doubt a better choice.</p>
<p>In the real world, however, costs and utilization of hardware resources are among the concerns of most customers. On the one hand, database hardware is expensive to purchase and maintain and needs to be replaced regularly. On the other hand, if database hardware is used for processing single business, only a minor portion of it is utilized on average. Inefficient use of hardware resources is absolutely a huge waste.</p>
<p>To make full use of hardware resources, logical isolation stands out because physical resources shared by OLAP and OLTP are logically isolated across different tenants or within the same tenant. Instead of a this-or-that choice, we believe that physical isolation and logical isolation are complementary. In view of the possible contention caused by shared resources, however, some worry that resource sharing impairs QoS and is therefore of limited value to users, while others are concerned about whether a perfect resource isolation solution is possible and whether the losses outweigh the benefits if the solution is too complex.</p>
<p>Well, on the one hand, we should get out of the box of perfectionism and recognize the obvious customer benefits of basic resource isolation capabilities. On the other hand, let's look at this issue from a forward-looking perspective and admit that the logical isolation technology is getting better over time.</p>
<p>Therefore, instead of making a choice between physical and logical isolation, an ideal HTAP solution is about finding a balance between absolute physical isolation and share-it-all. Infrastructure software should allow users to choose an isolation solution based on the scenario. It is necessary for database products to support physical and logical resource isolation at all levels.</p>
<p><strong>How to implement resource isolation for HTAP?</strong></p>
<p>Before implementing resource isolation, we must:</p>
<ul>
<li>Define resource groups and their QoS. For databases, a tenant is the most common resource group. You can also configure resource groups respectively for OLAP and OLTP.</li>
<li>Develop and implement resource isolation strategies based on the defined QoS.</li>
</ul>
<p>We will first look at the database management APIs for the database administrator (DBA), analyze the resources to be isolated (those having the greatest business impact), and then describe the isolation solution of OceanBase Database by taking CPU time, inputs and outputs per second (IOPS), and network bandwidth as examples.</p>
<p>Define resource groups and design resource plans for OLTP and OLAP</p>
<p>OceanBase Database aims to realize resource isolation between tenants and that between OLTP and OLAP within one tenant.</p>
<p>OceanBase Database allows users to define resource specifications of a tenant through unit configuration. Before you create an OceanBase Database tenant, you must create a resource pool and configure resource units in the pool to control the resource usage. If you are not familiar with this concept, go to OceanBase Documentation and take a look at the "Cluster and multi-tenant management" chapter.</p>
<p>create resource unit box1 max_cpu 4, max_memory 21474836480, max_iops 128, max_disk_size '5G', max_session_num 64, min_cpu=4, min_memory=21474836480, min_iops=128;</p>
<p>For users to define resource specifications of OLTP and OLAP within a tenant, OceanBase Database provides management APIs by referring to the classic Resource Manager service of Oracle. We have noted that customers tend to run batch processing tasks during off-peak hours, such as midnight or early morning, when OLTP is unlikely affected by OLAP, and most resources of a cluster can be allocated to OLAP with minimal resources reserved to support essential OLTP tasks. During peak hours in the daytime, the resource isolation plan can be adjusted to ensure sufficient resources for OLTP with minimal resources reserved to support essential OLAP tasks. OceanBase Database allows users to set two plans for resource management in the daytime and at night. You can activate the plans as needed to ensure isolation and maximize resource utilization.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/8834920c-824e-443d-ae10-5032f8445faf/image/2022-09-29/f2a3ed00-30b2-44df-b47e-68b793d59a67.png" alt="" class="img_ev3q"></p>
<p>For example, the following syntax defines a daytime resource plan where OLTP (<code>interactive_group</code>) and OLAP (<code>batch_group</code>) are respectively allocated with 80% and 20% of the resources.</p>
<p>DBMS_RESOURCE_MANAGER.CREATE_PLAN(
PLAN    =&gt; 'DAYTIME',
COMMENT =&gt; 'More resources for OLTP applications');
DBMS_RESOURCE_MANAGER.CREATE_PLAN_DIRECTIVE (
PLAN             =&gt; 'DAYTIME',
GROUP_OR_SUBPLAN =&gt; 'interactive_group',
COMMENT          =&gt; 'OLTP group',
MGMT_P1          =&gt; 80,
UTILIZATION_LIMIT =&gt; 100);</p>
<p>DBMS_RESOURCE_MANAGER.CREATE_PLAN_DIRECTIVE (
PLAN             =&gt; 'DAYTIME',
GROUP_OR_SUBPLAN =&gt; 'batch_group',
COMMENT          =&gt; 'OLAP group',
MGMT_P1          =&gt; 20,
UTILIZATION_LIMIT =&gt; 20);</p>
<p>After the plan is ready, you can execute the following statement to activate it:</p>
<p>ALTER SYSTEM SET RESOURCE_MANAGER_PLAN = 'DAYTIME';</p>
<p>Similarly, you can define a night resource plan and activate it during off-peak hours.</p>
<p>OceanBase Database supports user-based SQL categorization, which is simple but quite effective. You can create a user dedicated to executing analytical SQL queries, so that all SQL queries initiated by this user are processed as OLAP workloads. Also, if the execution of a request does not complete in 5 seconds, OceanBase Database identifies the request as a large query and downgrades its priority.</p>
<p>Ensure QoS with <code>min</code>, <code>max</code>, and <code>weight</code></p>
<p>QoS is a security mechanism that guarantees the smooth operation of critical processes when resources are overloaded. We will describe QoS through weight allocation and the definition of upper and lower limits on resources.</p>
<p>As the business traffic fluctuates over time, the QoS description should be flexible. If we use a fixed QoS description, just like specifying a fixed number of CPU cores and I/O bandwidth for Elastic Compute Service (ECS) of Alibaba Cloud, the system is prone to failure during peak hours due to insufficient database capacity.</p>
<p>Assume that Tenant A and Tenant B need to share 100 Mbit/s of bandwidth based on principles of resource sharing in off-peak hours and isolation in peak hours without interfering with each other.</p>
<p>How to ensure that resources are preferentially allocated to the tenant with higher priority? We can set the weight ratio between Tenant A and Tenant B to, for example, 1:3 to control the resource allocation. When both tenants need CPU resources, the ratio of CPU time spent on Tenant A and Tenant B will be 1:3. This weight ratio is specified by the <code>weight</code> parameter.</p>
<p>When a system has abundant physical resources, it is possible that a low-weight tenant takes up a lot of resources that it does not need. How to put a cap on it? We can specify the maximum resource usage for each tenant by setting the <code>max</code> parameter on top of the weight ratio. For example, with a weight ratio of 1:3 between Tenant A and Tenant B, Tenant A can use up to 25 Mbit/s of bandwidth. If we set the <code>max</code> parameter to 20 Mbit/s, the tenant will use no more than 20 Mbit/s of bandwidth.</p>
<p>The weight ratio will change if tenants are added or deleted. To ensure that each tenant obtains the minimum resources that it requires, we can specify the amount of reserved resources for each tenant by setting the <code>min</code> parameter. This not only guarantees the operation of basic functionality of all tenants but also describes QoS in a clearer way.</p>
<p>Provide better resource isolation in OceanBase Database</p>
<p>Database resources can be classified into rigid and elastic resources depending on their usage behaviors. Generally, elastic resources can be isolated. Rigid resources are necessary for programs to fulfill their duties and, once occupied, will not be released in a short period of time. Typical rigid resources include disk, memory, and the number of connections. After you make a static plan for such resources, the amount of resources allocated to each group is fixed. Elastic resources, such as IOPS, CPU time, and network bandwidth, have nothing to do with program functionality but are related to system performance. These resources can be preempted or quickly released. Users can schedule elastic resources for sharing in off-peak hours and isolation in peak hours. So, the sharing of elastic resources is what we need to focus on.</p>
<p>OceanBase Database prioritizes the isolation of the following resources that are relatively import: memory, disk space, CPU time, IOPS, and bandwidth.</p>
<p>CPU isolation</p>
<p>OceanBase Database has supported CPU time isolation and will support CPU cache isolation later. CPU isolation works in real time only when CPU is in kernel mode. This is because a resource can be scheduled only if it can be divided into many smaller pieces. For example, network I/O resources are natively in form of packets, and so do disk I/O resources. The operating system divides CPU time into many slices, which are transparent for the user mode and cannot be directly scheduled. To schedule CPU time in user mode, you need to insert many checkpoints into the code to divide the CPU time of user threads into many segments, and execute the scheduling at the checkpoints. The accuracy of checkpoint insertion, however, is not guaranteed. How to insert checkpoints into functions of a static database?</p>
<p>OceanBase Database adopts a kernel mode solution, where the CPU controller of cgroup is used. Currently, cgroup supports the <code>max</code> and <code>weight</code> parameters. Although the <code>min</code> parameter is not supported, it is not a problem because the total CPU time does not fluctuate. We can reserve the time slices for each group just by setting the <code>weight</code> parameter.</p>
<p>CPU isolation applies not only to user workloads, but also system tasks. For example, leader election among multiple replicas is a high-priority task for OBServer nodes, and we do not want the election to be affected by the CPU resource contention with user SQL queries. Therefore, we divide resources for election and user SQL queries into two directories in the root of cgroup, and further divide the user SQL directory into subdirectories corresponding to tenants and users within tenants.</p>
<p>IOPS isolation</p>
<p>If you use a solid-state disk (SSD), you can calculate the bandwidth based on this equation: Bandwidth = SSD size × IOPS. We can use normalized IOPS with an empirical formula. For example, we can take a 16 KB I/O as a normalized I/O, so that a 2 MB I/O is translated into several normalized I/Os based on the formula. Devices need to be distinguished during IOPS isolation. However, exposing the devices makes configuration more complicated. So, in most cases, multiple devices share one set of configurations.</p>
<p>These ideas are inspired by this paper about VM I/O isolation, titled "mClock: Handling Throughput Variability for Hypervisor IO Scheduling", by VMware Inc.</p>
<p>When OceanBase Database was deployed on a public cloud, we found that the I/O throughput of the cloud disk fluctuated. However, OceanBase Database quickly adapted to such fluctuation and maintained the stability of the most important OLTP business. Also, OceanBase Database associates I/O isolation with block cache, which means OceanBase Database limits not only the I/O bandwidth of OLAP but also the cache used for OLAP. In this way, the block cache can be protected from being polluted by OLAP to eventually ensure the low latency of OLTP.</p>
<p>Network bandwidth isolation</p>
<p>OBServer nodes communicate with each other by using remote procedure calls (RPCs). RPCs are sent to OBServer nodes within the same Internet data center (IDC) for the distributed execution of SQL statements and two-phase commit, and to OBServer nodes in other IDCs for log replication and data backup to ensure high availability. Unlike intra-IDC communication, the inter-IDC communication between an OBServer node and different IDCs is performed with varying latency and bandwidth usage. Usually, the bandwidth is shared for inter-IDC communication. Therefore, the bandwidth allocation and limitation must be considered globally. The question is how to define the scope of 'global'? If we have built multiple OceanBase clusters, do we need to consider them all? What if network partitioning is involved even we have only one OceanBase cluster? How can we get the global view?</p>
<p>OceanBase Database supports region-level bandwidth control since V3.2. Next, instead of holistic resource scheduling among multiple OceanBase clusters, we want the DBA to make a static resource plan. That is, the DBA needs to configure the bandwidth available to clusters for the intra-IDC and inter-IDC communication. OceanBase Database then dynamically assigns the bandwidth to OBServer nodes within a cluster, and each OBServer node further assigns the bandwidth to different groups based on their priorities.</p>
<p>For most business, bandwidth allocation for the intra-IDC communication is more important. While bandwidth isolation is quite similar to IOPS isolation, algorithms often take the network interface card (NIC) rather than each communication destination in calculations as an I/O device, given the large number of communication destinations.</p>
<p>Bandwidth isolation can be completed in two steps: tag traffic and isolate the tagged traffic based on pre-defined requirements. The first step can be performed only at the application layer, and the second step can be performed either at the application layer or the kernel layer. Since Linux Traffic Control (TC) provides a variety of throttling and priority strategies, OceanBase Database tags traffic at the application layer and throttles the tagged traffic at the kernel layer. This solution reuses capabilities of the kernel that are supported by a widely accepted ecosystem. Users do not bother to learn new throttling mechanisms.</p>
<p><strong>What is achieved by resource isolation in OceanBase Database?</strong></p>
<p>At present, OceanBase Database supports the isolation of memory, disk, CPU, and IOPS, and will support bandwidth isolation in the future. The following test takes CPU isolation as an example to show the performance of resource isolation in OceanBase Database.</p>
<p>When talking about the method of defining resource groups, we mentioned that a dedicated user can be created for OLAP. In this test, we created two test users named AP@ORACLE and TP@ORACLE, and bound OLAP tasks to AP_GROUP and OLTP tasks to TP_GROUP, assuming that the test business involves heavy OLTP workloads during daytime and most OLAP workloads are handled at night. Therefore, we set two resource plans for daytime and night. The daytime plan schedules 80% of the resources for OLTP and 20% for OLAP, and the night plan schedules 50% of the resources for OLTP and 50% for OLAP.</p>
<p>Switch from the daytime plan to the night plan</p>
<p>The result shows that the OLAP QPS increases significantly while the OLTP QPS decreases after the plan switchover due to a larger portion of CPU resources allocated to OLAP in the night plan. In the figure below, you can see the turning points of OLAP and OLTP throughput curves caused by the plan switchover.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/df4eeb08-6997-4447-9ba2-a934dd3cad25/image/2022-09-29/673ae0a8-e777-4612-8326-0dd0a3d81e09.png" alt="" class="img_ev3q"></p>
<p>It seems that the change in the OLTP throughput is not as noticeable in comparison to that of OLAP. This is actually a result as expected. The percentage of resources for OLAP is increased from 20% to 50%, an increase of 150%, and that for OLTP is reduced from 80% to 50%, a decrease of 37.5%. Given that the actual OLTP throughput drops from 19,000 to 14,300 QPS, a 24.7% decrease, the gap does not make much difference.</p>
<p>The performance of CPU isolation relies largely on the type of workload. If the network becomes a bottleneck, bandwidth isolation is also necessary. The test is not intended to bang the drum for CPU isolation as a cure-all, but it does show that simple CPU isolation works well for CPU-bound workloads, even without the CPU cache isolation. Keep in mind that isolation capabilities are getting better over time. CPU isolation alone takes effect on OLTP-simple OLAP isolation or OLTP-OLTP isolation. If we combine CPU isolation with IOPS isolation and network bandwidth isolation, the application scope will be even wider.</p>
<p>This article introduces thoughts of the OceanBase team on resource isolation technology and its implementation solution. Highly efficient and effective resource isolation is required to ensure sharing of hardware resources among different tenants and among OLTP and OLAP services within the same tenant in an HTAP database. We believe that a resource isolation solution that integrates complementary physical and logical isolation mechanisms is more suitable for an HTAP database. OceanBase will keep optimizing the resource isolation technology to better meet the needs of users.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Flink CDC + OceanBase integration solution for full and incremental synchronization]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/flink-and-ob</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/flink-and-ob</guid>
            <pubDate>Mon, 03 Jun 2024 13:23:07 GMT</pubDate>
            <description><![CDATA[Introduction]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://oceanbase.github.io/zh-Hans/blog/flink-and-ob#introduction" class="hash-link" aria-label="Introduction的直接链接" title="Introduction的直接链接">​</a></h2>
<p>Change Data Capture (CDC) is a widely applied technology that captures database changes. In this post, we will introduce you to the Flink CDC + OceanBase Database data integration solution. This solution combines CDC with extraordinary pipeline capabilities and diversified ecosystem tools of Flink, to synchronize processed CDC data to the downstream, formulating a solution for integrated full and incremental synchronization based on OceanBase Database Community Edition.</p>
<p>The solution brings two benefits. First, it synchronizes data by using one component and one link. Second, Flink SQL supports aggregation and extract-transform-load (ETL) of database and table shards, making it much easier for users to analyze, process, and synchronize CDC data by executing a Flink SQL job.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="keypoints">Keypoints<a href="https://oceanbase.github.io/zh-Hans/blog/flink-and-ob#keypoints" class="hash-link" aria-label="Keypoints的直接链接" title="Keypoints的直接链接">​</a></h2>
<p>This post is based on the content shared by Wang He, an open source tool expert with OceanBase.</p>
<p>It contains the following five parts:</p>
<ol>
<li>
<p>Introduction to the CDC technology</p>
</li>
<li>
<p>Introduction to OceanBase CDC components</p>
</li>
<li>
<p>Introduction to Flink CDC</p>
</li>
<li>
<p>Use Flink CDC OceanBase Connector</p>
</li>
<li>
<p>Conclusion</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="i-cdc-technology">I. CDC technology<a href="https://oceanbase.github.io/zh-Hans/blog/flink-and-ob#i-cdc-technology" class="hash-link" aria-label="I. CDC technology的直接链接" title="I. CDC technology的直接链接">​</a></h2>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/eaa84253-a48e-438c-a458-29cc39c6d194/image/2022-05-07/7f3f97dc-eb5c-43f5-a8c8-44f5064bee08.png" alt="" class="img_ev3q"></p>
<p>The CDC technology monitors and captures changes in a database, such as the INSERT, UPDATE, and DELETE operations on the data or data tables, and then writes the changes to message-oriented middleware, so that other services can subscribe to and consume the changes.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/7b3f7a72-e230-4ada-8f67-d1feb68d1168/image/2022-05-07/ac855f6e-3a00-44f3-b7e6-39517751fec0.png" alt="" class="img_ev3q"></p>
<p>Alibaba Canal is a popular open source CDC tool, which is mainly used in Alibaba Cloud open-source components for incremental MySQL data subscription and consumption. The latest version of Alibaba Canal supports OceanBase Database Community Edition data sources, incremental DDL and DML operations, and filtering of databases, tables, and columns. You can use it with ZooKeeper for the deployment of high-availability clusters. The client adapter of Alibaba Canal supports multiple types of containers as the destination. You can use it with Alibaba Otter to achieve active geo-redundancy.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/17c2e5c5-9b31-4290-8cbd-983815f1e469/image/2022-05-07/300d3459-5fa8-416c-a5d5-84e03fdcc488.png" alt="" class="img_ev3q"></p>
<p>Another popular open source CDC framework is Debezium.</p>
<p>It supports the synchronization of DDL and DML operation logs, uses the primary key or unique key as the key of the message body, and also supports the snapshot mode and full synchronization.</p>
<p>Debezium also supports a variety of data sources. You can integrate Debezium Server into a program as an embedded engine to directly write data to a message system without using Kafka.</p>
<h1>II. OceanBase CDC components</h1>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/b38cbaf6-fc19-4eef-b172-27a5016cc451/image/2022-05-07/15fd5ef4-7679-403b-aad1-a39d8700b0bc.png" alt="" class="img_ev3q"></p>
<p>OceanBase Database Community Edition provides four CDC components:</p>
<p>obcdc (formerly liboblog): pulls incremental logs in sequence.</p>
<p>oblogmsg: parses the format of incremental logs.</p>
<p>oblogproxy: pulls the incremental logs.</p>
<p>oblogclient: connects to oblogproxy to obtain the incremental logs.</p>
<p>OceanBase Migration Service (OMS) Community Edition is provided. It is an all-in-one data migration tool for incremental data migration, full data migration, and full data verification.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/0fa227bf-343a-4b54-9eb8-ec0adcb7b018/image/2022-05-07/8eceea9f-1f1e-445c-9a9f-24c7cd495cd6.png" alt="" class="img_ev3q"></p>
<p>The preceding figure shows the CDC logic of OceanBase Database Community Edition. Data is pulled by oblogproxy and OMS Community Edition. Canal and Flink CDC are integrated with oblogclient to obtain incremental logs from oblogproxy.</p>
<h1>III. Flink CDC</h1>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/fa9a24c3-5684-40c4-b584-35992433bff5/image/2022-05-07/ba8f10ce-eacf-48ef-b6b6-402368806f09.png" alt="" class="img_ev3q"></p>
<p>Flink CDC supports multiple data sources, such as MySQL, PostgreSQL, and Oracle. Flink CDC reads the full and incremental data from a variety of databases, and then automatically transfers data to the Flink SQL engine for processing.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/fc9856de-09ed-4eaa-b110-dff6d75f5b32/image/2022-05-07/4d648854-a0e5-4404-83ea-18d1ce2b94cd.png" alt="" class="img_ev3q"></p>
<p>Flink is a hybrid engine that supports both batch and streaming processing. Flink CDC converts streaming data into a dynamic table. In the preceding figure, the lower left part shows the mapping between streaming data and a dynamic table. The lower right part shows the results of multiple executions of continuous queries.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/091b66ed-1259-46f9-a385-3353c6c77a2f/image/2022-05-07/3217492e-54fc-4dac-831f-814dfd2b855c.png" alt="" class="img_ev3q"></p>
<p>The preceding figure shows the working principle of Flink CDC. It implements the SourceFunction API based on Debezium and supports MySQL, Oracle, MongoDB, PostgreSQL, and SQLServer.</p>
<p>The latest version of Flink CDC supports data reads from a MySQL data source by using the Source API, which provides enhanced concurrent reading compared to the SourceFunction API.</p>
<p>The OceanBaseRichSourceFunction API is implemented for full and incremental data reads respectively based on JDBC and oblogclient.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/ef1db322-542c-460c-a4cc-7052711bdbb8/image/2022-05-07/7144e0a0-de64-4123-966a-4de6cf427d0b.png" alt="" class="img_ev3q"></p>
<p>=============</p>
<h1>IV. Use Flink CDC OceanBase Connector</h1>
<p>Configure the <code>docker-compose.yml</code> file and start the container. Go to the directory where the <code>docker-compose.yml</code> file is stored, and run the <code>docker-compose up-d</code> command to start the required components.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/9b5d74d0-01c9-42b7-88e8-329996981c57/image/2022-05-07/b87e2f73-36ad-47c2-b991-1bdff8fb5aba.png" alt="" class="img_ev3q"></p>
<p>Run the <code>docker-compose exec observer obclient-h127.0.0.1-P2881-uroot-ppsw</code> command to log on by using newly created username and password. Download the required dependency packages and execute Flink DDL statements on the CLI of Flink SQL to create a table.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/67ccf926-fd34-4175-b7f7-6219ce53ec66/image/2022-05-07/022ef971-e3b8-4861-8590-3734401c1020.png" alt="" class="img_ev3q"></p>
<p>Set the checkpointing interval to 3 seconds and the local time zone to Asia/Shanghai. Then, create an order table, a commodity table, and the associated order data table. Perform data reads and writes.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/77f314e9-0062-455a-a0aa-e34bc0f3de1a/image/2022-05-07/1a5ea095-35cf-4106-b629-7064a8a75792.png" alt="" class="img_ev3q"></p>
<p>View the data in Kibana by visiting the following address:</p>
<p><a href="http://localhost:5601/app/kibana#/management/kibana/index_pattern" target="_blank" rel="noopener noreferrer">http://localhost:5601/app/kibana#/management/kibana/index_pattern</a></p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/f79cc96e-182c-4180-808b-a7b8b091f4a7/image/2022-05-07/50904e7a-a129-46d1-b841-f9b19c64edee.png" alt="" class="img_ev3q"></p>
<p>Create an index pattern named <code>enriched_orders</code>, and then view the written data by visiting <a href="http://localhost:5601/app/kibana#/discover%E7%9C%8B%E5%88%B0%E5%86%99%E5%85%A5%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BA%86%E3%80%82" target="_blank" rel="noopener noreferrer">http://localhost:5601/app/kibana#/discover</a>.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/69bd86f3-8601-4561-a9e0-30831203e859/image/2022-05-07/01c329c8-b97b-4dfc-8e6c-117d00e24f69.png" alt="" class="img_ev3q"></p>
<p>Modify the data of the monitored table and view the incremental data changes. Perform the following modification operations in OceanBase Database in sequence, and refresh Kibana once after each step. We can see that the order data displayed in Kibana is updated in real time.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/62c8b1a5-50d1-4903-a221-ce21c8f1d6c4/image/2022-05-07/991c53d2-425e-4d01-a20d-3851bff54609.png" alt="" class="img_ev3q"></p>
<p>Clean up the environment. Go to the directory where the <code>docker-compose.yml</code> file is located, and run the <code>docker-compose down</code> command to stop all containers. Go to the Flink deployment directory and run the <code>./bin/stop-cluster.sh</code> command to stop the Flink cluster.</p>
<h1>V. Conclusion</h1>
<p>Flink CDC supports full and incremental data migration between many types of data sources and works with Flink SQL to perform ETL operations on streaming data. As of the release of Flink CDC 2.2, the project has 44 contributors, 4 maintainers, and more than 4,000 community members.</p>
<p>OceanBase Connector can be integrated with Flink CDC 2.2 or later to read full data and incremental DML operations from multiple databases and tables in AT_LEAST_ONCE mode. Flink CDC OceanBase Connector will gradually support concurrent reads, incremental DDL operations, and the EXACTLY_ONCE mode in later versions.</p>
<p>Now, let's briefly compare the existing CDC solutions. OMS Community Edition is a proven online data migration tool with a GUI-based console. It provides full data migration, incremental data migration, data verification, and O&amp;M services. DataX + Canal/Otter is a fully open source solution. Canal supports many types of destinations and incremental DDL operations, and Otter supports active-active disaster recovery.</p>
<h1>Afterword</h1>
<p>Flink CDC is a fully open source solution and is supported by an active community. It supports full and incremental data synchronization between many types of data sources and destinations. It is worth mentioning that Flink CDC is easy to use, and supports aggregation and ETL of database and table shards. Compared with some existing CDC solutions that involve complex data cleaning, analysis, and aggregation operations, Flink SQL allows users to easily process data for various business needs by using methods such as stream-stream join and dimension table join.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="contact-us">Contact us<a href="https://oceanbase.github.io/zh-Hans/blog/flink-and-ob#contact-us" class="hash-link" aria-label="Contact us的直接链接" title="Contact us的直接链接">​</a></h2>
<p><strong>Feel free to contact us at any time.</strong></p>
<p><a href="https://open.oceanbase.com/answer" target="_blank" rel="noopener noreferrer">Visit the official forum of OceanBase Database Community Edition</a></p>
<p><a href="https://github.com/oceanbase/oceanbase/issues" target="_blank" rel="noopener noreferrer">Report an issue of OceanBase Database Community Edition</a></p>
<p><strong>DingTalk Group ID: 33254054</strong></p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/8223c1be-2a25-4658-9d9f-5fd4594e9900/image/2022-05-07/77e7a1ce-01b1-4b45-9aaa-ff184f43f822.png" alt="" class="img_ev3q"></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hello OceanBase! The first lesson for becoming an OceanBase contributor]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase</guid>
            <pubDate>Mon, 03 Jun 2024 13:23:07 GMT</pubDate>
            <description><![CDATA[About the author: Xia Ke, a contributor of OceanBase Community, has engaged in the design and development of financial core systems for years. He is now working on the investigation of China-made databases in a subsidiary of a stock exchange, and has recently obtained OceanBase Certified Associate (OBCA) and PingCAP Certified TiDB Associate (PCTA) certificates.]]></description>
            <content:encoded><![CDATA[<blockquote>
<p>About the author: Xia Ke, a contributor of OceanBase Community, has engaged in the design and development of financial core systems for years. He is now working on the investigation of China-made databases in a subsidiary of a stock exchange, and has recently obtained OceanBase Certified Associate (OBCA) and PingCAP Certified TiDB Associate (PCTA) certificates.</p>
</blockquote>
<p>The other day I read this post <a href="https://open.oceanbase.com/blog/8600156?currentPage=undefined" target="_blank" rel="noopener noreferrer">Decoding OceanBase (11): Expressions and Functions</a> by Zhuweng, a Peking University alumnus, director of OceanBase kernel R&amp;D. At the end, he mentioned that creating a built-in function is the first test for new recruits joining the SQL team of OceanBase. Though having no intention of looking for a new job, I was so intrigued by this <strong>first test</strong>. I am not a database administrator, so I have not spent much time working with databases until lately when I started learning about top-notch home-grown database products for job reasons. As far as my job description is concerned, I have "walked out of my circle" to do the test. After reading posts from popular IT communities and participating in open online courses, I was deeply impressed by the vigorous momentum of the database ecosystem in China. I want to say thank you to database developers and engineers who have shared their experience, which enables laymen like me to quickly get on track. Benefiting from their good deeds, I would like to take this opportunity to make my contributions to the OceanBase community, hoping that you might find it useful.</p>
<h1>Overview</h1>
<p>Seeing <code>Hello OceanBase</code> in the title, you may think of "Hello World". Yes, that is exactly the vibe I wanted to strike here. This post is a "Hello World" demo that shows you how to create or modify a built-in function in OceanBase Database, or in other words, how to do secondary development based on OceanBase Database Community Edition. In addition to being motivated by <code>Zhuweng</code>, I want to take this test also because of the requirement for extending database capabilities by using external functions. Oracle allows users to call external C or JAVA functions, just like calling built-in functions. In a sense, this feature makes a database more capable. Users can call external C or JAVA functions to, for example, implement complex mathematical algorithms, which may otherwise cause troubles by using SQL statements or Oracle built-in functions. Of course, you can always implement those algorithms at the business layer, but maybe we can talk about that in another post. Based on my research on a bunch of home-grown databases, I have come up with similar procedures for using external functions. You can take a look at them in my posts <a href="https://blog.csdn.net/xk_xx/article/details/123091480?spm=1001.2014.3001.5501" target="_blank" rel="noopener noreferrer">Implement Oracle external functions in Dameng DM8 Database</a> and <a href="https://blog.csdn.net/xk_xx/article/details/123011397?spm=1001.2014.3001.5501" target="_blank" rel="noopener noreferrer">Implement PostgreSQL UDFs by using the contrib module</a>. Calling external functions in DM8 is basically the same as in Oracle, except for some slight implementation differences. Some databases, such as openGauss, MogDB, TDSQL PostgreSQL, and KingbaseES, come with an PostgreSQL kernel, and they inherently support the extension mechanisms of PostgreSQL. To the best of my knowledge, however, OceanBase Database does not support external functions. So, I wondered if I could not find a way out, a way in might do the trick. Let's start with "Hello OceanBase".</p>
<h1>Preparations</h1>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="an-oceanbase-cluster">An OceanBase cluster<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#an-oceanbase-cluster" class="hash-link" aria-label="An OceanBase cluster的直接链接" title="An OceanBase cluster的直接链接">​</a></h2>
<p>You can find tons of posts in the community about how to deploy OceanBase Database in all kinds of supported modes. Here are some of my posts in this regard: <a href="https://blog.csdn.net/xk_xx/article/details/122757336" target="_blank" rel="noopener noreferrer">Use Docker to deploy OceanBase Database</a>, <a href="https://blog.csdn.net/xk_xx/article/details/122763419" target="_blank" rel="noopener noreferrer">Manually deploy OceanBase Database in standalone mode</a>, and <a href="https://blog.csdn.net/xk_xx/article/details/123166584" target="_blank" rel="noopener noreferrer">Use OBD to locally deploy OceanBase Database in standalone mode</a>. To implement the demo, I recommend that you pick an easy one and use OceanBase Deployer (OBD) to locally deploy a standalone OceanBase database in a development environment.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="oceanbase-source-code">OceanBase source code<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#oceanbase-source-code" class="hash-link" aria-label="OceanBase source code的直接链接" title="OceanBase source code的直接链接">​</a></h2>
<p>You can get the latest source code by running the following command: <code>git clone https://github.com/oceanbase/oceanbase</code></p>
<h1>Code structure</h1>
<p>You can take a look at the <code>Decoding OceanBase</code> serial posts of the community for details.</p>
<p>Here, let me briefly describe the code related to the <code>sql/resolver/expr</code> directory.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="register-a-built-in-function">Register a built-in function<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#register-a-built-in-function" class="hash-link" aria-label="Register a built-in function的直接链接" title="Register a built-in function的直接链接">​</a></h2>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/ec1d519b-9b37-4d6e-9b86-68074ff85e3b/image/2022-03-04/78c15e68-ea1b-40f0-b4e7-07b71454a82b.png" alt="" class="img_ev3q"></p>
<div class="language-C++ language-c++ codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-c++ codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">    #define REG_OP(OpClass)                                                \do {                                                                 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        OpClass op(alloc);                                                 \if (OB_UNLIKELY(i &gt;= EXPR_OP_NUM)) {                               \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          LOG_ERROR("out of the max expr");                                \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        } else {                                                           \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          NAME_TYPES[i].name_ = op.get_name();                             \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          NAME_TYPES[i].type_ = op.get_type();                             \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          OP_ALLOC[op.get_type()] = ObExprOperatorFactory::alloc&lt;OpClass&gt;; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          i++;                                                             \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        }                                                                  \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      } while (0)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="diagram-of-the-expr-class">Diagram of the expr class<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#diagram-of-the-expr-class" class="hash-link" aria-label="Diagram of the expr class的直接链接" title="Diagram of the expr class的直接链接">​</a></h2>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/69375afc-a8a4-4cdf-b339-5cd3ebfe84be/image/2022-03-04/726f726f-7ed3-4060-b909-6595697154b3.png" alt="" class="img_ev3q"></p>
<p>Built-in functions mainly implement the ObExprOperator interface class, which contains many functions.</p>
<p>The <code>calc_result_type0</code> and <code>calc_result0</code> functions specify the memory allocation and type definition for function registration. The <code>cg_expr</code> function registers the function pointer to the <code>eval_func_</code> function. The built-in function <code>rt_expr.eval_func_ = ObExprHello::eval_hello;</code> is called by using the function pointer. <code>eval_hello</code> is the function that actually do the job.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="develop-hello-oceanbase">Develop Hello OceanBase<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#develop-hello-oceanbase" class="hash-link" aria-label="Develop Hello OceanBase的直接链接" title="Develop Hello OceanBase的直接链接">​</a></h2>
<p>In this project, you need to modify the files shown in the following figure.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/b33bf6b7-47c4-4d2e-b3f8-31e1a2349567/image/2022-03-04/38ff080c-902e-4976-9b12-6113ba751b0d.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-create-the-obexprhello-class">1. Create the ObExprHello class<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#1-create-the-obexprhello-class" class="hash-link" aria-label="1. Create the ObExprHello class的直接链接" title="1. Create the ObExprHello class的直接链接">​</a></h3>
<p>Many implementation examples are provided in the <code>sql/resolver/expr</code> directory. You can select reference objects as needed.</p>
<div class="language-C++ language-c++ codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-c++ codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">    #ifndef _OB_EXPR_HELLO_H_</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    #define  _OB_EXPR_HELLO_H_</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    #include  "sql/engine/expr/ob_expr_operator.h"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    namespace  oceanbase {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    namespace  sql {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    class  ObExprHello : public  ObStringExprOperator {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    public:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      explicit  ObExprHello(common::ObIAllocator&amp;  alloc);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      virtual  ~ObExprHello();</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      virtual  int  calc_result_type0(ObExprResType&amp;  type, common::ObExprTypeCtx&amp;  type_ctx) const;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      virtual  int  calc_result0(common::ObObj&amp;  result, common::ObExprCtx&amp;  expr_ctx) const;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      static  int  eval_hello(const  ObExpr&amp;  expr, ObEvalCtx&amp;  ctx, ObDatum&amp;  expr_datum);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      virtual  int  cg_expr(ObExprCGCtx&amp;  op_cg_ctx, const  ObRawExpr&amp;  raw_expr, ObExpr&amp;  rt_expr) const  override;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    private:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      DISALLOW_COPY_AND_ASSIGN(ObExprHello);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    };</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    } /* namespace sql */</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    } /* namespace oceanbase */</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    #endif</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The content of the new file <code>ob_expr_hello.cpp</code> is as follows:</p>
<div class="language-C++ language-c++ codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-c++ codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">    #define  USING_LOG_PREFIX SQL_ENG</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    #include  "sql/engine/expr/ob_expr_hello.h"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    static  const  char* SAY_HELLO = "Hello OceanBase!";</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    namespace  oceanbase {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    using  namespace  common;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    namespace  sql {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ObExprHello::ObExprHello(ObIAllocator&amp; alloc) : ObStringExprOperator(alloc, T_FUN_SYS_HELLO, N_HELLO, 0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    {}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ObExprHello::~ObExprHello()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    {}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    int  ObExprHello::calc_result_type0(ObExprResType&amp;  type, ObExprTypeCtx&amp;  type_ctx) const</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      UNUSED(type_ctx);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      type.set_varchar();</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      type.set_length(static_cast&lt;common::ObLength&gt;(strlen(SAY_HELLO)));</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      type.set_default_collation_type();</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      type.set_collation_level(CS_LEVEL_SYSCONST);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      return  OB_SUCCESS;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    int  ObExprHello::calc_result0(ObObj&amp;  result, ObExprCtx&amp;  expr_ctx) const</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      UNUSED(expr_ctx);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      result.set_varchar(common::ObString(SAY_HELLO));</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      result.set_collation(result_type_);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      return  OB_SUCCESS;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    int  ObExprHello::eval_hello(const  ObExpr&amp;  expr, ObEvalCtx&amp;  ctx, ObDatum&amp;  expr_datum)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      UNUSED(expr);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      UNUSED(ctx);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      expr_datum.set_string(common::ObString(SAY_HELLO));</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      return  OB_SUCCESS;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    int  ObExprHello::cg_expr(ObExprCGCtx&amp;  op_cg_ctx, const  ObRawExpr&amp;  raw_expr, ObExpr&amp;  rt_expr) const</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      UNUSED(raw_expr);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      UNUSED(op_cg_ctx);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      rt_expr.eval_func_ = ObExprHello::eval_hello;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      return  OB_SUCCESS;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }  // namespace sql</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }  // namespace oceanbase</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-modify-or-add-the-function-name-definition">2. Modify or add the function name definition<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#2-modify-or-add-the-function-name-definition" class="hash-link" aria-label="2. Modify or add the function name definition的直接链接" title="2. Modify or add the function name definition的直接链接">​</a></h3>
<ul>
<li>ob_name_def.h</li>
</ul>
<p>The function name is registered here and can be used for syntax parsing.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/889fe5d5-bcc4-49ee-90c2-65c8d5a8e0d7/image/2022-03-04/0736c95b-c1be-4e3d-984b-2422fd3dee7f.png" alt="" class="img_ev3q"><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/d03c52a3-a866-4f3d-bac7-ac5201428edc/image/2022-03-04/606b6b77-f22a-4b38-9515-3c9cf03a3ffb.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-modify-the-factory-class">3. Modify the factory class<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#3-modify-the-factory-class" class="hash-link" aria-label="3. Modify the factory class的直接链接" title="3. Modify the factory class的直接链接">​</a></h3>
<p>ob_expr_operator_factory.cpp</p>
<p>The function pointer is registered at this step, and will be used for calling the specific built-in function at runtime.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/f07b83a3-4d9e-4169-bd6c-620e9a1b02b8/image/2022-03-04/d67d1e07-8e82-4169-b400-19f30d813d19.png" alt="" class="img_ev3q"></p>
<ul>
<li>Register a built-in function</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/e544598d-cb7d-4b8d-ae8a-85fa04f802c8/image/2022-03-04/bb53c1e7-27f7-4370-a7cc-619241c945b2.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-add-ids">4. Add IDs<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#4-add-ids" class="hash-link" aria-label="4. Add IDs的直接链接" title="4. Add IDs的直接链接">​</a></h3>
<ul>
<li>ob_item_type.h</li>
</ul>
<p>You can take an ID as a key that points to the function pointer.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/597a87f1-1998-41f0-8c5d-29a818026d92/image/2022-03-04/8223570b-4af3-438d-a533-e20eec745e17.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="5-modify-project-files">5. Modify project files<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#5-modify-project-files" class="hash-link" aria-label="5. Modify project files的直接链接" title="5. Modify project files的直接链接">​</a></h3>
<ul>
<li>CMakeLists.txt</li>
</ul>
<p>Add the new ObExprHello function to the project for compilation.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/7128397f-fcd9-4320-bcb7-982fe9b80a12/image/2022-03-04/458ceecc-d9ba-4fdd-a7ad-c00b89f39935.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="6-ob_expr_hellocpp">6. ob_expr_hello.cpp<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#6-ob_expr_hellocpp" class="hash-link" aria-label="6. ob_expr_hello.cpp的直接链接" title="6. ob_expr_hello.cpp的直接链接">​</a></h3>
<h3><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/98290631-db52-4f32-bf1f-4b185907c9ac/image/2022-03-04/2ac97743-4494-4d81-ba5f-25dec9aabe2c.png" alt="" class="img_ev3q"></h3>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="-1"><a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#-1" class="hash-link" aria-label="-1的直接链接" title="-1的直接链接">​</a></h3>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="7ob_expr_eval_functionscpp">7.ob_expr_eval_functions.cpp<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#7ob_expr_eval_functionscpp" class="hash-link" aria-label="7.ob_expr_eval_functions.cpp的直接链接" title="7.ob_expr_eval_functions.cpp的直接链接">​</a></h3>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/b3d7d96d-7c46-445f-883d-473ecb068cd4/image/2022-03-04/d587de86-0571-42bc-8f1e-3081b47b6899.png" alt="" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="compile-the-function">Compile the function<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#compile-the-function" class="hash-link" aria-label="Compile the function的直接链接" title="Compile the function的直接链接">​</a></h2>
<p>I'll skip this part. Please read other posts about how to compile the OceanBase source code for details.</p>
<p>And by the way, I have found some compilation errors in the latest code and created a pull request on GitHub, which has been accepted but not yet been merged.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="verify-the-function">Verify the function<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#verify-the-function" class="hash-link" aria-label="Verify the function的直接链接" title="Verify the function的直接链接">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-replace-the-observer-process">1. Replace the observer process<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#1-replace-the-observer-process" class="hash-link" aria-label="1. Replace the observer process的直接链接" title="1. Replace the observer process的直接链接">​</a></h3>
<p>Create a soft connection to points the observer process in the <code>/root/observer/bin</code> directory to the observer process in the <code>/root/.obd/repository/oceanbase-ce/3.1.2/7fafba0fac1e90cbd1b5b7ae5fa129b64dc63aed/bin</code> directory.</p>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/83dba035-7b05-49c2-828e-0da3bd8c0f8c/image/2022-03-04/d8015f5f-b55c-4bee-bfe1-e16bad69b2ec.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-start-the-observer-process">2. Start the observer process<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#2-start-the-observer-process" class="hash-link" aria-label="2. Start the observer process的直接链接" title="2. Start the observer process的直接链接">​</a></h3>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/9d017a42-5261-4e8a-84e8-e578707c3c41/image/2022-03-04/8de93958-9c77-4932-ac53-55f2389d153f.png" alt="" class="img_ev3q"></p>
<p>You may notice that the version is 3.1.3, which is not released yet. We got that result because the latest code was used.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="test-the-function">Test the function<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#test-the-function" class="hash-link" aria-label="Test the function的直接链接" title="Test the function的直接链接">​</a></h2>
<p><img decoding="async" loading="lazy" src="https://gw.alipayobjects.com/zos/oceanbase/57787d55-ce12-4960-8e87-f39478f97f68/image/2022-03-04/60705b08-75e1-477b-bf21-5d07bd7902ef.png" alt="" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="suggestions">Suggestions<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#suggestions" class="hash-link" aria-label="Suggestions的直接链接" title="Suggestions的直接链接">​</a></h3>
<p>In the past few months, I have been researching home-grown databases, such as OceanBase Database, TiDB, openGauss, MogDB, and Dameng. They are capable of online transaction processing (OLTP), online analytical processing (OLAP), or hybrid transaction/analytical processing (HTAP). I am not bold enough to compare them as a layman, but as a user, I would like to bring up a few points based on my experience with OceanBase Database.</p>
<p>It took me some time to build the environment, which is acceptable because, after all, it is a distributed system. However, it would be great if users are provided with a tool to quickly build a demo cluster, like the playground of TiDB.</p>
<p>The system is resource-consuming. Users with small-specification devices may suffer deployment failures due to resource insufficiency. They will be grateful if small-specification deployment is supported.</p>
<p>Maybe OceanBase Database can consider supporting user-defined extension interfaces? Some may think that is not a necessary feature, but it is quite useful in some enterprise-level applications, and wins OceanBase a point or two when comparing it to Oracle.</p>
<p>OceanBase Database Enterprise Edition can support more driver APIs for Oracle tenants. For more information, see "Use JDBC to connect to OceanBase Database through JayDeBeApi in Python".</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="afterword">Afterword<a href="https://oceanbase.github.io/zh-Hans/blog/hello-oceanbase#afterword" class="hash-link" aria-label="Afterword的直接链接" title="Afterword的直接链接">​</a></h2>
<p>Most of posts in the community are intended for database administrators, focusing on deployment, migration, application, performance, and O&amp;M. This one may not attract a large audience. However, I hope it can encourage more better content on the secondary development of open source databases.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How OB Cloud Achieves Cost Reduction and Efficiency Improvement in Replacing MySQL Scenarios ？]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/obcloud</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/obcloud</guid>
            <pubDate>Mon, 03 Jun 2024 13:23:07 GMT</pubDate>
            <description><![CDATA[Cost reduction and efficiency improvement seem an eternal topic for any enterprises that are seeking sustainable development and profit. However, hasty rigid cost reduction measures tend to cause uncontrollable impact, leading to inefficient operation and slow business growth. By promoting cost reduction, we should aim at improving productivity while cutting operation costs.]]></description>
            <content:encoded><![CDATA[<p>Cost reduction and efficiency improvement seem an eternal topic for any enterprises that are seeking sustainable development and profit. However, hasty rigid cost reduction measures tend to cause uncontrollable impact, leading to inefficient operation and slow business growth. By promoting cost reduction, we should aim at improving productivity while cutting operation costs.</p>
<p>In the years when the conventional centralized databases dominated the industry, reducing costs without hurting database efficiency was really a headache for IT departments of many enterprises. Based on the content shared by OceanBase solution architect Gao Jiwei in OceanBase Cloud Open Class, this post compares OceanBase Cloud with MySQL, analyzes features that help enterprises increase efficiency with reduced costs, and discusses how to make cost reduction plans for small and medium-sized enterprises (SMEs) and large corporations by using two examples.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647401464.png" alt="1691647401" class="img_ev3q"></p>
<p>In recent years, enterprises have been increasing their efforts to reduce costs and increase efficiency, but mostly by downgrading resource specifications, which could pose unbearable risk, especially for databases, the cornerstone of a software architecture.</p>
<p>Therefore, it is crucial to find a way that reduces costs while ensuring a very high throughput without compromising performance stability. This is what OceanBase is doing right now. We are designing cost reduction solutions leveraging our technical know-how. Let's first look at the following figure, which shows a general comparison between MySQL and OceanBase Cloud.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647424030.png" alt="1691647424" class="img_ev3q"></p>
<ul>
<li><strong>MySQL downside No. 1: numerous complex instances</strong>. In a MySQL cluster, the resource usage of some instances is higher than others. This resource usage imbalance makes O&amp;M more difficult. OceanBase Cloud features a natively distributed architecture, which allows you to consolidate MySQL instances into tenants of one cluster. This simplifies O&amp;M and improves the overall utilization of cluster resources.</li>
<li><strong>MySQL downside No. 2: uncompressed data</strong>. MySQL adopts a B+ tree-based storage structure, which has been in use for years. An inherent flaw of this structure is that unused space exists on each page. OceanBase Cloud comes with a fully self-developed log-structured merge-tree (LSM-tree)-based storage structure. By using unique compression algorithms, OceanBase Cloud compresses a data set to 1/4 to 1/5 or even less of its original size in MySQL. Such a high compression ratio greatly reduces the storage costs.</li>
<li><strong>MySQL downside No. 3: poor scalability</strong>. MySQL Database runs in primary/secondary mode. Server replacement is inevitable if you want to scale its capacity. To replace a server, a primary/secondary switchover is performed, which causes considerable application interruptions. For that reason, it is often necessary to configure a MySQL database against the maximum possible traffic to avoid a switchover. For example, if eight CPU cores are enough most of the time, but 16 CPU cores are required to handle occasional peak traffic, a MySQL database must be configured with 16 CPU cores. OceanBase Cloud is capable of fast and flexible scaling. You can scale specifications up and down as needed, saving costs significantly.</li>
<li><strong>MySQL downside No. 4: weak analysis capabilities</strong>. A MySQL database handles only online transaction processing (OLTP) tasks. To cope with the business involving analytical reports, users must migrate data to a separately built analytical instance, leading to doubled costs. OceanBase Cloud provides hybrid transaction/analytical processing (HTAP) capabilities, which allow users to complete most less complicated tasks all in the same database without creating an analytical instance.</li>
</ul>
<p>To reduce the overall cost, OceanBase provides targeted solutions to address inherent drawbacks of the MySQL architecture, such as low resource utilization and high resource redundancy. In fact, OceanBase Cloud reduces the total cost of ownership (TCO) by 30%. For more information, see <a href="https://mp.weixin.qq.com/s?__biz=MzU0ODg0OTIyNw==&amp;mid=2247500776&amp;idx=1&amp;sn=eb7f931f2d1c59fdac8ca07d55f15c5a&amp;chksm=fbba50cccccdd9daf6b0bfb49d68196450e64bbabe5fbb0d9a6e43c8724ba01fd73453a73d83&amp;scene=21#wechat_redirect" target="_blank" rel="noopener noreferrer">How can OceanBase Cloud help users achieve sustainable cost reduction and efficiency improvement? </a></p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647534704.png" alt="1691647534" class="img_ev3q"></p>
<p>Before we delve into the features, let me walk you through the overall system architecture of OceanBase Cloud, so that you can get familiar with the following terms: OBProxy, OBServer node, partition, zone, and Paxos group.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647552979.png" alt="1691647553" class="img_ev3q"></p>
<ul>
<li>
<p><strong>OBProxy:</strong> Applications connect to OceanBase Cloud through a component called OBProxy. Unlike other middleware, OBProxy is a lightweight node. It only forwards SQL queries without evaluating them. In OceanBase Cloud, OBProxy is not charged.</p>
</li>
<li>
<p><strong>Zone:</strong> A zone of OceanBase Cloud corresponds to an IDC. By default, an OceanBase cluster is deployed across three zones. Each zone contains at least one OBServer node, and hosts many partitions. Therefore, OceanBase Cloud provides IDC-level high availability by default.</p>
</li>
<li>
<p><strong>OBServer node:</strong> An OBServer node is where OceanBase Cloud execute SQL queries or stores data. OceanBase Cloud supports powerful horizontal scaling, which allows users to add more OBServer nodes to each zone.</p>
</li>
<li>
<p><strong>Partition:</strong> A partition is the elementary unit of load balancing in OceanBase Cloud. If a table is divided into several parts, each part is a partition. The whole of a non-partitioned table can be considered as a partition.</p>
</li>
<li>
<p><strong>Paxos group:</strong> In the preceding figure, for example, the three replicas of partition P7 distributed in three zones form a Paxos group. Unlike the primary/secondary mode of MySQL, OceanBase Cloud synchronizes data among replicas based on the Paxos consensus algorithm, which supports automatic elections without external intervention. In other words, the three replicas of P7 can automatically elect the leader. As shown in the figure, the leader is in zone1 at the moment.</p>
<p>In a MySQL database, the data to be synchronized is converted into logical binary logs (binlogs), which are then synchronized from the primary node to the secondary nodes and then converted back into physical logs.</p>
</li>
</ul>
<p>Not involving such complex conversion, the Paxos consensus algorithm works with less latency and higher reliability. At any given moment, Paxos requires the agreement of two of the three replicas to elect the leader and commit logs. This way, OceanBase Cloud can maintain data integrity even in the case of an IDC-level failure. In fact, OceanBase Cloud can recover its service from failures of any level within 30 seconds. In the latest version, the recovery time is reduced to 8 seconds, showing great availability improvement compared with MySQL.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="i-consolidate-multiple-instances-into-tenants-of-one-cluster-to-optimize-resource-utilization-and-make-om-easier"><strong>I. Consolidate multiple instances into tenants of one cluster to optimize resource utilization and make O&amp;M easier</strong><a href="https://oceanbase.github.io/zh-Hans/blog/obcloud#i-consolidate-multiple-instances-into-tenants-of-one-cluster-to-optimize-resource-utilization-and-make-om-easier" class="hash-link" aria-label="i-consolidate-multiple-instances-into-tenants-of-one-cluster-to-optimize-resource-utilization-and-make-om-easier的直接链接" title="i-consolidate-multiple-instances-into-tenants-of-one-cluster-to-optimize-resource-utilization-and-make-om-easier的直接链接">​</a></h2>
<p>The left part of the figure shows a common deployment mode, where one database instance is created for each application. However, this deployment mode often leads to resource utilization imbalance among multiple database instances. For example:</p>
<ul>
<li>Instance 1 hosts an internal HR system with very low daily traffic, and the resource utilization ranges from 5% to 10%.</li>
<li>Instance 2 hosts an e-commerce system, which handles flash sales from time to time, leading to fluctuating transaction volumes. As a result, the resource utilization also severely fluctuates from 3% to 80%</li>
<li>Instance 3 hosts a busy system, whose resource utilization remains high all the time.</li>
</ul>
<p>When a DBA or O&amp;M engineer maintains those systems, they have to check multiple MySQL instances in the console, which means high O&amp;M complexity and risks.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647654901.png" alt="1691647655" class="img_ev3q"></p>
<p>In an OceanBase cluster, you can deploy a resource pool of many OBServer nodes and resources. Then, you can allocate resources exclusive to each tenant, and host each original RDS or MySQL instance in a tenant of the OceanBase cluster. This way, you need to maintain only one OceanBase cluster, significantly reducing the O&amp;M complexity.</p>
<p>The benefits are that you can dynamically adjust tenant specifications anytime without affecting your business, and flexibly schedule resources to optimize the overall resource utilization, leading to a significant cost cut.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ii-reduce-storage-costs-by-using-an-advanced-lsm-tree-based-compression-engine"><strong>II. Reduce storage costs by using an advanced LSM-tree-based compression engine</strong><a href="https://oceanbase.github.io/zh-Hans/blog/obcloud#ii-reduce-storage-costs-by-using-an-advanced-lsm-tree-based-compression-engine" class="hash-link" aria-label="ii-reduce-storage-costs-by-using-an-advanced-lsm-tree-based-compression-engine的直接链接" title="ii-reduce-storage-costs-by-using-an-advanced-lsm-tree-based-compression-engine的直接链接">​</a></h2>
<p>OceanBase Cloud uses an advanced LSM-tree-based storage engine developed in house. Unlike B+ tree, the OceanBase storage engine first stores written data in memory, and when the amount of data in memory reaches a specified threshold, dumps the data to disk. The storage engine compresses the data dumps and merges them with baseline SSTables (ROS) on a daily basis during off-peak hours, starting from 2:00 a.m. by default. This process is named as major compaction. The LSM-tree-based storage stores baseline data in a compact manner. Therefore, compared with B+ tree-based MySQL, OceanBase Cloud inherently provides a higher compression ratio.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647698306.png" alt="1691647698" class="img_ev3q"></p>
<p>In addition, on top of the effect of the LSM-tree architecture, OceanBase Cloud compresses a data set twice.</p>
<p>OceanBase Cloud uses hybrid row-column storage architecture based on microblocks. Each microblock corresponds to a layer of a MySQL page. This way, row-based storage is converted to columnar storage. One of the benefits of columnar storage is that, OceanBase Cloud can compress the columnar storage for the first time by encoding.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647731261.png" alt="1691647731" class="img_ev3q"></p>
<p>We often deal with highly repeated columns in development, such as the <code>RATE_ID</code> column that contains four values that occur repeatedly in the preceding figure. In this case, we can map the values to a dictionary. For example, the value <code>1901321</code> in the column maps to <code>0</code> in the dictionary. As long as the dictionary is maintained, values of each column can be simply stored as, for example, <code>0</code>, <code>1</code>, and <code>2</code>, thus greatly compressing the storage. In practice, OceanBase will adaptively design a encoding method suitable for each application.</p>
<p>After the first compression, a data set of 100 GB may be compressed to 30 GB. Then, OceanBase Cloud will compress the 30 GB of data further by using a general compression algorithm, such as LZ4.</p>
<p>After the second compression, the data set may be compressed to 15 GB. In other words, 100 GB of data in MySQL may occupy only 15 GB of disk space in OceanBase Cloud. In practice, we have compressed data to less than 15% its original size for many of our customers, significantly reducing storage costs.</p>
<p>Some may worry that such a high compression ratio will affect the real-time read/write performance. Well, relax. The LSM-tree-based storage engine compresses data during the daily major compaction, which often takes place after 2:00 a.m. Besides, you can specify when to start the major compaction based on your actual off-peak hours. This way, your business will not suffer from performance loss during the day, and the high compression ratio allows you to cut the storage costs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="iii-handle-peak-traffic-with-rapid-flexible-scaling"><strong>III. Handle peak traffic with rapid flexible scaling</strong><a href="https://oceanbase.github.io/zh-Hans/blog/obcloud#iii-handle-peak-traffic-with-rapid-flexible-scaling" class="hash-link" aria-label="iii-handle-peak-traffic-with-rapid-flexible-scaling的直接链接" title="iii-handle-peak-traffic-with-rapid-flexible-scaling的直接链接">​</a></h2>
<p>With the multi-level auto scaling capability of OceanBase Cloud, you can adjust resources of an OceanBase cluster at any time as your business grows. This way, you can flexibly control your resource bills and O&amp;M tasks. OceanBase Cloud supports triple-level scalability. Specifically, you can scale your database by adjusting tenant specifications, OBServer node specifications, and the number of OBServer nodes.</p>
<p><strong>◼ Level 1 scaling: adjustment of tenant specifications</strong></p>
<p>With a distributed architecture, OceanBase Cloud contains multiple OBServer nodes in a resource pool, which is divided into isolated resource groups. Each resource group is called a tenant. To increase the capacity of your database, you can first scale up tenant specifications. The adjustment of tenant specifications is completed within the OceanBase Database kernel, and does not involve changes of physical resources. In addition, the adjustment takes effect within seconds and has no impact on applications.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647784065.png" alt="1691647784" class="img_ev3q"></p>
<p>The O&amp;M team can adjust the number of CPU cores and memory size of a tenant at any time to smoothly improve the maximum transactions that can be processed per second (TPS) by the tenant. For example, they can adjust the resources during normal business hours.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647808099.png" alt="1691647808" class="img_ev3q"></p>
<p><strong>◼ Level 2 scaling: adjustment of OBServer node specifications</strong></p>
<p>If the Level 1 scaling cannot handle the soaring business traffic, you can increase specifications of the OBServer nodes, which is also known as vertical scaling. For example, you can scale the number of CPU cores of a cluster from 30 to 62. Unlike a MySQL database, whose capacity expansion involves a primary/secondary switchover, and causes business interruptions, OceanBase Cloud synchronizes data among nodes over the Paxos protocol, which features automatic leader election and automatic decision of whether to commit a log.</p>
<p>This brings two advantages:</p>
<ul>
<li>OceanBase Cloud synchronizes data by a smaller data unit, delivering higher performance and flexibility. Compared with node-level log synchronization of MySQL, partitions of a Paxos group in OceanBase Cloud are smaller in size. This avoids the need of compromising performance for ensuring the global order. In addition, OceanBase Cloud supports distributed transactions and allows leaders to be stored on different nodes. As shown in the figure depicting the overall system architecture of OceanBase Cloud, the leaders (indicated in dark blue) are distributed on different nodes to support multi-point writing. This makes full use of resources of multiple servers and allows you to add more nodes.</li>
<li>OceanBase Cloud synchronizes data by using more lightweight physical transaction logs (clogs) at lower costs over the Paxos protocol. When a MySQL database synchronizes data, the primary node generates logical binlogs, which are synchronized to the secondary nodes and then converted into relay logs for execution. With lighter and more efficient clogs, plus the partition-level synchronization granularity of the Paxos protocol, OceanBase Cloud is not affected by the troublesome primary/secondary synchronization latency like MySQL.</li>
</ul>
<p>When you adjust node specifications during scaling, OceanBase Cloud first mounts a node and synchronizes data to it from the leader. When the leader commits its last clog, OceanBase Cloud initiates a Paxos-based leader election, where the current leader gives up its leader role and votes for another node. Then, the leader role is smoothly switched to another node. This scale-up process of OceanBase Cloud, compared with MySQL, is nearly transparent to applications.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647875877.png" alt="1691647876" class="img_ev3q"></p>
<p><strong>◼ Level 3 scaling: adjustment of the number of OBServer nodes</strong></p>
<p>This is something that cannot be done by using a MySQL database in primary/secondary architecture. With its native distributed architecture, OceanBase Cloud supports distributed transactions and horizontal scale-out with zero business interruptions. Specifically, once an OBServer node is added to an OceanBase cluster, business traffic is automatically channeled to this new node. During this process, applications use the cluster like a standalone MySQL database without being affected. This benefit has been proven to be a better solution than database and table sharding in many engineering practices.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647918807.png" alt="1691647918" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="iv-handle-oltp-and-olap-in-one-htap-capable-system"><strong>IV. Handle OLTP and OLAP in one HTAP-capable system</strong><a href="https://oceanbase.github.io/zh-Hans/blog/obcloud#iv-handle-oltp-and-olap-in-one-htap-capable-system" class="hash-link" aria-label="iv-handle-oltp-and-olap-in-one-htap-capable-system的直接链接" title="iv-handle-oltp-and-olap-in-one-htap-capable-system的直接链接">​</a></h2>
<p>MySQL is a typical OLTP database system. Therefore, its optimizer shows unsatisfactory performance in joining large tables and handling queries with large result sets. In a MySQL database, it sometimes takes forever to execute an SQL query, and it does not support flexible resource isolation. Transaction processing (TP) tasks are often affected by large SQL statements. That is why everybody is avoiding using MySQL for analytical processing (AP) tasks.</p>
<p>What if AP tasks are unavoidable? A common solution is to build a dedicated analytical instance and execute AP tasks through the asynchronous extract-transform-load (ETL) process. This solution inevitably incurs costs of the transmission service and the analytical instance.</p>
<p>OceanBase Cloud is an HTAP-capable system that handles both OLTP and online analytical processing (OLAP) requests in the same cluster. So why does OceanBase Cloud outperform MySQL in this regard? We have strengthened its AP capabilities from the following four aspects:</p>
<ul>
<li>OceanBase Cloud provides an enterprise-level optimizer that is comparable to the Oracle optimizer. Despite the query complexity, for example, joining more than 10 tables, or the way an SQL statement is written, the OceanBase optimizer generates optimal execution plans to guarantee the lowest cost of SQL executions.</li>
<li>As mentioned earlier, OceanBase Cloud stores data in microblocks by column. AP tasks that are executed by columns can be faster based on columnar storage.</li>
<li>OceanBase Cloud supports parallel execution. It divides a large SQL execution plan into multiple smaller tasks and launch multiple threads to process these small tasks in parallel. At present, SQL queries and DML and DDL operations can be executed in parallel to accelerate AP tasks.</li>
<li>OceanBase Cloud provides a vectorized execution engine. Compared with a volcano model that reads data by row, the vectorized engine reads data in batches, which is more friendly to large analytical SQL queries.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691647973201.png" alt="1691647973" class="img_ev3q"></p>
<p>The HTAP capabilities of OceanBase Cloud allow you to handle AP and TP requests in one system by using the same data set, without incurring extra costs or the need of creating a dedicated analytical node.</p>
<p>You may be worried that TP tasks are impacted by AP tasks. OceanBase Cloud solves that issue by providing multiple resource isolation methods:</p>
<ul>
<li>Physical isolation between tenants: A dedicated tenant is created to handle AP business.</li>
<li>Physical isolation between zones in the same tenant: A dedicated zone that contains only read-only replicas is created to handle AP business.</li>
<li>Isolation between resource groups in the same tenant: Dedicated resources of a node are allocated to handle AP business through resource isolation.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691648013527.png" alt="1691648013" class="img_ev3q"></p>
<p>As a DBA or O&amp;M engineer, how can you make use of OceanBase Cloud capabilities to achieve significant cost reduction for your enterprise?</p>
<p>Generally, you can deploy an OceanBase cluster whose CPU and memory resources are 80% to 95% of those of your original MySQL database and storage capacity is 15% to 20% thereof. Here are two examples to help you develop a cost reduction plan based on OceanBase Cloud and estimate the result.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="example-1-cost-reduction-plan-for-an-sme">Example 1: Cost reduction plan for an SME<a href="https://oceanbase.github.io/zh-Hans/blog/obcloud#example-1-cost-reduction-plan-for-an-sme" class="hash-link" aria-label="Example 1: Cost reduction plan for an SME的直接链接" title="Example 1: Cost reduction plan for an SME的直接链接">​</a></h3>
<p>As shown in the following figure, the enterprise has one RDS instance with 16 CPU cores, two RDS instances with 16 CPU cores each, and four RDS instances with 2 CPU cores each, which is a typical product array.</p>
<ul>
<li>The largest 16C instance hosts core external business of the enterprise, and runs with high resource utilization.</li>
<li>The two 4C instances handle secondary business, such as the business of an inventory or order management system. Their resource utilization is lower than the core instance.</li>
<li>The four 2C instances handle internal business.</li>
</ul>
<p>The resource utilization of these instances will fluctuate with the business traffic, causing troubles in O&amp;M. Given the aforesaid specification estimation rule, the enterprise can migrate all its business systems to an OceanBase cluster with 30 CPU cores and 1.5 TB of storage. The OceanBase cluster not only takes care of all its business modules, but also optimizes its imbalanced resource utilization to a healthy level.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691648052884.png" alt="1691648052" class="img_ev3q"></p>
<p>In this example, OceanBase Cloud is deployed across multiple zones and is compared against a MySQL cluster. The MySQL cluster also requires multi-zone deployment where each zone is configured with exclusive resources. They are tested by using Sysbench with 1,000 concurrent read/write processes. OceanBase Cloud reduces the cost by about 30%, and the result comparison is shown in the following figure.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691648078609.png" alt="1691648078" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="example-2-cost-reduction-plan-for-a-corporation">Example 2: Cost reduction plan for a corporation<a href="https://oceanbase.github.io/zh-Hans/blog/obcloud#example-2-cost-reduction-plan-for-a-corporation" class="hash-link" aria-label="Example 2: Cost reduction plan for a corporation的直接链接" title="Example 2: Cost reduction plan for a corporation的直接链接">​</a></h2>
<p>This corporation runs a 32C RDS instance to support its core business, a 16C RDS instance to support its secondary business, and five 4C RDS instances to support its internal low-traffic business. The overall resource utilization of the RDS instances is roughly the same with that in Example 1. The corporation can host all its RDS instances in an OceanBase cluster with 62 CPU cores and 4 TB of storage.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691648109839.png" alt="1691648109" class="img_ev3q"></p>
<p>Regarding how to plan costs, here is a brief introduction to the difference between the two. In the Sysbench test scenario with 1,000 concurrent read/write processes, if the primary node of a MySQL database uses 68 CPU cores, another 136 CPU cores must be used for the secondary nodes, resulting in serious resource waste. OceanBase Cloud, on the contrary, allows both its leader and follower replicas to share the 62 CPU cores of all OBServer nodes, reducing the total cost by about 40%. The result comparison is shown in the following figure.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2023-08/1691648148198.png" alt="1691648148" class="img_ev3q"></p>
<p>In general, we should combine cost reduction with efficiency improvement. OceanBase Cloud relies on technical means to reduce costs, in the expectation that the overall value of a database can be improved without sacrificing data throughput, high availability, online DDL capability, or O&amp;M friendliness for cost reduction.</p>
<p>As OceanBase Cloud becomes better, we hope that it can bring more benefits to enterprises and help enterprises boost efficiency at reduced costs. In this way, related personnel such as DBAs and O&amp;M engineers can be freed from burdensome maintenance chores and have more time to create greater value for the enterprise.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Adaptive Techniques in the OceanBase SQL Execution Engine]]></title>
            <link>https://oceanbase.github.io/zh-Hans/blog/adaptive-sql-execution-engine</link>
            <guid>https://oceanbase.github.io/zh-Hans/blog/adaptive-sql-execution-engine</guid>
            <pubDate>Mon, 03 Jun 2024 13:23:07 GMT</pubDate>
            <description><![CDATA[I have been studying the book "An Interpretation of OceanBase Database Source Code" and noticed that it contains very little content about the SQL executor. The book focuses on parallel execution in the executor. This blog post introduces some common adaptive techniques in the executor of OceanBase Database. You can take it as a supplement to the executor part in this book.]]></description>
            <content:encoded><![CDATA[<blockquote>
<p>I have been studying the book "An Interpretation of OceanBase Database Source Code" and noticed that it contains very little content about the SQL executor. The book focuses on parallel execution in the executor. This blog post introduces some common adaptive techniques in the executor of OceanBase Database. You can take it as a supplement to the executor part in this book.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="challenges-facing-ap-performance-improvement">Challenges facing AP performance improvement<a href="https://oceanbase.github.io/zh-Hans/blog/adaptive-sql-execution-engine#challenges-facing-ap-performance-improvement" class="hash-link" aria-label="Challenges facing AP performance improvement的直接链接" title="Challenges facing AP performance improvement的直接链接">​</a></h2>
<p>If you want to improve the AP performance of a database, you will face three challenges:</p>
<ul>
<li>First, the optimizer cannot ensure that its estimates are always absolutely accurate. The reasons are complex. For example, the statistics are inaccurate in some scenarios, or the cost model is inconsistent with the actual model. These reasons will contribute to a non-optimal execution plan.</li>
<li>Second, data skew often occurs in production and business scenarios, which will significantly affect the execution efficiency, especially the parallel execution efficiency.</li>
<li>Third, the semantics of NULL are special. Characteristics of widespread NULL values are different from those of normal values in operations such as joins, but this is easily ignored. The executor must perform special processing on NULL values. Otherwise, various bad cases can occur.</li>
</ul>
<p>Adaptive techniques enable the execution engine to dynamically adjust the execution strategy based on the actual situation, thereby improving the execution performance. In a word, adaptive techniques are introduced to address the preceding challenges. Next, let me introduce some typical adaptive techniques in the executor of OceanBase Database.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="adaptive-join-filter">Adaptive join filter<a href="https://oceanbase.github.io/zh-Hans/blog/adaptive-sql-execution-engine#adaptive-join-filter" class="hash-link" aria-label="Adaptive join filter的直接链接" title="Adaptive join filter的直接链接">​</a></h2>
<p>Let me take a hash join shown in the following figure as an example to introduce the background of join filters. The hash join involves two tables and uses hash repartitioning as the data shuffle mode. In other words, each row in the left-side and right-side tables will be repartitioned based on the hash value.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636564738.png" alt="1705636564" class="img_ev3q"></p>
<p>Generally, the right-side table of a hash join is very large in size, which will lead to a high cost in data shuffle. When the left-side table is read to build the hash table, a join filter can extract the data characteristics of the left-side table and send them to the right-side table. This can filter out some of the data in the right-side table before a shuffle. If the join filter has high filtering performance, this step can significantly reduce the network overhead.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636577135.png" alt="1705636577" class="img_ev3q"></p>
<p>OceanBase Database has implemented join filters as early as in V3.x and has been undergoing join filter optimization ever since. The following figure shows the impact of join filters on the overall performance during the TPC-H benchmark run in 2021, in which OceanBase Database won the first place in the world.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636587368.png" alt="1705636587" class="img_ev3q"></p>
<p>Join filters help significantly improve the performance for joins on large tables, such as Q9 shown in the preceding figure.</p>
<p>However, join filters cannot always bring positive benefits in all scenarios, such as Q18 shown in the preceding figure. The overhead of a join filter is used for three tasks:</p>
<ul>
<li>Create the join filter. This is to extract the data characteristics of the left-side table in a hash join when the left-side table is read to build the hash table.</li>
<li>Send the join filter. This is to send the data characteristics of the left-side table to the right-side table.</li>
<li>Apply the join filter. This is to apply the data characteristics of the left-side table on the right-side table for row filtering.</li>
</ul>
<p>If the selectivity of the join filter is low, the reduced network overhead cannot make up the preceding overhead, and the overall performance will deteriorate.</p>
<p>The optimizer determines whether to allocate a join filter based on the cost. The optimizer can roughly estimate the selectivity of a join filter based on statistics such as the number of distinct values (NDV) and MIN/MAX values. However, the optimizer cannot provide accurate estimates of intermediate calculation results in the executor.</p>
<p>To resolve this issue, OceanBase Database V4.1 implements sliding window-based adaptive join filters. This algorithm aims to make up the performance loss when an incorrect join filter is applied.</p>
<p>This algorithm splits data into multiple sliding windows and collects statistics on the apply process of each window. If the algorithm detects that the filtering effects of a window are not as expected, it will not apply the join filter on the next window and pass this window. If the filtering effects of continuous windows are not as expected, the number of passed windows will also increase linearly to reduce the apply cost.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636596184.png" alt="1705636596" class="img_ev3q"></p>
<p>The following figure shows a bad case of join filter. Different strategies are used for performance tests. In the performance test where an adaptive join filter is used, the performance loss is made up by half. However, the performance after compensation is still lower than that achieved when the join filter is not allocated.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636604838.png" alt="1705636604" class="img_ev3q"></p>
<p>Although this solution makes up the performance loss caused by applying a join filter on the right-side table, the cost in creating and sending the join filter is not made up. OceanBase Database will enhance the capability for adaptive join filter creation in later versions.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="adaptive-hash-group-by">Adaptive HASH GROUP BY<a href="https://oceanbase.github.io/zh-Hans/blog/adaptive-sql-execution-engine#adaptive-hash-group-by" class="hash-link" aria-label="Adaptive HASH GROUP BY的直接链接" title="Adaptive HASH GROUP BY的直接链接">​</a></h2>
<p>This section introduces the adaptive algorithm for HASH GROUP BY in OceanBase Database.</p>
<p>The following figures show the execution plans for HASH GROUP BY in a parallel scenario.</p>
<p>Here is the execution plan for two-phase HASH GROUP BY.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636615372.png" alt="1705636615" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636625338.png" alt="1705636625" class="img_ev3q"></p>
<p>Here is the execution plan for one-phase HASH GROUP BY.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636635870.png" alt="1705636636" class="img_ev3q"></p>
<p>The difference is that two-phase HASH GROUP BY performs a partial GROUP BY operation on data before a shuffle. Like join filters, one-phase and two-phase HASH GROUP BY have their own advantages and disadvantages.</p>
<ul>
<li>Two-phase HASH GROUP BY applies to scenarios with a high data aggregation rate, where the amount of data to be shuffled can be decreased through pre-aggregation.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636648890.png" alt="1705636648" class="img_ev3q"></p>
<ul>
<li>One-phase HASH GROUP BY applies to scenarios with a low data aggregation rate.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636705335.png" alt="1705636705" class="img_ev3q"></p>
<p>If the data aggregation rate is low, the network overhead will still be high because a two-phase plan will consume extra CPU resources to probe the hash table, leading to poorer performance than a one-phase plan.</p>
<p>The following figure compares the performance of two-phase and one-phase plans for queries in ClickBench. It can be observed that some queries are suitable for two-phase execution while others are suitable for one-phase execution. Generally, the optimizer tends to select a two-phase plan to avoid serious performance deterioration.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636714319.png" alt="1705636714" class="img_ev3q"></p>
<p>In versions earlier than OceanBase Database V4.x, the optimizer will determine whether to select a two-phase plan or one-phase plan based on the NDV value in the statistics. You can also use the session variable _GROUPBY_NOPUSHDOWN_CUT_RATIO to set the plan preference. If the ratio of the input data amount to the data amount after aggregation is greater than the specified value, a two-phase plan is generated. Otherwise, a one-phase plan is generated. In practice, it is difficult to use this variable. The input and output data amounts of GROUP BY are estimated by the optimizer based on statistics. Generally, it is challenging for O&amp;M personnel to set this variable to an appropriate value, ensuring that the optimizer selects a better plan for GROUP BY.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636723981.png" alt="1705636724" class="img_ev3q"></p>
<p>In OceanBase Database V4.x, the _GROUPBY_NOPUSHDOWN_CUT_RATIO variable is deprecated and the optimizer is forced to select a two-phase plan. In a two-phase plan in V4.x, the first phase must be adaptive GROUP BY.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636736767.png" alt="1705636736" class="img_ev3q"></p>
<p>The core idea of the adaptive GROUP BY technique is to determine whether to perform deduplication or directly send data based on an NDV value collected in real time. The technique splits data into multiple rounds and measures the aggregation rate of each round. If the deduplication rate of a round is not as expected, the technique will clear the hash table, flush all the data obtained in the first phase to the network, and aggregate the final data in the second phase.</p>
<p>Data is split into rounds based on the size of three-level CPU caches. This is because if the hash table can be accommodated in the L2 cache, the performance can be improved by more than 30% compared with that of a large hash table. A cache-aware mechanism is provided to increase the size of data in each round from the L2 cache size to the L3 cache size when the deduplication rate becomes low so that data will be accommodated in the L3 cache.</p>
<p>If the hash deduplication effects of multiple consecutive rounds are poor, the bypass strategy is used. Specifically, rows are directly delivered to the upper-layer operator without hash deduplication, which looks like a one-phase plan.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636744584.png" alt="1705636744" class="img_ev3q"></p>
<p>This strategy greatly improves the performance but also has bad cases where a large amount of data is involved while the overall deduplication rate is favorable. If the overall deduplication rate is estimated based on only a small part of the data, the estimate is probably inaccurate. In OceanBase Database V4.2, the NDV values of multiple data rounds are merged to improve the estimate accuracy.</p>
<p>The following figure compares the performance of one-phase, adaptive, and two-phase GROUP BY for queries in ClickBench.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636754240.png" alt="1705636754" class="img_ev3q"></p>
<p>The result shows that adaptive GROUP BY can select an appropriate execution strategy almost in all scenarios. With adaptive GROUP BY, one plan applies to different data models. This is the goal we aim to achieve. OceanBase Database V4.3 will support a global NDV estimate strategy to make adaptive decision-making more accurate.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="adaptive-hybrid-hash-shuffling">Adaptive hybrid hash shuffling<a href="https://oceanbase.github.io/zh-Hans/blog/adaptive-sql-execution-engine#adaptive-hybrid-hash-shuffling" class="hash-link" aria-label="Adaptive hybrid hash shuffling的直接链接" title="Adaptive hybrid hash shuffling的直接链接">​</a></h2>
<p>Next, let me introduce some adaptive techniques we have developed based on data skew. The following figures show two common plans for a simple distributed hash join.</p>
<p>One is a broadcast plan, which broadcasts the left-side table to each thread of the right-side table. The threads will use data in the right-side table to probe data in the left-side table.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636765811.png" alt="1705636765" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636772964.png" alt="1705636773" class="img_ev3q"></p>
<p>The other is a hash repartitioning plan, which distributes the data in the left-side and right-side tables to different threads based on the hash value. Each thread performs a join separately.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636780959.png" alt="1705636781" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636790880.png" alt="1705636790" class="img_ev3q"></p>
<p>The two plans have their own advantages and disadvantages. Generally, the broadcast plan applies to a scenario where a large table is joined with a small table. In this scenario, the small table is broadcast to limit the broadcast cost. In a scenario where two large tables are joined and repartitioning based on the join key is not supported, hash-hash is almost the only choice. However, the hash-hash strategy also has bad cases in data skew scenarios. For example, the following figure shows a high-frequency value. Since data is distributed to different threads based on the hash value for hash repartitioning, all instances of the high-frequency value are distributed to the same hash join, leading to a long-tail situation of the hash join.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636798440.png" alt="1705636798" class="img_ev3q"></p>
<p>The following figure shows a similar business scenario as observed by the SQL plan monitor tool.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636805216.png" alt="1705636805" class="img_ev3q"></p>
<p>To resolve this issue, OceanBase Database V4.x implements the hybrid hash shuffling algorithm. The following figure shows an execution plan that uses this algorithm. It looks like a plan that uses the hash repartitioning algorithm. The only difference is that a HYBRID keyword is contained in the EXCHANGE operator.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636812110.png" alt="1705636812" class="img_ev3q"></p>
<p>The hybrid hash shuffling algorithm will obtain related information about high-frequency values from the optimizer. For regular values, normal hash shuffling is used for hash repartitioning. For a high-frequency value, if it exists on the left side (hash join build side), the value will be broadcast to all threads to build the hash table. If it exists on the right side (hash join probe side), the instances of this value are randomly distributed to ensure the evenness. This algorithm can effectively resolve performance issues caused by hash repartitioning in data skew scenarios.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636821493.png" alt="1705636821" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="adaptive-null-aware-hash-join">Adaptive NULL-aware hash join<a href="https://oceanbase.github.io/zh-Hans/blog/adaptive-sql-execution-engine#adaptive-null-aware-hash-join" class="hash-link" aria-label="Adaptive NULL-aware hash join的直接链接" title="Adaptive NULL-aware hash join的直接链接">​</a></h2>
<p>Finally, let me briefly introduce some optimization techniques we have applied to handle NULL values. For a join, the return result of a NULL value is always NULL in an equal condition. However, the semantics of NULL vary based on the join method.</p>
<p>In inner joins and semi-joins, values whose join key is NULL can be ignored.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636829687.png" alt="1705636829" class="img_ev3q"></p>
<p>In left outer joins, NULL values on the left side also need to be output.</p>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636835954.png" alt="1705636836" class="img_ev3q"></p>
<p>If NULL values are processed as normal values, correct results can be obtained. However, data skew of NULL values or a useless network shuffle of massive NULL values may occur. The following special measures are taken inside hash joins and during a hash join shuffle:</p>
<ul>
<li>Skip NULL values in join keys. This measure usually applies to inner joins and semi-joins, in which values whose join key is NULL will not be output.</li>
<li>Randomly distribute NULL values to avoid data skew. Generally, for an outer join, specifically, the left side of a left outer join, right side of a right outer join, or both sides of a full outer join, if the NULL values of one side or both sides need to be output, these NULL values will not successfully match any value. In this case, random hash values are assigned to these NULL values and the NULL values are randomly distributed to different threads. A NULL value that does not need to be output will still be skipped.</li>
<li>Use the NULL-aware anti-join algorithm, which will not be described in this post, to process anti-joins that contain NOT IN. The semantics of such anti-joins are special.</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://obcommunityprod.oss-cn-shanghai.aliyuncs.com/prod/blog/2024-01/1705636844393.png" alt="1705636844" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="preview-of-the-next-post">Preview of the next post<a href="https://oceanbase.github.io/zh-Hans/blog/adaptive-sql-execution-engine#preview-of-the-next-post" class="hash-link" aria-label="Preview of the next post的直接链接" title="Preview of the next post的直接链接">​</a></h2>
<p>This post introduces some representative adaptive techniques in the executor of OceanBase Database, based on the assumption that you have a basic understanding of the two-phase pushdown technique for HASH GROUP BY. If you are unfamiliar with the multi-phase pushdown technique of the executor, please look forward to the next post <a href="https://open.oceanbase.com/blog/5382203648" target="_blank" rel="noopener noreferrer">Distributed Pushdown Techniques of OceanBase Database</a>.</p>]]></content:encoded>
        </item>
    </channel>
</rss>